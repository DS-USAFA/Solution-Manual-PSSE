[["index.html", "Solutions for Probability and Statistics for Scientists and Engineers Preface 0.1 Book Structure and How to Use It 0.2 Packages 0.3 File Creation Information", " Solutions for Probability and Statistics for Scientists and Engineers Matthew Davis Brianna Hitt Kenneth Horton Bradley Warner 2021-11-22 Preface Contained in this volume are the solutions to homework problems in the Probability and Statistics for Scientists and Engineers book. 0.1 Book Structure and How to Use It This solution manual is setup to match the structure of the accompanying book. The learning outcomes for this course are to use computational and mathematical statistical/probabilistic concepts for: Developing probabilistic models Developing statistical models for inference and description Advancing practical and theoretical analytic experience and skills 0.2 Packages These notes make use of the following packages in R knitr (Xie 2021), rmarkdown (Allaire et al. 2021), mosaic (Pruim, Kaplan, and Horton 2021), mosaicCalc (Kaplan, Pruim, and Horton 2020), tidyverse (Wickham 2021), ISLR (James et al. 2021), vcd (Meyer, Zeileis, and Hornik 2020), ggplot2 (Wickham et al. 2021), MASS (Ripley 2021), openintro (Çetinkaya-Rundel et al. 2021), broom (Robinson, Hayes, and Couch 2021), infer (Bray et al. 2021), ISLR (James et al. 2021), kableExtra (Zhu 2021), DT (Xie, Cheng, and Tan 2021). This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. 0.3 File Creation Information File creation date: 2021-11-22 R version 4.1.2 (2021-11-01) References "],["CS1.html", "Chapter 1 Case Study 1.1 Objectives 1.2 Homework", " Chapter 1 Case Study 1.1 Objectives Use R for basic analysis and visualization. Compile a report using knitr. 1.2 Homework Load tidyverse,mosaic, and knitr packages. library(tidyverse) library(mosaic) library(knitr) 1.2.1 Problem 1 Stent study continued Complete a similar analysis for the stent data but this time for the one year data. In particular Read the data into your working directory. stent_study &lt;-read_csv(&#39;data/stent_study.csv&#39;) Complete similar steps as in the class notes. i. Use inspect on the data. ii. Create a table of outcome365 and group. Comment on the results. iii. Create a barchart of the data. Using inspect inspect(stent_study) ## ## categorical variables: ## name class levels n missing ## 1 group character 2 451 0 ## 2 outcome30 character 2 451 0 ## 3 outcome365 character 2 451 0 ## distribution ## 1 control (50.3%), trmt (49.7%) ## 2 no_event (89.8%), stroke (10.2%) ## 3 no_event (83.8%), stroke (16.2%) The table: tally(outcome365~group,data=stent_study,format=&quot;proportion&quot;,margins = TRUE) ## group ## outcome365 control trmt ## no_event 0.8766520 0.7991071 ## stroke 0.1233480 0.2008929 ## Total 1.0000000 1.0000000 Patients in the treatment group had a higher proportion of strokes than those in the control group after one year. The treatment does not appear to help the rate of strokes and in fact may hurt it. Barchart: stent_study %&gt;% gf_props(~group,fill=~outcome365,position=&#39;fill&#39;) %&gt;% gf_labs(title=&quot;Impact of Stents of Stroke&quot;, subtitle=&#39;Experiment with 451 Patients&#39;, x=&quot;Experimental Group&quot;, y=&quot;Number of Events&quot;) 1.2.2 Problem 2 Migraine and acupuncture A migraine is a particularly painful type of headache, which patients sometimes wish to treat with acupuncture. To determine whether acupuncture relieves migraine pain, researchers conducted a randomized controlled study where 89 females diagnosed with migraine headaches were randomly assigned to one of two groups: treatment or control. 43 patients in the treatment group received acupuncture that is specifically designed to treat migraines. 46 patients in the control group received placebo acupuncture (needle insertion at nonacupoint locations). 24 hours after patients received acupuncture, they were asked if they were pain free.1 The data is in the file migraine_study.csv in the folder data. Complete the following work: Read the data into an object called migraine_study. migraine_study &lt;- read_csv(&quot;data/migraine_study.csv&quot;) head(migraine_study) ## # A tibble: 6 × 2 ## group pain_free ## &lt;chr&gt; &lt;chr&gt; ## 1 treatment yes ## 2 treatment yes ## 3 treatment yes ## 4 treatment yes ## 5 treatment yes ## 6 treatment yes Create a table of the data. tally(pain_free~group,data=migraine_study,format=&quot;proportion&quot;,margin=TRUE) ## group ## pain_free control treatment ## no 0.95652174 0.76744186 ## yes 0.04347826 0.23255814 ## Total 1.00000000 1.00000000 Report the percent of patients in the treatment group who were pain free 24 hours after receiving acupuncture. There are 23.2% of the treatment group pain free. Repeat for the control group. There are only 4.3% of the control group pain free. At first glance, does acupuncture appear to be an effective treatment for migraines? Explain your reasoning. Yes, a substantial increase in the percentage of patients pain free after acupuncture versus those with no acupuncture, so it appears to be effective. Do the data provide convincing evidence that there is a real pain reduction for those patients in the treatment group? Or do you think that the observed difference might just be due to chance? Either of these is acceptable: We could get slightly different group estimates even if there is no real difference. Though the difference is big, I’m skeptical the results show a real difference and think this might be due to chance. The difference in these rates looks pretty big, and so I suspect acupuncture is having a positive impact on pain. Compile, knit, this report into a pdf. Complete on your computer or server. G. Allais et al. “Ear acupuncture in the treatment of migraine attacks: a randomized trial on the efficacy of appropriate versus inappropriate acupoints.” In: Neurological Sci. 32.1 (2011), pp. 173–175.↩︎ "],["DB.html", "Chapter 2 Data Basics 2.1 Objectives 2.2 Homework", " Chapter 2 Data Basics 2.1 Objectives Define and use properly in context all new terminology to include but not limited to case, observational unit, variables, data frame, associated variables, independent, and discrete and continuous variables. Identify and define the different types of variables. From reading a study, explain the research question. Create a scatterplot in R and determine the association of two numerical variables from the plot. 2.2 Homework Identify study components Identify (i) the cases, (ii) the variables and their types, and (iii) the main research question in the studies described below. 2.2.1 Problem 1 Researchers collected data to examine the relationship between pollutants and preterm births in Southern California. During the study air pollution levels were measured by air quality monitoring stations. Specifically, levels of carbon monoxide were recorded in parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter (PM\\(_{10}\\)) in \\(\\mu g/m^3\\). Length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. The analysis suggested that increased ambient PM\\(_{10}\\) and, to a lesser degree, CO concentrations may be associated with the occurrence of preterm births.2 The cases are 143,196 eligible study subjects who were born in Southern California between 1989 and 1993. The variables are measurements of carbon monoxide (CO), nitrogen dioxide, ozone, and particulate matter less than 10\\(\\mu m\\) (PM10) collected at air-quality-monitoring stations as well as length of gestation. All of these variables are continuous numerical variables. The research question was Is there an association between air pollution exposure and preterm births? 2.2.2 Problem 2 The Buteyko method is a shallow breathing technique developed by Konstantin Buteyko, a Russian doctor, in 1952. Anecdotal evidence suggests that the Buteyko method can reduce asthma symptoms and improve quality of life. In a scientific study to determine the effectiveness of this method, researchers recruited 600 asthma patients aged 18-69 who relied on medication for asthma treatment. These patients were split into two research groups: one practiced the Buteyko method and the other did not. Patients were scored on quality of life, activity, asthma symptoms, and medication reduction on a scale from 0 to 10. On average, the participants in the Buteyko group experienced a significant reduction in asthma symptoms and an improvement in quality of life.3 The cases are 600 adult patients aged 18-69 years diagnosed and currently treated for asthma. The variables were whether or not the patient practiced the Buteyko method (categorical) and measures of quality of life, activity, asthma symptoms and medication reduction of the patients (categorical, ordinal). It may also be reasonable to treat the ratings on a scale of 1 to 10 as discrete numerical variables. The research question was Do asthmatic patients who practice the Buteyko method experience improvement in their condition? 2.2.3 Problem 3 In the package Stat2Data is a data set called Election16. Create a scatterplot for the percent of advanced degree versus per capita income in the state. Describe the relationship between these two variables. Note: you may have to load the library and data set. Load the library: library(Stat2Data) data(Election16) Create the scatterplot. Election16 %&gt;% gf_point(Income~Adv) There appears to be a positive association between the percentage of advanced degrees in a state and the per capita income. B. Ritz et al. “Effect of air pollution on preterm birth among children born in Southern California between 1989 and 1993”. In: Epidemiology 11.5 (2000), pp. 502–511.↩︎ J. McGowan. “Health Education: Does the Buteyko Institute Method make a difference?” In: Thorax 58 (2003).↩︎ "],["ODCP.html", "Chapter 3 Overview of Data Collection Principles 3.1 Objectives 3.2 Homework", " Chapter 3 Overview of Data Collection Principles 3.1 Objectives Define and use properly in context all new terminology. From a description of a research project, at a minimum be able to describe the population of interest, the generalizability of the study, the response and predictor variables, differentiate whether it is observational or experimental, and determine the type of sample. Explain in the context of a problem how to conduct a sample for the different types of sampling procedures. 3.2 Homework 3.2.1 Problem 1 Generalizability and causality Identify the population of interest and the sample in the studies described below, these are the same studies from the previous lesson. Also comment on whether or not the results of the study can be generalized to the population and if the findings of the study can be used to establish causal relationships. Researchers collected data to examine the relationship between pollutants and preterm births in Southern California. During the study air pollution levels were measured by air quality monitoring stations. Specifically, levels of carbon monoxide were recorded in parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter (PM\\(_{10}\\)) in \\(\\mu g/m^3\\). Length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. The analysis suggested that increased ambient PM\\(_{10}\\) and, to a lesser degree, CO concentrations may be associated with the occurrence of preterm births.4 The population of interest is all births. The sample consists of the 143,196 births between 1989 and 1993 in Southern California. If births in this time span at the geography can be considered to be representative of all births, then the results are generalizable to the population of Southern California. However, since the study is observational the findings cannot be used to establish causal relationships. The Buteyko method is a shallow breathing technique developed by Konstantin Buteyko, a Russian doctor, in 1952. Anecdotal evidence suggests that the Buteyko method can reduce asthma symptoms and improve quality of life. In a scientific study to determine the effectiveness of this method, researchers recruited 600 asthma patients aged 18-69 who relied on medication for asthma treatment. These patients were split into two research groups: one practiced the Buteyko method and the other did not. Patients were scored on quality of life, activity, asthma symptoms, and medication reduction on a scale from 0 to 10. On average, the participants in the Buteyko group experienced a significant reduction in asthma symptoms and an improvement in quality of life.5 The population is all 18-69 year olds diagnosed and currently treated for asthma. The sample is the 600 adult patients aged 18-69 years diagnosed and currently treated for asthma. Since the sample is not random (voluntary) the results cannot be generalized to the population at large. However, since the study is an experiment, the findings can be used to establish causal relationships. 3.2.2 Problem 2 GPA and study time A survey was conducted on 55 undergraduates from Duke University who took an introductory statistics course in Spring 2012. Among many other questions, this survey asked them about their GPA and the number of hours they spent studying per week. The scatterplot below displays the relationship between these two variables. What is the explanatory variable and what is the response variable? Describe the relationship between the two variables. Make sure to discuss unusual observations, if any. Is this an experiment or an observational study? Can we conclude that studying longer hours leads to higher GPAs? Solutions The explanatory variable is the number of study hours per week, and the response variable is GPA. There is a somewhat weak positive relationship between the two variables, though the data become more sparse as the number of study hours increases. One responded reported a GPA above 4.0, which is clearly a data error. Also, there are a few respondents who reported unusually high study hours (60 and 70 hours/week). It should also be noted that the variability in GPA is much higher for students who study less than those who study more, also might be due to the fact that there aren’t many respondents who reported studying higher hours. This is an observational study. Since this is an observational study, we cannot conclude that there is a causal relationship between the two variables even though there appears to be an association. 3.2.3 Problem 3 Income and education The scatterplot below shows the relationship between per capita income (in thousands of dollars) and percent of population with a bachelor’s degree in 3,143 counties in the US in 2010. What are the explanatory and response variables? Describe the relationship between the two variables. Make sure to discuss unusual observations, if any. Can we conclude that having a bachelor’s degree increases one’s income? Solutions The explanatory variable is percent of population with a bachelor’s degree and the response variable is per capita income (in thousands). There is a strong positive linear relationship between the two variables. As the percentage of population with a bachelor’s degree increases the per capita income increases as well. There are very few counties where more than 60% of the population have a bachelor’s degree and very few countries that have a more than $50,000 in per capita income. This is an observational study so we cannot make a causal statement based on the results. However, we can say that having a higher percentage of population with bachelor’s degree is associated with a higher per capita income. B. Ritz et al. “Effect of air pollution on preterm birth among children born in Southern California between 1989 and 1993”. In: Epidemiology 11.5 (2000), pp. 502–511.↩︎ J. McGowan. “Health Education: Does the Buteyko Institute Method make a difference?” In: Thorax 58 (2003).↩︎ "],["STUDY.html", "Chapter 4 Studies 4.1 Objectives 4.2 Homework", " Chapter 4 Studies 4.1 Objectives Define and use properly in context all new terminology. Given a study description, be able to identify and explain the study using correct terms. Given a scenario, describe flaws in reasoning and propose study and sampling designs. 4.2 Homework 4.2.1 Problem 1 Propose a sampling strategy A large college class has 160 students. All 160 students attend the lectures together, but the students are divided into 4 groups, each of 40 students, for lab sections administered by different teaching assistants. The professor wants to conduct a survey about how satisfied the students are with the course, and he believes that the lab section a student is in might affect the student’s overall satisfaction with the course. What type of study is this? Observational study. Suggest a sampling strategy for carrying out this study. Stratified sample, sample randomly within each section. 4.2.2 Problem 2 Flawed reasoning Identify the flaw in reasoning in the following scenarios. Explain what the individuals in the study should have done differently if they wanted to make such strong conclusions. Students at an elementary school are given a questionnaire that they are required to return after their parents have completed it. One of the questions asked is, Do you find that your work schedule makes it difficult for you to spend time with your kids after school? Of the parents who replied, 85% said no. Based on these results, the school officials conclude that a great majority of the parents have no difficulty spending time with their kids after school. Solution Non-responders may have a different response to this question. The parents who returned the surveys are probably those who do not have difficulty spending time with their kids after school. Parents who work might not have returned the surveys since they probably have a busier schedule. A survey is conducted on a simple random sample of 1,000 women who recently gave birth, asking them about whether or not they smoked during pregnancy. A follow-up survey asking if the children have respiratory problems is conducted 3 years later, however, only 567 of these women are reached at the same address. The researcher reports that these 567 women are representative of all mothers. Solution It is unlikely that the women who were reached at the same address 3 years later are a random sample. These missing responders are probably renters (as opposed to homeowners) which means that they might be in a lower socio-economic status than the respondents. 4.2.3 Problem 3 Sampling strategies A Math 377 student who is curious about the relationship between the amount of time students spend on social networking sites and their performance at school decides to conduct a survey. Four research strategies for collecting data are described below. In each, name the sampling method proposed and any bias you might expect. He randomly samples 40 students from the study’s population, gives them the survey, asks them to fill it out and bring it back the next day. He gives out the survey only to his friends, and makes sure each one of them fills out the survey. He posts a link to an online survey on his Facebook wall and asks his friends to fill out the survey. He stands outside the QRC and asks every third person that walks out the door to fill out the survey. Solution a. Simple random sample. Non-response bias, if only those people who have strong opinions about the survey responds his sample may not be representative of the population. b. Convenience sample. Under coverage bias, his sample may not be representative of the population since it consists only of his friends. It is also possible that the study will have non-response bias if some choose to not bring back the survey. c. Convenience sample. This will have a similar issues to handing out surveys to friends. d. Systematic sample. Could have non-respons bias. It could also have bias as three students may not be enough separation to get good coverage. 4.2.4 Problem 4 Vitamin supplements In order to assess the effectiveness of taking large doses of vitamin C in reducing the duration of the common cold, researchers recruited 400 healthy volunteers from staff and students at a university. A quarter of the patients were assigned a placebo, and the rest were evenly divided between 1g Vitamin C, 3g Vitamin C, or 3g Vitamin C plus additives to be taken at onset of a cold for the following two days. All tablets had identical appearance and packaging. The nurses who handed the prescribed pills to the patients knew which patient received which treatment, but the researchers assessing the patients when they were sick did not. No significant differences were observed in any measure of cold duration or severity between the four medication groups, and the placebo group had the shortest duration of symptoms. Was this an experiment or an observational study? Why? What are the explanatory and response variables in this study? Were the patients blinded to their treatment? Was this study double-blind? Participants are ultimately able to choose whether or not to use the pills prescribed to them. We might expect that not all of them will adhere and take their pills. Does this introduce a confounding variable to the study? Explain your reasoning. Solution a. Experiment, since the researchers randomly assigned different treatments to the participants. b. Response variable: Duration of the cold. Explanatory variable: Treatment, with 4 levels; placebo, 1g, 3g, 3g with additives. c. The patients were blinded as they did not know which treatment they received. d. The study was double-blind with respect to the researchers evaluating the patients, but the nurses who briely interacted with patients during the distribution of the medication were not blinded. (It was partially double-blind.) e. Since the patients were randomly assigned to the treatment groups and they are blinded we would expect about an equal number of patients in each group to not adhere to the treatment. While this means that final results of the study will be based on fewer number of participants, non-adherence does not introduce a confounding variable to the study. 4.2.5 Problem 5 Exercise and mental health A researcher is interested in the effects of exercise on mental health and she proposes the following study: Use stratified random sampling to ensure representative proportions of 18-30, 31-40 and 41-55 year olds from the population. Next, randomly assign half the subjects from each age group to exercise twice a week, and instruct the rest not to exercise. Conduct a mental health exam at the beginning and at the end of the study, and compare the results. What type of study is this? What are the treatment and control groups in this study? Does this study make use of blocking? If so, what is the blocking variable? Does this study make use of blinding? Comment on whether or not the results of the study can be used to establish a causal relationship between exercise and mental health, and indicate whether or not the conclusions can be generalized to the population at large. Suppose you are given the task of determining if this proposed study should get funding. Would you have any reservations about the study proposal? Solution a. This is an experiment since we assigned subjects to the exercise program. b. The treatment is exercise twice a week and control is no exercise. c, Yes, the blocking variable is age. d. No, the study is not blinded since the patients will know whether or not they are exercising. e. Since this is an experiment, we can make a causal statement. Since the sample is random, the causal statement can be generalized to the population at large. However, we should be cautious about making a causal statement because of a possible placebo effect. f. It would be very difficult, if not impossible, to successfully conduct this study since randomly sampled people cannot be required to participate in a clinical trial. "],["NUMDATA.html", "Chapter 5 Numerical Data 5.1 Objectives 5.2 Homework", " Chapter 5 Numerical Data 5.1 Objectives Define and use properly in context all new terminology. Generate in R summary statistics for a numeric variable including breaking down by cases. Generate in R appropriate graphical summaries of numerical variables. Be able to interpret and explain output both graphically and numerically. 5.2 Homework 5.2.1 Problem 1 Mammals exploratory Data were collected on 39 species of mammals distributed over 13 orders. The data is in the openintro package as mammals Using help, report the units for the variable BrainWt. ?mammals Using inspect how many variables are numeric? inspect(mammals) ## ## categorical variables: ## name class levels n missing ## 1 species factor 62 62 0 ## distribution ## 1 Africanelephant (1.6%) ... ## ## quantitative variables: ## name class min Q1 median Q3 max mean ## ...1 body_wt numeric 0.005 0.600 3.3425 48.2025 6654.0 198.789984 ## ...2 brain_wt numeric 0.140 4.250 17.2500 166.0000 5712.0 283.134194 ## ...3 non_dreaming numeric 2.100 6.250 8.3500 11.0000 17.9 8.672917 ## ...4 dreaming numeric 0.000 0.900 1.8000 2.5500 6.6 1.972000 ## ...5 total_sleep numeric 2.600 8.050 10.4500 13.2000 19.9 10.532759 ## ...6 life_span numeric 2.000 6.625 15.1000 27.7500 100.0 19.877586 ## ...7 gestation numeric 12.000 35.750 79.0000 207.5000 645.0 142.353448 ## ...8 predation integer 1.000 2.000 3.0000 4.0000 5.0 2.870968 ## ...9 exposure integer 1.000 1.000 2.0000 4.0000 5.0 2.419355 ## ...10 danger integer 1.000 1.000 2.0000 4.0000 5.0 2.612903 ## sd n missing ## ...1 899.158011 62 0 ## ...2 930.278942 62 0 ## ...3 3.666452 48 14 ## ...4 1.442651 50 12 ## ...5 4.606760 58 4 ## ...6 18.206255 58 4 ## ...7 146.805039 58 4 ## ...8 1.476414 62 0 ## ...9 1.604792 62 0 ## ...10 1.441252 62 0 What type of variable is danger? Categorical Create a histogram of total_sleep and describe the distribution. gf_histogram(~total_sleep,data=mammals,binwidth = 2) gf_dens(~total_sleep,data=mammals) The distribution is unimodal and skewed to the right. It appears it is centered around the value of 11. Create a boxplot of life_span and describe the distribution. gf_boxplot(~life_span,data=mammals) Report the mean and median life span of a mammal. mean(~life_span,data=mammals,na.rm=TRUE) ## [1] 19.87759 median(~life_span,data=mammals,na.rm=TRUE) ## [1] 15.1 Calculate the summary statistics for LifeSpan broken down by Danger. favstats(life_span~danger,data=mammals) ## danger min Q1 median Q3 max mean sd n missing ## 1 1 3.0 7.700 17.60 32.500 100.0 24.20556 23.53829 18 1 ## 2 2 2.3 4.500 10.40 13.000 50.0 12.92308 13.15948 13 1 ## 3 3 2.0 4.175 5.35 7.875 38.6 9.43750 11.99559 8 2 ## 4 4 2.6 9.775 22.10 27.000 69.0 23.11000 18.75482 10 0 ## 5 5 17.0 20.000 23.60 30.000 46.0 26.95556 10.18910 9 0 5.2.2 Problem 2 Mammals life spans Continue using the mammals data set. Create side-by-side boxplots for life_span broken down by exposure. Note: you will have to change exposure to a factor(). Report on any findings. mammals %&gt;% gf_boxplot(life_span~factor(exposure)) Mammals who are more exposed have a longer life span. There must be a confounding variable, maybe the size of the animal or the danger variable. What happened to the median and third quartile in exposure group 4? favstats(life_span~factor(exposure),data=mammals) ## factor(exposure) min Q1 median Q3 max mean sd n missing ## 1 1 2.0 4.35 7.25 14.550 100.0 14.55000 20.98594 24 3 ## 2 2 2.3 6.00 11.20 17.275 50.0 15.39167 14.55819 12 1 ## 3 3 7.6 19.90 26.50 32.000 41.0 25.40000 13.84582 4 0 ## 4 4 7.0 20.20 27.00 27.000 39.3 24.10000 11.78431 5 0 ## 5 5 16.3 20.00 28.00 38.600 69.0 30.53077 14.98084 13 0 The median and third quartile are equal in exposure group 4. There are a large number of the observed mammals with the same life span in this group. Create faceted histograms. What are the shortcomings of this plot? gf_histogram(~life_span,color=~factor(exposure),data=mammals) This is awful. gf_histogram(~life_span|factor(exposure),data=mammals) Not enough data for each histogram; some of the histograms provide little to no information. Let’s do denisty plots. gf_dens(~life_span,color=~factor(exposure),data=mammals) gf_dens(~life_span|factor(exposure),data=mammals) Which do you think is the best graph? Create a new variable exposed that is a factor with level Low if exposure is 1 or 2 and High otherwise. mammals &lt;- mammals %&gt;% mutate(exposed=factor(ifelse((exposure==1)|(exposure==2),&quot;Low&quot;,&quot;High&quot;))) inspect(mammals) ## ## categorical variables: ## name class levels n missing ## 1 species factor 62 62 0 ## 2 exposed factor 2 62 0 ## distribution ## 1 Africanelephant (1.6%) ... ## 2 Low (64.5%), High (35.5%) ## ## quantitative variables: ## name class min Q1 median Q3 max mean ## ...1 body_wt numeric 0.005 0.600 3.3425 48.2025 6654.0 198.789984 ## ...2 brain_wt numeric 0.140 4.250 17.2500 166.0000 5712.0 283.134194 ## ...3 non_dreaming numeric 2.100 6.250 8.3500 11.0000 17.9 8.672917 ## ...4 dreaming numeric 0.000 0.900 1.8000 2.5500 6.6 1.972000 ## ...5 total_sleep numeric 2.600 8.050 10.4500 13.2000 19.9 10.532759 ## ...6 life_span numeric 2.000 6.625 15.1000 27.7500 100.0 19.877586 ## ...7 gestation numeric 12.000 35.750 79.0000 207.5000 645.0 142.353448 ## ...8 predation integer 1.000 2.000 3.0000 4.0000 5.0 2.870968 ## ...9 exposure integer 1.000 1.000 2.0000 4.0000 5.0 2.419355 ## ...10 danger integer 1.000 1.000 2.0000 4.0000 5.0 2.612903 ## sd n missing ## ...1 899.158011 62 0 ## ...2 930.278942 62 0 ## ...3 3.666452 48 14 ## ...4 1.442651 50 12 ## ...5 4.606760 58 4 ## ...6 18.206255 58 4 ## ...7 146.805039 58 4 ## ...8 1.476414 62 0 ## ...9 1.604792 62 0 ## ...10 1.441252 62 0 Repeat part c with the new variable. gf_dens(~life_span,color=~exposed,data=mammals) 5.2.3 Problem 3 Mammals life spans continued Create a scatterplot of life span versus length of gestation. mammals %&gt;% gf_point(life_span~gestation) What type of an association is apparent between life span and length of gestation? It is a weak positive association. What type of an association would you expect to see if the axes of the plot were reversed, i.e. if we plotted length of gestation versus life span? The same as this is observational data there is no reason to beliee is a causal relationship just by looking at the data. Switching the axis will preserve the association. Create the new scatterplot suggested in c.  mammals %&gt;% gf_point(gestation~life_span) Are life span and length of gestation independent? Explain your reasoning. No there is an association and it appears to be linear. If the plot looked like a “shotgun” blast, we would consider the variables to be independent. However, remember there may be confounding variables that could impact the association between these variables. "],["CATDATA.html", "Chapter 6 Categorical Data 6.1 Objectives 6.2 Homework", " Chapter 6 Categorical Data 6.1 Objectives Define and use properly in context all new terminology. Generate in R tables for categorical variable(s). Generate in R appropriate graphical summaries of categorical and numerical variables. Be able to interpret and explain output both graphically and numerically. 6.2 Homework Make sure your plots have a title and the axes are labeled. 6.2.1 Problem 1 Views on immigration 910 randomly sampled registered voters from Tampa, FL were asked if they thought workers who have illegally entered the US should be (i) allowed to keep their jobs and apply for US citizenship, (ii) allowed to keep their jobs as temporary guest workers but not allowed to apply for US citizenship, or (iii) lose their jobs and have to leave the country. The data is in the openintro package in the immigration data object. How many levels of political are there? levels(immigration$political) ## [1] &quot;conservative&quot; &quot;liberal&quot; &quot;moderate&quot; inspect(immigration) ## ## categorical variables: ## name class levels n missing ## 1 response factor 4 910 0 ## 2 political factor 3 910 0 ## distribution ## 1 Leave the country (38.5%) ... ## 2 conservative (40.9%), moderate (39.9%) ... There are three levels for political and they are conservative, liberal, and moderate. Create a table using tally. round(tally(~response+political,data=immigration,format=&quot;percent&quot;,margins = TRUE),2) ## political ## response conservative liberal moderate Total ## Apply for citizenship 6.26 11.10 13.19 30.55 ## Guest worker 13.30 3.08 12.42 28.79 ## Leave the country 19.67 4.95 13.85 38.46 ## Not sure 1.65 0.11 0.44 2.20 ## Total 40.88 19.23 39.89 100.00 What percent of these Tampa, FL voters identify themselves as conservatives? From the table, 40.88% of voters identified themselves as conservatives. What percent of these Tampa, FL voters are in favor of the citizenship option? Again, from the table 30.55% of the voters favor the citizenship option. What percent of these Tampa, FL voters identify themselves as conservatives and are in favor of the citizenship option? From the table, 6.26% of the voters are conservative and favor the citizenship option. What percent of these Tampa, FL voters who identify themselves as conservatives are also in favor of the citizenship option? What percent of moderates and liberal share this view? We need a different table for this question. round(tally(response~political,data=immigration,format=&quot;percent&quot;,margins = TRUE),2) ## political ## response conservative liberal moderate ## Apply for citizenship 15.32 57.71 33.06 ## Guest worker 32.53 16.00 31.13 ## Leave the country 48.12 25.71 34.71 ## Not sure 4.03 0.57 1.10 ## Total 100.00 100.00 100.00 Of the conservative voters, 15.32% are in favor of the citizenship option. The numbers are 57.71% for liberals and 33.06% for moderates. Create a stacked bar chart. immigration %&gt;% gf_props(~political,fill=~response,position=&quot;fill&quot;) %&gt;% gf_labs(title=&quot;Tampa Florida Voter Views on Illegal Immigrant Workers&quot;, subtitle=&quot;Broken down by political views&quot;,x=&quot;Political View&quot;,y=&quot;Proportion&quot;) %&gt;% gf_theme(theme_bw()) Using your plot, do political ideology and views on immigration appear to be independent? Explain your reasoning. The percentages of Tampa, FL conservatives, moderates, and liberals who are in favor of illegal immigrants working in the US staying and applying for citizenship are quite different from one another. Therefore, the two variables appear to be dependent. 6.2.2 Problem 2 Views on the DREAM Act The same survey from Exercise 1 also asked respondents if they support the DREAM Act, a proposed law which would provide a path to citizenship for people brought illegally to the US as children. The data is in the openintro package in the dream data object. Create a mosaic plot. mosaic(stance~ideology,data=dream,sub=&quot;Voter views on illegal worker status&quot;) Based on the mosaic plot, are views on the DREAM Act and political ideology independent? The vertical locations at which the ideological groups break into the Yes, No, and Not Sure categories differ, which indicates the variables are dependent. 6.2.3 Problem 3 Heart transplants The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable transplant indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. Another variable called survived was used to indicate whether or not the patient was alive at the end of the study. The data is in the openintro package and is called heart_transplant. Create a mosaic plot. mosaic(survived~transplant,data=heart_transplant) Based on the mosaic plot, is survival independent of whether or not the patient got a transplant? Explain your reasoning. Proportion of patients who are alive at the end of the study is higher in the treatment group than in the control group. These data suggest that survival is not independent of whether or not the patient got a transplant. Using survtime create side-by-side boxplots for the control and treatment groups. heart_transplant %&gt;% gf_boxplot(survtime~transplant) %&gt;% gf_labs(title=&quot;Survival times for tranplant experiment&quot;, sub=&quot;Treatment group had the transplant&quot;,x=&quot;Tranplant&quot;,y=&quot;Survival time in days&quot;) %&gt;% gf_theme(theme_classic()) What do the box plots suggest about the efficacy (effectiveness) of transplants? The shape of the distribution of survival times in both groups is right skewed with one very clear outlier for the control group and other possible outliers in both groups on the high end. The median survival time for the control group is much lower than the median survival time for the treatment group; patients who got a transplant typically lived longer. Tying this together with the much lower variability in the control group, evident by a much smaller IQR than the treatment group (about 50 days versus 500 days), and we can see that patients who did not get a heart transplant tended to consistently die quite early relative to those who did have a transplant. Overall, very few patients without transplants made it beyond a year while nearly half of the transplant patients survived at least one year. It should also be noted that while the first and third quartiles of the treatment group is higher than those for the control group, the IQR for the treatment group is much bigger, indicating that there is more variability in survival times in the treatment group. "],["CS2.html", "Chapter 7 Case Study 7.1 Objectives 7.2 Homework", " Chapter 7 Case Study 7.1 Objectives Use R to simulate a probabilistic model. Use basic counting methods. 7.2 Homework 7.2.1 Problem 1 Exactly 2 people with the same birthday - Simulation Complete a similar analysis for case where exactly 2 people in a room of 23 people have the same birthday. In this exercise you will use a computational simulation. Create a new R Markdown file and create a report. Yes, we know you could use this file but we want you to practice generating your own report. Simulate having 23 people in the class with each day of the year equally likely. Find the cases where exactly 2 people have the same birthday, you will have to alter the code from the Notes more than changing 18 to 23. Plot the frequency of occurrences as a bar chart. Estimate the probability of exactly two people having the same birthday. (do(10000)*length(unique(sample(days,size=23,replace = TRUE)))) %&gt;% mutate(match=if_else(length==22,1,0)) %&gt;% summarise(prob=mean(match)) ## prob ## 1 0.3579 (do(1000)*length(unique(sample(days,size=23,replace = TRUE)))) %&gt;% gf_bar(~length) 7.2.2 Problem 2 Exactly 2 people with the same birthday - Mathematical Repeat problem 1 but do it mathematically. As a big hint, you will need to use the choose() function. The idea is that with 23 people we need to choose 2 of them to match. We thus need to multiply, the multiplication rule again, by choose(23,2). If you are having trouble, work with a total of 3 people in the room first. Find a formula to determine the exact probability of exactly 2 people in a room of 23 having the same birthday. Generalize your solution to any number n people in the room and create a function. Vectorize the function. Plot the probability of exactly 2 people having the same birthday versus number of people in the room. Comment on the shape of the curve and explain it. knit and compile your report. For two people we have choose(23,2)*prod(365:344)/365^23 ## [1] 0.3634222 exactly_two &lt;- function(n){ choose(n,2)*prod(365:(365-(n-2)))/365^n } exactly_two(23) ## [1] 0.3634222 exactly_two &lt;- Vectorize(exactly_two) gf_line(exactly_two(1:100)~ seq(1,100), xlab=&quot;Number of People&quot;, ylab=&quot;Probability of Match&quot;, title=&quot;Probability of exactly least 2 people with matching birthdays&quot;) By the way, exactly three matches in simulation is hard. We have to table the data set.seed(10) temp &lt;- table(sample(days,size=23,replace = TRUE)) temp ## ## 13 24 50 72 92 110 137 143 154 155 211 231 263 271 285 330 338 342 344 351 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 ## 365 ## 1 (sum(temp==2) == 2)+0 ## [1] 1 (do(10000)*((sum(table(sample(days,size=23,replace = TRUE)) == 3)==1)+0)) %&gt;% summarise(prob=mean(result)) ## prob ## 1 0.0117 Two sets that have same but different birthday (do(10000)*((sum(table(sample(days,size=23,replace = TRUE)) == 2)==2)+0)) %&gt;% summarise(prob=mean(result)) ## prob ## 1 0.1139 (do(10000)*length(unique(sample(days,size=23,replace = TRUE)))) %&gt;% mutate(match=if_else(length==21,1,0)) %&gt;% summarise(prob=mean(match)) ## prob ## 1 0.1187 Mathematically exactly 3 is easy. Simulation seems to be off a little or the math formula is off. choose(23,3)*prod(365:345)/365^23 ## [1] 0.007395218 "],["PROBRULES.html", "Chapter 8 Probability Rules 8.1 Objectives 8.2 Homework", " Chapter 8 Probability Rules 8.1 Objectives Define and use properly in context all new terminology related to probability to include but not limited to: outcome, event, sample space, probability. Apply basic probability and counting rules to find probabilities. Describe the basic axioms of probability. Use R to calculate and simulate probabilities of events. 8.2 Homework 8.2.1 Problem 1 Let \\(A\\), \\(B\\) and \\(C\\) be events such that \\(\\Prob(A)=0.5\\), \\(\\Prob(B)=0.3\\), and \\(\\Prob(C)=0.4\\). Also, we know that \\(\\Prob(A \\cap B)=0.2\\), \\(\\Prob(B \\cap C)=0.12\\), \\(\\Prob(A \\cap C)=0.1\\), and \\(\\Prob(A \\cap B \\cap C)=0.05\\). Find the following: \\(\\Prob(A\\cup B)\\) \\[ \\Prob(A\\cup B) = \\Prob(A)+\\Prob(B)-\\Prob(A\\cap B)= 0.5+0.3-0.2 = 0.6 \\] \\(\\Prob(A\\cup B \\cup C)\\) \\[ \\Prob(A\\cup B \\cup C) = \\Prob(A)+\\Prob(B)+\\Prob(C)-\\Prob(A\\cap B)-\\Prob(A\\cap C)-\\Prob(B\\cap C)+\\Prob(A\\cap B \\cap C) \\] \\[ = 0.5+0.3+0.4-0.2-0.12-0.1+0.05 = 0.83 \\] \\(\\Prob(B&#39;\\cap C&#39;)\\) \\[ \\Prob(B&#39;\\cap C&#39;)=\\Prob((B\\cup C)&#39;) = 1-\\Prob(B\\cup C) = 1-[\\Prob(B)+\\Prob(C)-\\Prob(B\\cap C)] \\] \\[ = 1-(0.3+0.4-0.12) = 0.42 \\] \\(\\Prob(A\\cup (B\\cap C))\\) \\[ \\Prob(A\\cup (B\\cap C)) = \\Prob(A)+\\Prob(B\\cap C) -\\Prob(A\\cap B \\cap C) = 0.5+0.12-0.05 = 0.57 \\] \\(\\Prob((A\\cup B \\cup C)\\cap (A\\cap B \\cap C)&#39;)\\) \\[ \\Prob((A\\cup B \\cup C)\\cap (A\\cap B \\cap C)&#39;)=\\Prob(A\\cup B \\cup C)-\\Prob(A\\cap B \\cap C) = 0.83-0.05 = 0.78 \\] 8.2.2 Problem 2 Consider the example of the family in the reading. What is the probability that the family has at least one boy? \\[ \\Prob(\\mbox{at least one boy})=1-\\Prob(\\mbox{no boys})=1-\\Prob(\\mbox{GGG})=1-\\frac{1}{8} = 0.875 \\] 8.2.3 Problem 3 The Birthday Problem Revisited. Suppose there are \\(n=20\\) students in a classroom. My birthday, the instructor, is April 3rd. What is the probability that at least one student shares my birthday? Assume only 365 days in a year and assume that all birthdays are equally likely. \\[ \\Prob(\\mbox{at least one other person shares my bday})=1-\\Prob(\\mbox{no one else has my bday}) = \\] \\[ 1-\\left( \\frac{364}{365}\\right)^{20} = 0.0534 \\] In R, find the probability that at least one other person shares my birthday for each value of \\(n\\) from 1 to 80. Plot these probabilities with \\(n\\) on the \\(x\\)-axis and probability on the \\(y\\)-axis. At what value of \\(n\\) would the probability be at least 50%? Generalizing, \\[ \\Prob(\\mbox{at least one other person shares my bday})=1-\\Prob(\\mbox{no one else has my bday}) = 1-\\left( \\frac{364}{365}\\right)^{n} \\] n&lt;-1:300 mybday&lt;-function(x) 1-(364/365)^x mybday &lt;- Vectorize(mybday) Check our function. mybday(20) ## [1] 0.05339153 gf_line(mybday(n)~ n, xlab=&quot;Number of People&quot;, ylab=&quot;Probability of Match&quot;, title=&quot;Probability of at least 1 person matching my birthday&quot;) %&gt;% gf_theme(theme_bw) prob &lt;- mybday(n) which(prob&gt;= .5) ## [1] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 ## [20] 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 ## [39] 291 292 293 294 295 296 297 298 299 300 So 253 people. 8.2.4 Problem 4 Thinking of the cards again. Answer the following questions: Define two events that are mutually exclusive. The first card drawn is red. The first card drawn is black. Define two events that are independent. The first card drawn is black. The first card drawn is a face card. Define an event and its complement. The first card drawn is less than 5. The first card drawn is equal to or more than 5. 8.2.5 Problem 5 Consider the license plate example from the reading. What is the probability that a license plate contains exactly one “B?” #fourth spot num4&lt;-10*10*10*1*25*25 #fifth spot num5&lt;-10*10*10*25*1*25 #sixth spot num6&lt;-10*10*10*25*25*1 denom&lt;-10*10*10*26*26*26 (num4+num5+num6)/denom ## [1] 0.1066796 What is the probability that a license plate contains at least one “B?” \\[ 1-\\Prob(\\mbox{no B&#39;s}) \\] num0&lt;-10*10*10*25*25*25 1-num0/denom ## [1] 0.1110036 8.2.6 Problem 6 Consider the party example in the reading. Suppose 8 people showed up to the party dressed as zombies. What is the probability that all three awards are won by people dressed as zombies? \\[ \\frac{8\\cdot 7 \\cdot 6}{25\\cdot 24 \\cdot 23} \\] (8*7*6)/(25*24*23) ## [1] 0.02434783 What is the probability that zombies win “most creative” and “funniest” but not “scariest?” \\[ \\frac{8 \\cdot 17 \\cdot 7}{25 \\cdot 24 \\cdot 23} \\] (8*17*7)/(25*24*23) ## [1] 0.06898551 8.2.7 Problem 7 Consider the cards example from the reading. How many ways can we obtain a “two pairs” (2 of one number, 2 of another, and the final different)? We have to pick the rank of the two pairs. \\[\\binom{13}{2}\\] Notice here the order does matter because a pair of Kings and 4s is the same as a pair of 4s and Kings. This is different from the full house example. Make sure you understand this point. Now we have to pick two of the fours cards for each rank \\[\\binom{4}{2}\\binom{4}{2}\\] And finally we need the last card to come from the 44 remaining cards so that we don’t get a full house. \\(\\binom{44}{1}\\) Putting it all together: \\(\\binom{13}{2}\\binom{4}{2}\\binom{4}{2}\\binom{44}{1}\\) choose(13,2)*choose(4,2)*choose(4,2)*choose(44,1) ## [1] 123552 What is the probability of drawing a “four of a kind” (four cards of the same value)? \\[ \\Prob(\\mbox{4 of a kind})=\\frac{\\binom{13}{1}\\binom{4}{4}\\binom{48}{1}}{\\binom{52}{5}} \\] (13*1*48)/choose(52,5) ## [1] 0.000240096 8.2.8 Problem 8 Advanced Question: Consider rolling 5 dice. What is the probability of a pour resulting in a full house? First pick the value for the three of a kind, there are 6. Then pick the value from the remaining 5 for the two of a kind. This is actually a permutation. There are 30 distinct “flavors” of full house (three 1’s &amp; two 2’s, three 1’s &amp; two 3’s, etc.). In the reading we did this as \\[ \\binom{6}{1} \\times \\binom{5}{1} \\] We now have the 5 dice. We have to select three to have the same value and the order doesn’t matter since they are the same value. Thus we multiple by \\(\\binom{5}{3}\\). Divide this by the total distinct ways the dice could have landed (assuming order matters). \\[ \\Prob(\\mbox{full house}) = \\frac{30 \\times \\frac{5!}{3!2!}}{6^5} \\] \\[ \\Prob(\\mbox{full house}) = \\frac{\\binom{6}{1} \\times \\binom{5}{1} \\times \\binom{5}{3}}{6^5} \\] 30*10/(6^5) ## [1] 0.03858025 Simulating is tough so let’s write some code that may help. set.seed(23) temp&lt;-table(sample(1:6,size=5,replace=TRUE)) temp ## ## 1 3 4 5 ## 1 2 1 1 sum(temp==2) &amp; sum(temp==3) ## [1] FALSE temp&lt;-c(1,1,1,2,2) temp&lt;-table(temp) temp ## temp ## 1 2 ## 3 2 sum(temp==2) &amp; sum(temp==3) ## [1] TRUE Let’s write a function. full_house &lt;-function(x){ temp&lt;-table(x) sum(temp==2) &amp; sum(temp==3) } temp&lt;-c(1,1,1,2,2) full_house(temp) ## [1] TRUE set.seed(751) results&lt;-do(10000)*full_house(sample(1:6,size=5,replace=TRUE)) mean(~full_house,data=results) ## [1] 0.039 "],["CONDPROB.html", "Chapter 9 Conditional Probability 9.1 Objectives 9.2 Homework", " Chapter 9 Conditional Probability 9.1 Objectives Define conditional probability and distinguish it from joint probability. Find a conditional probability using its definition. Using conditional probability, determine whether two events are independent. Apply Bayes’ Rule mathematically and via simulation. 9.2 Homework 9.2.1 Problem 1 Consider Exercise 1 from Lesson 2. Recall: \\(A\\), \\(B\\) and \\(C\\) are events such that \\(\\Prob(A)=0.5\\), \\(\\Prob(B)=0.3\\), \\(\\Prob(C)=0.4\\), \\(\\Prob(A \\cap B)=0.2\\), \\(\\Prob(B \\cap C)=0.12\\), \\(\\Prob(A \\cap C)=0.1\\), and \\(\\Prob(A \\cap B \\cap C)=0.05\\). Are \\(A\\) and \\(B\\) independent? No. \\(\\Prob(A)\\Prob(B)=0.15\\neq \\Prob(A\\cap B)\\). Are \\(B\\) and \\(C\\) independent? Yes. \\(\\Prob(B)\\Prob(C)=0.12 = \\Prob(B\\cap C)\\). Also, \\[ \\Prob(B|C)=\\frac{\\Prob(B\\cap C)}{\\Prob(C)}= 0.12/0.4 = 0.3 =\\Prob(B) \\] 9.2.2 Problem 2 Suppose I have a biased coin (the probability I flip a heads is 0.6). I flip that coin twice. Assume that the coin is memoryless (flips are independent of one another). What is the probability that the second flip results in heads? 0.6 What is the probability that the second flip results in heads, given the first also resulted in heads? The coin is memoryless. So, \\[ \\Prob(\\mbox{2nd flip heads}|\\mbox{1st flip heads}) = 0.6 \\] What is the probability both flips result in heads? Since the flips are independent, \\[ \\Prob(\\mbox{both heads})=\\Prob(\\mbox{1st flip heads})\\Prob(\\mbox{2nd flip heads}) = 0.6*0.6=0.36 \\] What is the probability exactly one coin flip results in heads? This could happen in two ways. The first could be heads OR the second could be heads. \\[ \\Prob(\\mbox{exactly one heads})=\\Prob(\\mbox{1st flip heads})\\Prob(\\mbox{2nd flip tails}) + \\Prob(\\mbox{1st flip tails})\\Prob(\\mbox{2nd flip heads}) \\] \\[ 0.6*0.4+0.4*0.6 = 0.48 \\] Now assume I flip the coin five times. What is the probability the result is 5 heads? \\[ \\Prob(\\mbox{5 heads})= 0.6^5 = 0.0778 \\] 0.6^5 ## [1] 0.07776 What is the probability the result is exactly 2 heads (out of 5 flips)? There are \\(\\binom{5}{2} = 10\\) ways for this to happen ({HHTTT},{HTHTT},…). So, \\[ \\Prob(\\mbox{2 heads out of 5 flips})=\\binom{5}{2} 0.6^2(1-0.6)^3 = 0.2304 \\] choose(5,2)*0.6^2*0.4^3 ## [1] 0.2304 9.2.3 Problem 3 (Adapted from IPSUR, (Kerns 2010)). Suppose there are three assistants working at a company: Moe, Larry and Curly. All three assist with a filing process. Only one filing assistant is needed at a time. Moe assists 60% of the time, Larry assists 30% of the time and Curly assists the remaining 10% of the time. Occasionally, they make errors (misfiles); Moe has a misfile rate of 0.01, Larry has a misfile rate of 0.025, and Curly has a rate of 0.05. Suppose a misfile was discovered, but it is unknown who was on schedule when it occurred. Who is most likely to have committed the misfile? Calculate the probabilities for each of the three assistants. Let \\(E\\) be the event a misfile was committed. Also, let \\(M\\), \\(L\\), and \\(C\\) denote the events that Moe, Larry and Curly was the assistant at the time, respectively. \\[ \\Prob(E)=\\Prob(E \\cap M)+\\Prob(E \\cap L)+\\Prob(E\\cap C) \\] \\[ = \\Prob(E|M)\\Prob(M)+\\Prob(E|L)\\Prob(L)+\\Prob(E|C)\\Prob(C) = 0.01*0.6+0.025*0.3+0.05*0.1 = 0.0185 \\] Thus, \\[ \\Prob(M|E)=\\frac{\\Prob(E \\cap M)}{\\Prob(E)}= \\frac{0.01*0.6}{0.0185}=0.3243 \\] Similarly, \\(\\Prob(L|E)=0.4054\\) and \\(\\Prob(C|E)=0.2702\\). Larry is the assistant most likely to have committed the error. 9.2.4 Problem 4 You are playing a game where there are two coins. One coin is fair and the other comes up heads 80% of the time. One coin is flipped 3 times and the result is three heads, what is the probability that the coin flipped is the fair coin? You will need to make an assumption about the probability of either coin being selected. Use Bayes formula to solve this problem. I will assume either coin is selected with a 50% probability. \\[ \\Prob(Fair) = \\Prob(Biased) = .5 \\] \\[ \\Prob(3 Heads|Fair)=\\frac{1}{2}^3=\\frac{1}{8} \\] \\[ \\Prob(3 Heads|Biased)=.8^3=0.512 \\] Now \\[ \\Prob(Fair | 3 Heads) = \\frac{\\Prob(3 Heads | Fair)\\Prob(Fair)}{\\Prob(3 Heads | Fair)\\Prob(Fair)+\\Prob(3 Heads| Biased)\\Prob(Biased)} \\] Which is \\[ \\Prob(Fair | 3 Heads) = \\frac{\\frac{1}{8}\\frac{1}{2}}{\\frac{1}{8}\\frac{1}{2}+.8^{3}\\frac{1}{2}} = 0.196 \\] .125*.5/(.125*.5+.8^3*.5) ## [1] 0.1962323 Use simulation to solve this problem. Let’s use the same assumptions. We could do this problem in two ways. We could flip each coin a fixed number of times and combine the information or use a random process to pick a flipped coin and then flip it three times. Let’s do the first. Let’s flip a fair coin 50,000 times and count how many heads we get. set.seed(1154) data.frame(do(50000)*rflip(3)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 6157 Now flip the biased coin. data.frame(do(50000)*rflip(3,prob=0.8)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 25743 So we have a total 6157 + 25743 heads of which 6157 came from the fair coin. Thus the probability of the coin being fair given 3 heads on the flips is: 6157/(6157 + 25743) ## [1] 0.1930094 Or 19.3%. Next pick a one of the coins with equal probability 100,000 times. set.seed(501) results &lt;- rflip(100000,summarize = TRUE) results ## n heads tails prob ## 1 1e+05 50226 49774 0.5 Now the fair coin was flipped 50226 times. Let’s see how many times we get 3 heads when we flip that coin 3 times. data.frame(do(50226)*rflip(3)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 6270 We have 6270 cases with 3 heads. Now for the biased coin. data.frame(do(49774)*rflip(3,prob=0.8)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 25512 Now we can determine the probability of a fair coin given 3 heads. 6270/(6270+25512) ## [1] 0.1972815 This code we could easily adapt if we don’t think each coin is being selected with the same frequency. Suppose we think the fair coin has a 75% chance of being selected. The analysis would look like this: set.seed(9021) results &lt;- rflip(100000,prob=.75,summarize = TRUE) results ## n heads tails prob ## 1 1e+05 75023 24977 0.75 Now the fair coin was flipped 75023 times. Let’s see how many times we get 3 heads when we flip that coin 3 times. data.frame(do(75023)*rflip(3)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 9579 We have 9579 cases with 3 heads. Now for the biased coin. data.frame(do(24977)*rflip(3,prob=0.8)) %&gt;% filter(heads==3) %&gt;% summarise(count=n()) %&gt;% pull() ## [1] 12789 Now we can determine the probability of a fair coin given 3 heads. 9579/(9579+12789) ## [1] 0.4282457 A much different answer. That is because prior to getting the data we believed the fair coin would be selected with a 75% probability. The data indicates that we need to update and lower this probability. We only flipped 3 times but the evidence is so in favor of the biased coin, that our probability dropped substantially. This is why Bayes is such a powerful tool. Think about what we just did with this problem. We started with a subjective believe that either coin would be selected with equal probability. This is called the prior probability. We then collected data on three flips of the coin. We used this empirical data to update our belief into a posterior probability. This is the basis for Bayesian statistical analysis. Bayesian statistics is an entire discipline unto itself. References "],["RANDVAR.html", "Chapter 10 Random Variables 10.1 Objectives 10.2 Homework", " Chapter 10 Random Variables 10.1 Objectives Define and use properly in context all new terminology. Given a discrete random variable, obtain the pmf and cdf, and use them to obtain probabilities of events. Simulate random variable for a discrete distribution. Find the moments of a discrete random variable. Find the expected value of a linear transformation of a random variable. 10.2 Homework 10.2.1 Problem 1 Suppose we are flipping a fair coin, and the result of a single coin flip is either heads or tails. Let \\(X\\) be a random variable representing the number of flips until the first heads. Is \\(X\\) discrete or continuous? What is the domain/support of \\(X\\)? \\(X\\) is discrete since number of flips is a discrete process (I can’t perform a fraction of a flip). The wording is specific in that it is the number of flips until the first heads, so we must flip at least once. The domain of \\(X\\) is \\(S_X=\\{1,2,...\\}\\). What values do you expect \\(X\\) to take? What do you think is the average of \\(X\\)? Don’t actually do any formal math, just think about if you were flipping a regular coin, how long it would take you to get the first heads. I would expect \\(X\\) to be 1 or 2 fairly often, since the coin is fair and has an even chance of landing on heads or tails. I would expect large values of \\(X\\) to be rare. For these reasons, I think the average of \\(X\\) should be around 2 flips or a little less than 2. Advanced: In R, generate 10,000 observations from \\(X\\). What is the average value of \\(X\\) based on this simulation? Note: There are many ways to do this. Below is a description of one approach. set.seed(68) which(sample(c(&quot;H&quot;,&quot;T&quot;),1000,replace=TRUE)==&quot;H&quot;)[1] ## [1] 2 Now repeat using replicate() or do(). We will repeat 10000 times. results &lt;- do(10000)*which(sample(c(&quot;H&quot;,&quot;T&quot;),1000,replace=TRUE)==&quot;H&quot;)[1] mean(~result,data=results) ## [1] 1.9849 tally(~result,data=results,format=&quot;percent&quot;) ## result ## 1 2 3 4 5 6 7 8 9 10 11 12 13 ## 49.89 25.35 12.33 6.56 3.25 1.27 0.66 0.39 0.12 0.07 0.06 0.03 0.02 results %&gt;% gf_props(~result,fill=&quot;cyan&quot;,color = &quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Number of flips&quot;, subtitle=&quot;Number of flips until first heads&quot;) As predicted, the mean is close to 2, and the most common values of \\(X\\) are 1 and 2. The most common is 1 occurring 50% of the time, this is what we would think since the coin comes up Heads 50% of the time. We know that \\(\\mbox{P}(X=1) = \\frac{1}{2}\\) and \\(\\mbox{P}(X=2) = \\frac{1}{2^2}\\) so in general \\(\\mbox{P}(X=x) = \\frac{1}{2^x}\\). This is the pmf. As an extra, to show that the sum of the infinite sequence of probabilities is 1 requires some Calculus knowledge. Let’s start with a partial sum: \\[S_n=\\frac{1}{2}+\\frac{1}{4} +\\cdots + \\frac{1}{2^n}\\] Now multiply both sides by \\(\\frac{1}{2}\\). \\[\\frac{1}{2}S_n=\\frac{1}{4}+\\frac{1}{8} +\\cdots + \\frac{1}{2^{n+1}}\\] The difference between these two sums is \\[S_n-\\frac{1}{2}S_n=\\frac{1}{2}S_n=\\frac{1}{2}-\\frac{1}{2^{n+1}}\\] Now as \\[\\lim_{n \\to +\\infty} \\frac{1}{2^{n+1}} = 0\\] So \\[\\lim_{n \\to +\\infty} \\left[ \\frac{1}{2}S_n=\\frac{1}{2}-\\frac{1}{2^{n+1}} \\right]\\] This implies that \\(S = 1\\). 10.2.2 Problem 2 Repeat Problem 1, except part d, but with a different random variable, \\(Y\\): the number of coin flips until the fifth heads. \\(Y\\) is discrete for the same reasons as \\(X\\). The domain of \\(Y\\) is \\(S_Y=\\{5,6,...\\}\\). In order to land on heads five times, it would be reasonable to expect around 9 to 13 flips. Thus, I would expect \\(Y\\) to take values 8, 9, 10, 11, and 12 fairly often, and values outside of that range less often. I think the average of \\(Y\\) should be around 10 or so. set.seed(102) results &lt;- do(10000)*which(sample(c(&quot;H&quot;,&quot;T&quot;),1000,replace=TRUE)==&quot;H&quot;)[5] mean(~result,data=results) ## [1] 9.9728 tally(~result,data=results,format=&quot;percent&quot;) ## result ## 5 6 7 8 9 10 11 12 13 14 15 16 17 ## 3.06 8.21 12.14 13.26 13.71 11.52 10.74 8.17 6.06 4.50 3.02 1.86 1.32 ## 18 19 20 21 22 23 24 25 28 29 ## 0.88 0.65 0.31 0.21 0.16 0.12 0.04 0.04 0.01 0.01 results %&gt;% gf_props(~result,fill=&quot;cyan&quot;,color = &quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Number of flips&quot;, subtitle=&quot;Number of flips until 5th heads&quot;) The most common values of \\(Y\\) are between 6 and 11. The average of \\(Y\\) in this simulation is 9.97, close to what we predicted. The pmf is not that bad but you must know about the binomial distribution first. If we get the fifth heads on the nth flip, the prior n-1 flips are a binomial with n-1 successes. The final flip is a success so we multiply the binomial by the probability of success. 10.2.3 Problem 3 Suppose you are a data analyst for a large international airport. Your boss, the head of the airport, is dismayed that this airport has received negative attention in the press for inefficiencies and sluggishness. In a staff meeting, your boss gives you a week to build a report addressing the “timeliness” at the airport. Your boss is in a big hurry and gives you no further information or guidance on this task. Prior to building the report, you will need to conduct some analysis. To aid you in this, create a list of at least three random variables that will help you address timeliness at the airport. For each of your random variables, Determine whether it is discrete or continuous. Report its domain. What is the experimental unit? Explain how this random variable will be useful in addressing timeliness at the airport. I will provide one example: Let \\(D\\) be the difference between a flight’s actual departure and its scheduled departure. This is a continuous random variable, since time can be measured in fractions of minutes. A flight can be early or late, so domain is any real number. The experimental unit is each individual (non-canceled) flight. This is a useful random variable because the average value of \\(D\\) will describe whether flights take off on time. We could also find out how often \\(D\\) exceeds 0 (implying late departure) or how often \\(D\\) exceeds 30 minutes, which could indicate a “very late” departure. There are many correct answers. \\(X\\): Time it takes for a passenger to go through security (defined as time from entering security line to departing security with all belongings). Continuous. Experimental unit is individual passenger. This variable would help identify whether security line is too long. We could also explore how \\(X\\) changes based on day or time of day. \\(Y\\): Status of each scheduled departure (on time, somewhat late, very late, canceled). Discrete. Experimental unit is each scheduled departure. This variable will help describe how often flights are canceled or late. We could also explore \\(Y\\) by airline, destination, time of day, etc. \\(Z\\): Number of time-related complaints at customer service desk in a given day. Discrete. Experimental unit is day. This variable will describe attitudes/perceptions of customers. It is probably a bad sign if customers feel like the airport is not working efficiently. We can explore how \\(Z\\) changes over time. 10.2.4 Problem 4 Consider the experiment of rolling two fair six-sided dice. Let the random variable \\(Y\\) be the absolute difference between the two numbers that appear upon rolling the dice. What is the domain/support of \\(Y\\)? \\(S_Y=\\{0,1,2,3,4,5\\}\\). What values do you expect \\(Y\\) to take? What do you think is the average of \\(Y\\)? Don’t actually do any formal math, just think about the experiment. I’d say that \\(Y\\) should take values 0,1 and 2 fairly often. I’d guess that the average should be around 1.5. Find the probability mass function and cumulative distribution function of \\(Y\\). Using counting methods, we know there are 36 possible values. We can just count them. The number 0 will occur when both numbers are the same, which happens six times. The number 1 happens when the first die is one larger than the second, 5 times, or vice versa. Thus 1 happens 10 times. Continue this process. Thus, the pmf of \\(Y\\) becomes: \\[ f_Y(y)=\\left\\{ \\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} \\frac{6}{36}, &amp; y=0 \\\\ \\frac{10}{36}, &amp; y=1 \\\\ \\frac{8}{36}, &amp; y=2 \\\\ \\frac{6}{36}, &amp; y=3 \\\\ \\frac{4}{36}, &amp; y=4 \\\\ \\frac{2}{36}, &amp; y=5 \\\\ 0, &amp; \\mbox{otherwise} \\end{array} \\right . \\] We could also create a table and count the entries. \\[ \\begin{array}{cc|cccccc} &amp; &amp; &amp; &amp;\\textbf{Die} &amp; \\textbf{2} \\\\ &amp; &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\\\&amp;\\hline 1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\\\textbf{Die 1} &amp; 2 &amp; 1 &amp; 0 &amp; 1 &amp; 2 &amp;3 &amp; 4 \\\\&amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 \\\\&amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 1 &amp; 2 \\\\&amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 &amp; 1 \\\\&amp; 6 &amp; 5 &amp; 4 &amp; 3 &amp; 2 &amp; 1 &amp; 0 \\end{array} \\] The cdf of \\(Y\\) is thus, \\[ F_Y(y)=\\left\\{\\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} 0, &amp; y &lt; 0 \\\\ \\frac{6}{36}, &amp; 0\\leq y &lt;1 \\\\ \\frac{16}{36}, &amp; 1\\leq y &lt;2 \\\\ \\frac{24}{36}, &amp; 2 \\leq y &lt;3 \\\\ \\frac{30}{36}, &amp; 3 \\leq y &lt;4 \\\\ \\frac{34}{36}, &amp; 4 \\leq y &lt;5 \\\\ \\frac{36}{36}, &amp; y\\geq 5 \\end{array} \\right . \\] Find the expected value and variance of \\(Y\\). \\[ \\mbox{E}(Y)=\\sum_{y=0}^5 y\\mbox{P}(Y=y) = 0\\times {6\\over 36} + 1 \\times {10\\over 36} + 2\\times {8\\over 36} + 3\\times {6\\over 36} + 4 \\times {4\\over 36} + 5 \\times {2\\over 36} = \\] \\[ {70\\over 36} = 1.944 \\] y&lt;-c(0,1,2,3,4,5) mean_y&lt;-sum(y*c(6,10,8,6,4,2)/36) mean_y ## [1] 1.944444 The variance is: sum((y-mean_y)^2*(c(6,10,8,6,4,2)/36)) ## [1] 2.052469 Advanced: In R, obtain 10,000 realizations of \\(Y\\). In other words, simulate the roll of two fair dice, record the absolute difference and repeat this 10,000 times. Construct a frequency table of your results (what percentage of time did you get a difference of 0? difference of 1? etc.) Find the mean and variance of your simulated sample of \\(Y\\). Were they close to your answers in part d? set.seed(9) sim_diffs&lt;-do(10000)*abs(diff(sample(1:6,2,replace=T))) tally(~abs,data=sim_diffs,format=&quot;proportion&quot;) ## abs ## 0 1 2 3 4 5 ## 0.1643 0.2752 0.2273 0.1618 0.1116 0.0598 mean(~abs,data=sim_diffs) ## [1] 1.9606 var(sim_diffs)*9999/10000 ## abs ## abs 2.077248 true_mean&lt;-sum(c(6,10,8,6,4,2)/36*c(0,1,2,3,4,5)) true_mean ## [1] 1.944444 sum(c(6,10,8,6,4,2)/36*(c(0,1,2,3,4,5)-true_mean)^2) ## [1] 2.052469 We got similar mean and variance to the theoretical values. 10.2.5 Problem 5 Prove the Lemma from the Notes: Let \\(X\\) be a discrete random variable, and let \\(a\\) and \\(b\\) be constants. Show \\(\\mbox{E}(aX + b)=a\\mbox{E}(X)+b\\). \\[ \\mbox{E}(aX+b)=\\sum_x (ax+b)f_X(x) = \\sum_x axf_X(x)+\\sum_x bf_X(x) + a\\sum_x xf_X(x)+b\\sum_x f_X(x) \\] Since \\(\\sum_x xf_X(x) = \\mbox{E}(X)\\) and \\(\\sum_x f_X(x)=1\\), this reduces to \\(a\\mbox{E}(X)+b\\). \\[ \\mbox{Var}(aX+b)=\\mbox{E}\\left[(aX+b-\\mbox{E}(aX+b))^2\\right]=\\mbox{E}\\left[(aX+b-a\\mbox{E}(X)-b)^2\\right]=\\mbox{E}\\left[(aX-a\\mbox{E}(X)^2\\right] \\] \\[ =\\mbox{E}\\left[a^2(X-\\mbox{E}(X))^2\\right]=a^2\\mbox{E}\\left[(X-\\mbox{E}(X))^2\\right]=a^2\\mbox{Var}(X) \\] 10.2.6 Problem 6 In the Notes, we saw that \\(\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]\\). Show that \\(\\mbox{Var}(X)\\) is also equal to \\(\\mbox{E}(X^2)-[\\mbox{E}(X)]^2\\). \\[ \\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]=\\mbox{E}[X^2-2\\mu_XX+\\mu_X^2] = \\mbox{E}(X^2)-\\mbox{E}(2\\mu_XX)+\\mbox{E}(\\mu_X^2) \\] The quantity \\(\\mu_X\\) is a constant with respect to \\(X\\), so \\[ =\\mbox{E}(X^2)-2\\mu_X\\mbox{E}(X)+\\mu_X^2=\\mbox{E}(X^2)-2\\mu_X^2+\\mu_X^2 = \\mbox{E}(X^2)-\\mu_X^2 \\] "],["CONRANDVAR.html", "Chapter 11 Continuous Random Variables 11.1 Objectives 11.2 Homework", " Chapter 11 Continuous Random Variables 11.1 Objectives Define and properly use the new terms to include probability density function (pdf) and cumulative distribution function (cdf) for continuous random variables. Given a continuous random variable, find probabilities using the pdf and/or the cdf. Find the mean and variance of a continuous random variable. 11.2 Homework 11.2.1 Problem 1 Let \\(X\\) be a continuous random variable on the domain \\(-k \\leq X \\leq k\\). Also, let \\(f(x)=\\frac{x^2}{18}\\). Assume that \\(f(x)\\) is a valid pdf. Find the value of \\(k\\). Because \\(f\\) is a valid pdf, we know that \\(\\int_{-k}^k \\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = 1\\). So, \\[ \\int_{-k}^k \\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = \\frac{x^3}{54}\\bigg|_{-k}^k = \\frac{k^3}{54}-\\frac{-k^3}{54}=\\frac{k^3}{27}=1 \\] Thus, \\(k=3\\). Using R, see if you can follow the code. my_pdf &lt;- function(x)integrate(function(y)y^2/18,-x,x)$value my_pdf&lt;-Vectorize(my_pdf) domain &lt;- seq(.01,5,.1) gf_line(my_pdf(domain)~domain) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Cumulative probability for different values of k&quot;,x=&quot;k&quot;,y=&quot;Cummulative Probability&quot;) %&gt;% gf_hline(yintercept = 1,color = &quot;blue&quot;) Looks like \\(k \\approx 3\\) from the plot. uniroot(function(x)my_pdf(x)-1,c(-10,10))$root ## [1] 2.999997 Plot the pdf of \\(X\\). x&lt;-seq(-3,3,0.001) fx&lt;-x^2/18 gf_line(fx~x,ylab=&quot;f(x)&quot;,title=&quot;pdf of X&quot;) %&gt;% gf_theme(theme_classic()) ggplot(data.frame(x=c(-3, 3)), aes(x)) + stat_function(fun=function(x) x^2/18) + theme_classic() + labs(y=&quot;f(x)&quot;,title=&quot;pdf of X&quot;) curve(x^2/18,from=-3,to=3,ylab=&quot;f(x)&quot;,main=&quot;pdf of X&quot;) Find and plot the cdf of \\(X\\). \\[ F_X(x)=\\mbox{P}(X\\leq x)=\\int_{-3}^x \\frac{t^2}{18}\\mathop{}\\!\\mathrm{d}t = \\frac{t^3}{54}\\bigg|_{-3}^x = \\frac{x^3}{54}+\\frac{1}{2} \\] \\[ F_X(x)=\\left\\{\\begin{array}{ll} 0, &amp; x&lt;-3 \\\\ \\frac{x^3}{54}+\\frac{1}{2}, &amp; -3\\leq x \\leq 3 \\\\ 1, &amp; x&gt;3 \\end{array}\\right. \\] x&lt;-seq(-3.5,3.5,0.001) fx&lt;-pmin(1,(1*(x&gt;=-3)*(x^3/54+1/2))) gf_line(fx~x,ylab=&quot;F(x)&quot;,title=&quot;cdf of X&quot;) %&gt;% gf_theme(theme_classic()) Find \\(\\mbox{P}(X&lt;1)\\). \\[ \\mbox{P}(X&lt;1)=F(1)=\\frac{1}{54}+\\frac{1}{2}=0.519 \\] integrate(function(x)x^2/18,-3,1) ## 0.5185185 with absolute error &lt; 5.8e-15 Find \\(\\mbox{P}(1.5&lt;X\\leq 2.5)\\). \\[ \\mbox{P}(1.5&lt; X \\leq 2.5)=F(2.5)-F(1.5)=\\frac{2.5^3}{54}+\\frac{1}{2}-\\frac{1.5^3}{54}-\\frac{1}{2}=0.227 \\] integrate(function(x)x^2/18,1.5,2.5) ## 0.2268519 with absolute error &lt; 2.5e-15 Find the 80th percentile of \\(X\\) (the value \\(x\\) for which 80% of the distribution is to the left of that value). Need \\(x\\) such that \\(F(x)=0.8\\). Solving \\(\\frac{x^3}{54}+\\frac{1}{2}=0.8\\) for \\(x\\) yields \\(x=2.530\\). uniroot(function(x)x^3/54+.5-.8,c(-3,3)) ## $root ## [1] 2.530293 ## ## $f.root ## [1] -1.854422e-06 ## ## $iter ## [1] 6 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 Find the value \\(x\\) such that \\(\\mbox{P}(-x \\leq X \\leq x)=0.4\\). Because this distribution is symmetric, finding \\(x\\) is equivalent to finding \\(x\\) such that \\(\\mbox{P}(X&gt;x)=0.3\\). (It helps to draw a picture). Thus, we need \\(x\\) such that \\(F(x)=0.7\\). Solving \\(\\frac{x^3}{54}+\\frac{1}{2}=0.7\\) for \\(x\\) yields \\(x=2.210\\). Find the mean and variance of \\(X\\). \\[ \\mbox{E}(X)=\\int_{-3}^3 x\\cdot\\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = \\frac{x^4}{72}\\bigg|_{-3}^3=\\frac{81}{72}-\\frac{81}{72} = 0 \\] \\[ \\mbox{E}(X^2)=\\int_{-3}^3 x^2\\cdot\\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = \\frac{x^5}{90}\\bigg|_{-3}^3=\\frac{243}{90}-\\frac{-243}{90} = 5.4 \\] \\[ \\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2=5.4-0^2=5.4 \\] Simulate 10000 values from this distribution and plot the density. This is tricky since we need a cube root function. Just raising to the one-third power won’t work. Let’s write our own function. cuberoot &lt;- function(x) { sign(x) * abs(x)^(1/3)} set.seed(4) results &lt;- do(10000)*cuberoot((runif(1)-.5)*54) results %&gt;% gf_dens(~cuberoot) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;pdf from simulation&quot;,x=&quot;x&quot;,y=&quot;f(x)&quot;) Notice that the smoothing operation goes past the support of \\(X\\) and thus shows a concave down curve. We could clean up by limiting the x-axis to the interval [-3,3]. inspect(results) ## ## quantitative variables: ## name class min Q1 median Q3 max ## ...1 cuberoot numeric -2.999981 -2.382864 -0.1574198 2.376346 2.999347 ## mean sd n missing ## ...1 -0.002416475 2.322639 10000 0 11.2.2 Problem 2 Let \\(X\\) be a continuous random variable. Prove that the cdf of \\(X\\), \\(F_X(x)\\) is a non-decreasing function. (Hint: show that for any \\(a &lt; b\\), \\(F_X(a) \\leq F_X(b)\\).) Let \\(a&lt;b\\), where \\(a\\) and \\(b\\) are both in the domain of \\(X\\). Note that \\(F_X(a)=\\mbox{P}(X\\leq a)\\) and \\(F_X(b)=\\mbox{P}(X\\leq b)\\). Since \\(a&lt;b\\), we can partition \\(\\mbox{P}(X\\leq b)\\) as \\(\\mbox{P}(X\\leq a)+\\mbox{P}(a &lt; X \\leq b)\\). One of the axioms of probability is that a probability must be non-negative, so I know that \\(\\mbox{P}(a &lt; X \\leq b)\\geq 0\\). Thus, \\[ \\mbox{P}(X\\leq b)=\\mbox{P}(X\\leq a)+\\mbox{P}(a &lt; X \\leq b) \\geq \\mbox{P}(X\\leq a) \\] So, we have shown that \\(F_X(a)\\leq F_X(b)\\). Thus, \\(F_X(x)\\) is a non-decreasing function. "],["DISCRETENAMED.html", "Chapter 12 Named Discrete Distributions 12.1 Objectives 12.2 Homework", " Chapter 12 Named Discrete Distributions 12.1 Objectives Recognize and setup for use common discrete distributions (Uniform, Binomial, Poisson, Hypergeometric) to include parameters, assumptions, and moments. Use R to calculate probabilities and quantiles involving random variables with common discrete distributions. 12.2 Homework For each of the problems below, 1) define a random variable that will help you answer the question, 2) state the distribution and parameters of that random variable; 3) determine the expected value and variance of that random variable, and 4) use that random variable to answer the question. We will demonstrate using 1a and 1b. 12.2.1 Problem 1 The T-6 training aircraft is used during UPT. Suppose that on each training sortie, aircraft return with a maintenance-related failure at a rate of 1 per 100 sorties. Find the probability of no maintenance failures in 15 sorties. \\(X\\): the number of maintenance failures in 15 sorties. \\(X\\sim \\textsf{Bin}(n=15,p=0.01)\\) \\(\\mbox{E}(X)=15*0.01=0.15\\) and \\(\\mbox{Var}(X)=15*0.01*0.99=0.1485\\). \\(\\mbox{P}(\\mbox{No maintenance failures})=\\mbox{P}(X=0)={15\\choose 0}0.01^0(1-0.01)^{15}=0.99^{15}\\) 0.99^15 ## [1] 0.8600584 ## or dbinom(0,15,0.01) ## [1] 0.8600584 This probability makes sense, since the expected value is fairly low. Because, on average, only 0.15 failures would occur every 15 trials, 0 failures would be a very common result. Graphically, the pmf looks like this: gf_dist(&quot;binom&quot;,size=15,prob=0.01) %&gt;% gf_theme(theme_classic()) Find the probability of at least two maintenance failures in 15 sorties. We can use the same \\(X\\) as above. Now, we are looking for \\(\\mbox{P}(X\\geq 2)\\). This is equivalent to finding \\(1-\\mbox{P}(X\\leq 1)\\): ## Directly 1-(0.99^15 + 15*0.01*0.99^14) ## [1] 0.009629773 ## or, using R sum(dbinom(2:15,15,0.01)) ## [1] 0.009629773 ## or 1-sum(dbinom(0:1,15,0.01)) ## [1] 0.009629773 ## or 1-pbinom(1,15,0.01) ## [1] 0.009629773 ## or pbinom(1,15,0.01,lower.tail = F) ## [1] 0.009629773 Find the probability of at least 30 successful (no mx failures) sorties before the first failure. \\(X\\): the number of maintenance failures out of 30 sorties. \\(X\\sim \\textsf{Binom}(n=30,p=0.01)\\), and \\(\\mbox{E}(X)=0.3\\) and \\(\\mbox{Var}(X)=0.297\\). \\(\\mbox{P}(\\mbox{0 failures})=\\mbox{P}(X=0)=0.99^{30}\\) 0.99^30 ## [1] 0.7397004 ##or dbinom(0,30,0.01) ## [1] 0.7397004 Using negative binomial, which was not in the reading but you can research: \\(Y\\): the number of successful sorties before the first failure. \\(Y\\sim \\textsf{NegBin}(n=1,p=0.01)\\), and \\(\\mbox{E}(X)=99\\) and \\(\\mbox{Var}(X)=9900\\). \\(\\mbox{P}(\\mbox{at least 30 successes before first failure})=\\mbox{P}(Y\\geq 30)\\) 1-pnbinom(29,1,0.01) ## [1] 0.7397004 Find the probability of at least 50 successful sorties before the third failure. Using a binomial random variable, we have 52 trials and need at least 50 to be a success. The random variable is \\(X\\) the number of successful sorties out of 52. 1-pbinom(49,52,.99) ## [1] 0.9846474 Or using a negative binomial, let \\(Y\\): the number of successful sorties before the third failure. \\(Y\\sim \\textsf{NegBin}(n=3,p=0.01)\\), and \\(\\mbox{E}(X)=297\\) and \\(\\mbox{Var}(X)=29700\\). \\(\\mbox{P}(\\mbox{at least 50 successes before 3rd failure})=\\mbox{P}(Y\\geq 50)\\) 1-pnbinom(49,3,0.01) ## [1] 0.9846474 Notice if the question had been exactly 50 successful sorties before the 3 failure, that is a different question. Then we could use either: dbinom(50,52,.99)*.01 ## [1] 0.000802238 The \\(0.01\\) is because the last trial is a failure. Or dnbinom(50,3,0.01) ## [1] 0.000802238 12.2.2 Problem 2 On a given Saturday, suppose vehicles arrive at the USAFA North Gate according to a Poisson process at a rate of 40 arrivals per hour. Find the probability no vehicles arrive in 10 minutes. \\(X\\): number of vehicles that arrive in 10 minutes \\(X\\sim \\textsf{Pois}(\\lambda=40/6=6.67)\\) and \\(\\mbox{E}(X)=\\mbox{Var}(X)=6.67\\). \\(\\mbox{P}(\\mbox{no arrivals in 10 minutes})=\\mbox{P}(X=0)=\\frac{6.67^0 e^{-6.67}}{0!}=e^{-6.67}\\) exp(-40/6) ## [1] 0.001272634 ##or dpois(0,40/6) ## [1] 0.001272634 Find the probability at least 50 vehicles arrive in an hour. \\(X\\): number of vehicles that arrive in an hour \\(X\\sim \\textsf{Pois}(\\lambda=40)\\) and \\(\\mbox{E}(X)=\\mbox{Var}(X)=40\\). \\(\\mbox{P}(\\mbox{at least 50 arrivals in 1 hour})=\\mbox{P}(X\\geq 50)\\) 1-ppois(49,40) ## [1] 0.07033507 Find the probability that at least 5 minutes will pass before the next arrival. \\(X\\): number of vehicles that arrive in 5 minutes \\(X\\sim \\textsf{Pois}(\\lambda=40/12=3.33)\\) and \\(\\mbox{E}(X)=\\mbox{Var}(X)=3.33\\). \\(\\mbox{P}(\\mbox{no arrivals in 5 minutes})=\\mbox{P}(X=0)=\\frac{3.33^0 e^{-3.33}}{0!}=e^{-3.33}\\) exp(-40/12) ## [1] 0.03567399 ##or dpois(0,40/12) ## [1] 0.03567399 12.2.3 Problem 3 Suppose there are 12 male and 7 female cadets in a classroom. I select 5 completely at random (without replacement). Find the probability I select no female cadets. \\(X\\): number of female cadets selected out of sample of size 5 \\(X\\sim \\textsf{Hypergeom}(m=7,n=12,k=5)\\) and \\(\\mbox{E}(X)=1.842\\) and \\(\\mbox{Var}(X)=0.905\\). \\[ \\mbox{P}(\\mbox{no female cadets selected})=\\mbox{P}(X=0)=\\frac{{7\\choose 0}{12\\choose 5}}{{19\\choose 5}} \\] choose(12,5)/choose(19,5) ## [1] 0.06811146 ##or dhyper(0,7,12,5) ## [1] 0.06811146 Find the probability I select more than 2 female cadets. Using the same random variable: \\[ \\mbox{P}(\\mbox{more than 2 female})=\\mbox{P}(X&gt;2)=1-\\mbox{P}(X\\leq 2) \\] 1-phyper(2,7,12,5) ## [1] 0.2365841 ##or sum(dhyper(3:5,7,12,5)) ## [1] 0.2365841 "],["CONTNNAMED.html", "Chapter 13 Named Continuous Distributions 13.1 Objectives 13.2 Homework", " Chapter 13 Named Continuous Distributions 13.1 Objectives Recognize when to use common continuous distributions (Uniform, Exponential, Gamma, Normal, Weibull, and Beta), identify parameters, and find moments. Use R to calculate probabilities and quantiles involving random variables with common continuous distributions. Understand the relationship between the Poisson process and the Poisson &amp; Exponential distributions. Know when to apply and then use the memoryless property. 13.2 Homework For problems 1-3 below, 1) define a random variable that will help you answer the question, 2) state the distribution and parameters of that random variable; 3) determine the expected value and variance of that random variable, and 4) use that random variable to answer the question. 13.2.1 Problem 1 On a given Saturday, suppose vehicles arrive at the USAFA North Gate according to a Poisson process at a rate of 40 arrivals per hour. Find the probability no vehicles arrive in 10 minutes. \\(X\\): number of vehicles that arrive in 10 minutes \\(X\\sim \\textsf{Pois}(\\lambda=40/6=6.67)\\) and \\(\\mbox{E}(X)=\\mbox{Var}(X)=6.67\\). \\(\\mbox{P}(\\mbox{no arrivals in 10 minutes})=\\mbox{P}(X=0)=\\frac{6.67^0 e^{-6.67}}{0!}=e^{-6.67}\\) exp(-40/6) ## [1] 0.001272634 ##or dpois(0,40/6) ## [1] 0.001272634 or, using the exponential distribution: \\(Y\\): time in minutes until the next arrival \\(Y\\sim \\textsf{Expon}(\\lambda=40/60=0.667)\\) and \\(\\mbox{E}(Y)=1.5\\) and \\(\\mbox{Var}(Y)=2.25\\). \\[ \\mbox{P}(\\mbox{at least 10 minutes until the next arrival})=\\mbox{P}(Y\\geq 10)=\\int_{10}^\\infty \\frac{2}{3}e^{-\\frac{2}{3}y}\\mathop{}\\!\\mathrm{d}y \\] 1-pexp(10,2/3) ## [1] 0.001272634 or using simulation: set.seed(616) mean(rpois(100000,40/6) == 0) ## [1] 0.00126 mean(rexp(100000,2/3) &gt;=10) ## [1] 0.00127 Find the probability that at least 5 minutes will pass before the next arrival. \\(Y\\): same as in part a \\[ \\mbox{P}(\\mbox{at least 5 minutes until next arrival})=\\mbox{P}(Y\\geq 5)=\\int_{5}^\\infty \\frac{2}{3}e^{-\\frac{2}{3}y}\\mathop{}\\!\\mathrm{d}y \\] 1-pexp(5,2/3) ## [1] 0.03567399 Find the probability that the next vehicle will arrive between 2 and 10 minutes from now. Same \\(Y\\) as defined above. pexp(10,2/3)-pexp(2,2/3) ## [1] 0.2623245 Find the probability that at least 7 minutes will pass before the next arrival, given that 2 minutes have already passed. Compare this answer to part (b). This is an example of the memoryless property of the exponential distribution. \\[ \\mbox{P}(Y\\geq 7|Y\\geq 2) = \\frac{\\mbox{P}(Y\\geq 7, Y\\geq 2)}{\\mbox{P}(Y\\geq 2)} = \\frac{\\mbox{P}(Y\\geq 7)}{\\mbox{P}(Y\\geq 2)} \\] (1-pexp(7,2/3))/(1-pexp(2,2/3)) ## [1] 0.03567399 This is the same answer and a result of the memoryless property. Fill in the blank. There is a probability of 90% that the next vehicle will arrive within __ minutes. This value is known as the 90% percentile of the random variable. qexp(0.9,2/3) ## [1] 3.453878 Use the function stripplot() to visualize the arrival of 30 vehicles using a random sample from the appropriate exponential distribution. set.seed(202) stripplot(cumsum(rexp(30,2/3)),xlab=&quot;Arrival Time&quot;) 13.2.2 Problem 2 Suppose time until computer errors on the F-35 follows a Gamma distribution with mean 20 hours and variance 10. Find the probability that 20 hours pass without a computer error. \\(X\\): time in hours until next computer error. \\(X\\sim \\textsf{Gamma}(\\alpha = 40, \\lambda = 2)\\) We need to find \\(\\alpha\\) and \\(\\lambda\\) from the given moments. \\(\\mbox{E}(X) = 20 = \\frac{\\alpha}{\\lambda}\\) \\(\\mbox{Var}(X) = 10 = \\frac{\\alpha}{\\lambda^2}\\) Notice that \\(\\frac{\\mbox{E}(X)}{\\mbox{Var}(X)} = \\lambda = \\frac{20}{10}=2\\) and then using \\(\\mbox{E}(X) = 20 = \\frac{\\alpha}{\\lambda}\\) we get \\(\\alpha = 40\\). \\(\\mbox{P}(X\\geq 20)\\): 1-pgamma(20,shape=40,rate=2) ## [1] 0.4789711 Find the probability that 45 hours pass without a computer error, given that 25 hours have already passed. Does the memoryless property apply to the Gamma distribution? \\[ P(X\\geq 45|X\\geq 25) = \\frac{P(X\\geq 45, X\\geq 25)}{P(X\\geq 25)} = \\frac{P(X\\geq 45)}{P(X\\geq 25)} \\] (1-pgamma(45,40,2))/(1-pgamma(25,40,2)) ## [1] 1.77803e-08 No, the memoryless property does not apply to the Gamma distribution. Find \\(a\\) and \\(b\\) where there is a 95% probability that the time until next computer error will be between \\(a\\) and \\(b\\). (Note: technically, there are many answers to this question, but find \\(a\\) and \\(b\\) such that each tail has equal probability.) qgamma(c(0.025,0.975),40,2) ## [1] 14.28829 26.65714 So in the time interval \\([14,29,26.66]\\). qgamma(.95,40,2) ## [1] 25.46987 Another answer is between \\([0,25,47]\\). 13.2.3 Problem 3 Suppose PFT scores in the cadet wing follow a normal distribution with mean 330 and standard deviation 50. Find the probability a randomly selected cadet has a PFT score higher than 450. \\(X\\): PFT score of a randomly selected cadet \\(X\\sim \\textsf{Norm}(\\mu=330,\\sigma=50)\\) \\(\\mbox{E}(X) = 330\\) and \\(\\mbox{Var}(X)=50^2=2500\\). 1-pnorm(450,330,50) ## [1] 0.008197536 Find the probability a randomly selected cadet has a PFT score within 2 standard deviations of the mean. Need \\(\\mbox{P}(230 \\leq X \\leq 430)\\). pnorm(430,330,50)-pnorm(230,330,50) ## [1] 0.9544997 Find \\(a\\) and \\(b\\) such that 90% of PFT scores will be between \\(a\\) and \\(b\\). Need \\(a\\) such that \\(\\mbox{P}(X\\leq a)=0.05\\) and \\(b\\) such that \\(\\mbox{P}(X\\geq b)=0.05\\): qnorm(0.05,330,50) ## [1] 247.7573 qnorm(0.95,330,50) ## [1] 412.2427 Find the probability a randomly selected cadet has a PFT score higher than 450 given he/she is among the top 10% of cadets. Need \\(\\mbox{P}(X&gt;450|X&gt;x_{0.9})\\) where \\(x_{0.9}\\) is the 90th percentile of \\(X\\). The 90th percentile is: qnorm(0.9,330,50) ## [1] 394.0776 \\[ \\mbox{P}(X&gt;450|X&gt;x_{0.9})=\\frac{\\mbox{P}(X&gt;450, X&gt;x_{0.9})}{\\mbox{P}(X&gt;x_{0.9})}=\\frac{\\mbox{P}(X&gt;450, X&gt;394.08)}{\\mbox{P}(X&gt;x_{0.9})}=\\frac{\\mbox{P}(X&gt;450)}{0.1} \\] This is assuming that \\(x_{0.9}&lt;450\\). Otherwise the problem is trivial and the probability is 1. (1-pnorm(450,330,50))/0.1 ## [1] 0.08197536 13.2.4 Problem 4 Let \\(X \\sim \\textsf{Beta}(\\alpha=1,\\beta=1)\\). Show that \\(X\\sim \\textsf{Unif}(0,1)\\). Hint: write out the beta distribution pdf where \\(\\alpha=1\\) and \\(\\beta=1\\). The beta pdf is: \\[ f_X(x)=\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} \\] When \\(X\\sim\\textsf{Beta}(\\alpha=1,\\beta=1)\\), this becomes: \\[ f_X(x)=\\frac{\\Gamma(2)}{\\Gamma(1)\\Gamma(1)}x^{1-1}(1-x)^{1-1} = 1 \\] 13.2.5 Problem 5 When using R to calculate probabilities related to the gamma distribution, we often use pgamma. Recall that pgamma is equivalent to the cdf of the gamma distribution. If \\(X\\sim\\textsf{Gamma}(\\alpha,\\lambda)\\), then \\[ \\mbox{P}(X\\leq x)=\\textsf{pgamma(x,alpha,lambda)} \\] The dgamma function exists in R too. In plain language, explain what dgamma returns. I’m not looking for the definition found in R documentation. I’m looking for a simple description of what that function returns. Is the output of dgamma useful? If so, how? The dgamma function returns the value of probability density function. While this is not a probability, it is still a useful quantity. It can be said that larger densities (\\(f(x)\\)) imply that values near \\(x\\) are more likely to occur than values associated with smaller densities. It is also useful when computing conditional probability distributions. 13.2.6 Problem 6 Advanced. You may have heard of the 68-95-99.7 rule. This is a helpful rule of thumb that says if a population has a normal distribution, then 68% of the data will be within one standard deviation of the mean, 95% of the data will be within two standard deviations and 99.7% of the data will be within three standard deviations. Create a function in R that has two inputs (a mean and a standard deviation). It should return a vector with three elements: the probability that a randomly selected observation from the normal distribution with the inputted mean and standard deviation lies within one, two and three standard deviations. Test this function with several values of mu and sd. You should get the same answer each time. rulethumb&lt;-function(mu,sd){ pnorm(mu+c(1,2,3)*sd,mu,sd)-pnorm(mu-c(1,2,3)*sd,mu,sd) } rulethumb(15,12) ## [1] 0.6826895 0.9544997 0.9973002 rulethumb(0,1) ## [1] 0.6826895 0.9544997 0.9973002 13.2.7 Problem 7 Derive the mean of a general uniform distribution, \\(U(a,b)\\). From the definition \\[E(X)=\\int_{a}^{b}xf(x)dx=\\] \\[ =\\int_{a}^{b}\\frac{x}{b-a}dx =\\] \\[ =\\frac{1}{b-a}\\int_{a}^{b}xdx = \\frac{1}{b-a}\\cdot\\frac{x^2}{2}\\bigg|_{a}^{b}=\\] \\[ =\\frac{1}{b-a}\\cdot\\frac{b^2-a^2}{2}= \\frac{1}{b-a}\\cdot\\frac{(b-a)(b+a)}{2}=\\frac{(b+a)}{2}\\] "],["MULTIDISTS.html", "Chapter 14 Multivariate Distributions 14.1 Objectives 14.2 Homework", " Chapter 14 Multivariate Distributions 14.1 Objectives Define (and distinguish between) the terms joint probability mass/density function, marginal pmf/pdf, and conditional pmf/pdf. Given a joint pmf/pdf, obtain the marginal and conditional pmfs/pdfs. Use joint, marginal and conditional pmfs/pdfs to obtain probabilities. 14.2 Homework 14.2.1 Problem 1 Let \\(X\\) and \\(Y\\) be continuous random variables with joint pdf: \\[ f_{X,Y}(x,y)=x + y \\] where \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 1\\). Verify that \\(f\\) is a valid pdf. \\[ \\int_0^1\\int_0^1 x+y\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = \\int_0^1 xy + \\frac{y^2}{2}\\bigg|_0^1 \\mathop{}\\!\\mathrm{d}x = \\int_0^1 x+\\frac{1}{2}\\mathop{}\\!\\mathrm{d}x = \\frac{x^2}{2}+\\frac{x}{2}\\bigg|_0^1=1 \\] Or library(cubature) # load the package &quot;cubature&quot; f &lt;- function(x) { (x[1] + x[2]) } # &quot;x&quot; is vector adaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1, 1)) ## $integral ## [1] 1 ## ## $error ## [1] 0 ## ## $functionEvaluations ## [1] 17 ## ## $returnCode ## [1] 0 Find the marginal pdfs of \\(X\\) and \\(Y\\). \\[ f_X(x)=\\int_0^1 x+y\\mathop{}\\!\\mathrm{d}y = xy + \\frac{y^2}{2}\\bigg|_0^1 = x+\\frac{1}{2} \\] where \\(0\\leq x \\leq 1\\). Similarly, \\(f_Y(y)=y+\\frac{1}{2}\\) for \\(0 \\leq y \\leq 1\\). Find the conditional pdfs of \\(X|Y=y\\) and \\(Y|X=x\\). \\[ f_{X|Y=y}(x)=\\frac{x+y}{y+\\frac{1}{2}} \\] where \\(0\\leq x \\leq 1\\). Similarly, \\(f_{Y|X=x}(y)=\\frac{x+y}{x+\\frac{1}{2}}\\) for \\(0\\leq y \\leq 1\\). Find the following probabilities: \\(\\mbox{P}(X&lt;0.5)\\); \\(\\mbox{P}(Y&gt;0.8)\\); \\(\\mbox{P}(X&lt;0.2,Y\\geq 0.75)\\); \\(\\mbox{P}(X&lt;0.2|Y\\geq 0.75)\\); \\(\\mbox{P}(X&lt;0.2|Y= 0.25)\\); Optional - \\(\\mbox{P}(X\\leq Y)\\). \\[ \\mbox{P}(X&lt;0.5)=\\int_0^{0.5} x+\\frac{1}{2}\\mathop{}\\!\\mathrm{d}x = \\frac{x^2}{2}+\\frac{x}{2}\\bigg|_0^{0.5}=0.375 \\] integrate(function(x)(x+1/2),0,1/2) ## 0.375 with absolute error &lt; 4.2e-15 Or using multivariate integration, integrate out \\(y\\). adaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1/2, 1)) ## $integral ## [1] 0.375 ## ## $error ## [1] 5.551115e-17 ## ## $functionEvaluations ## [1] 17 ## ## $returnCode ## [1] 0 \\[ \\mbox{P}(Y&lt;0.8)=\\int_{0.8}^1 y+\\frac{1}{2}\\mathop{}\\!\\mathrm{d}y = \\frac{y^2}{2}+\\frac{y}{2}\\bigg|_{0.8}^1=1-0.72=0.28 \\] adaptIntegrate(f, lowerLimit = c(0, 0.8), upperLimit = c(1, 1)) ## $integral ## [1] 0.28 ## ## $error ## [1] 5.551115e-17 ## ## $functionEvaluations ## [1] 17 ## ## $returnCode ## [1] 0 \\[ \\mbox{P}(X&lt;0.2,Y\\geq 0.75)=\\int_0^{0.2}\\int_{0.75}^1 x+y\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x= \\int_0^{0.2} xy+\\frac{y^2}{2}\\bigg|_{0.75}^1\\mathop{}\\!\\mathrm{d}x \\] \\[ =\\int_0^{0.2} x+\\frac{1}{2}-\\frac{3x}{4}-\\frac{9}{32}\\mathop{}\\!\\mathrm{d}x = \\int_0^{0.2} \\frac{x}{4}+\\frac{7}{32}\\mathop{}\\!\\mathrm{d}x = \\frac{x^2}{8}+\\frac{7x}{32}\\bigg|_0^{0.2}=0.04875 \\] adaptIntegrate(f, lowerLimit = c(0, 0.75), upperLimit = c(0.2, 1)) ## $integral ## [1] 0.04875 ## ## $error ## [1] 0 ## ## $functionEvaluations ## [1] 17 ## ## $returnCode ## [1] 0 \\[ \\mbox{P}(X&lt;0.2|Y\\geq 0.75)=\\frac{\\mbox{P}(X&lt;0.2,Y\\geq 0.75)}{\\mbox{P}(Y\\geq 0.75)}=\\frac{0.04875}{\\int_{0.75}^1 y+\\frac{1}{2}\\mathop{}\\!\\mathrm{d}y}=\\frac{0.04875}{0.34375} \\approx 0.142 \\] For \\[ \\mbox{P}(X&lt;0.2|Y= 0.25) \\] we need \\[ f_{X|Y=.25}(x)=\\frac{x+y}{y+\\frac{1}{2}}\\bigg|_{y=0.25}=\\frac{x+.25}{.25+\\frac{1}{2}}=\\frac{x+.25}{.75}=\\frac{4x+1}{3} \\] \\[ \\mbox{P}(X&lt;0.2|Y= 0.25) = \\int_{0}^{0.2} \\frac{4x+1}{3} \\mathop{}\\!\\mathrm{d}x \\] \\[ =\\frac{1}{3}\\left( 2x^2 +x \\right) \\bigg|_0^{0.2} = \\frac{1}{3}\\left( 2\\cdot0.2^2 +0.2 \\right) \\approx 0.0933 \\] f2 &lt;- function(x) { (4*x[1] + 1)/3 } # &quot;x&quot; is vector adaptIntegrate(f2, lowerLimit = c(0), upperLimit = c(0.2)) ## $integral ## [1] 0.09333333 ## ## $error ## [1] 1.036208e-15 ## ## $functionEvaluations ## [1] 15 ## ## $returnCode ## [1] 0 Optional \\[ \\mbox{P}(X\\leq Y)=\\int_0^1\\int_0^y x+y \\mathop{}\\!\\mathrm{d}x \\mathop{}\\!\\mathrm{d}y = \\int_0^1 xy+\\frac{x^2}{2}\\bigg|_0^y \\mathop{}\\!\\mathrm{d}x = \\int_0^1 \\frac{3y^2}{2}\\mathop{}\\!\\mathrm{d}y = \\frac{y^3}{2}\\bigg|_0^1 = \\frac{1}{2} \\] 14.2.2 Problem 2 In the Notes, we saw an example where \\(f_X(x)=f_{X|Y=y}(x)\\) and \\(f_Y(y)=f_{Y|X=x}(y)\\). This is not common and is important. What does this imply about \\(X\\) and \\(Y\\)? Since the conditional density function is always equal to the marginal, it means that \\(X\\) and \\(Y\\) are independent of one another. Also, if the conditioning variable does not appear in the conditional density function and the domain of the joint density is rectangular, the bounds of the two variables are constants, the random variables are independent. The variables in the previous problem are dependent, look at the conditional density functions to see that the conditional density depends on the conditioned variable. 14.2.3 Problem 3 ADVANCED: Recall on an earlier assignment, we came up with random variables to describe timeliness at an airport. Suppose over the course of 210 days, on each day we recorded the number of customer complaints regarding timeliness. Also on each day, we recorded the weather (our airport is located somewhere without snow and without substantial wind). The data are displayed below. \\[ \\begin{array}{cc|ccc} &amp; &amp; &amp;\\textbf{Weather Status} &amp; \\\\ &amp; &amp; \\mbox{Clear} &amp; \\mbox{Light Rain} &amp; \\mbox{Rain} \\\\ &amp;\\hline 0 &amp; 28 &amp; 11 &amp; 4 \\\\ \\textbf{num complaints} &amp; 1 &amp; 18 &amp; 15 &amp; 8 \\\\ &amp; 2 &amp; 17 &amp; 25 &amp; 12 \\\\ &amp; 3 &amp; 13 &amp; 15 &amp; 16 \\\\ &amp; 4 &amp; 8 &amp; 8 &amp; 10 \\\\ &amp; 5 &amp; 0 &amp; 1 &amp; 1 \\\\ \\end{array} \\] First, define two random variables for this scenario. One of them (# of complaints) is essentially already a random variable. For the other (weather status) you will need to assign a number to each status. Use the table above to build an empirical joint pmf of the two random variables. We will simply label the weather random variable as 0, 1, 2. We convert to probabilities by dividing by 210. \\[ \\begin{array}{cc|ccc} &amp; &amp; &amp;\\textbf{Weather Status} &amp; \\\\ &amp; &amp; \\mbox{Clear} &amp; \\mbox{Light Rain} &amp; \\mbox{Rain} \\\\ &amp;\\hline 0 &amp; 0.133 &amp; 0.052 &amp; 0.019 \\\\ \\textbf{num complaints} &amp; 1 &amp; 0.086 &amp; 0.071 &amp; 0.038 \\\\ &amp; 2 &amp; 0.081 &amp; 0.119 &amp; 0.057 \\\\ &amp; 3 &amp; 0.062 &amp; 0.071 &amp; 0.076 \\\\ &amp; 4 &amp; 0.038 &amp; 0.038 &amp; 0.048 \\\\ &amp; 5 &amp; 0 &amp; 0.005 &amp; 0.005 \\\\ \\end{array} \\] Find the marginal pmfs of each random variable. \\[ f_X(x)=\\left\\{\\begin{array}{ll} 0.400, &amp; x=0 \\\\ 0.357, &amp; x=1 \\\\ 0.243, &amp; x=2 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] \\[ f_Y(y)=\\left\\{\\begin{array}{ll} 0.205, &amp; y=0 \\\\ 0.195, &amp; y=1 \\\\ 0.257, &amp; y=2 \\\\ 0.210, &amp; y=3 \\\\ 0.124, &amp; y=4 \\\\ 0.010, &amp; y=5 \\\\ 0, &amp; \\mbox{otherwise} \\end{array}\\right. \\] Find the probability of fewer than 3 complaints. \\[ \\mbox{P}(Y&lt;3)=0.205+0.195+0.257=0.657 \\] Find the probability of fewer than 3 complaints given there is no rain. \\[ \\mbox{P}(Y&lt;3|X=0)=\\frac{0.133+0.086+0.081}{0.4}=0.75 \\] 14.2.4 Problem 4 Optional for those of you that like Calc III and want a challenge. Let \\(X\\) and \\(Y\\) be continuous random variables with joint pmf: \\[ f_{X,Y}(x,y)=1 \\] where \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 2x\\). Verify that \\(f\\) is a valid pdf. \\[ \\int_0^1 \\int_0^{2x} 1 \\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = \\int_0^1 y\\bigg|_0^{2x}\\mathop{}\\!\\mathrm{d}x = \\int_0^1 2x\\mathop{}\\!\\mathrm{d}x = x^2\\bigg|_0^1 = 1 \\] Find the marginal pdfs of \\(X\\) and \\(Y\\). \\[ f_X(x)=\\int_0^{2x} 1 \\mathop{}\\!\\mathrm{d}y = y\\bigg|_0^{2x}=2x \\] where \\(0\\leq x \\leq 1\\). \\[ f_Y(y)=\\int_{y/2}^1 1 \\mathop{}\\!\\mathrm{d}x = x\\bigg|_{y/2}^1 = 1-\\frac{y}{2} \\] where \\(0 \\leq y \\leq 2\\). Find the conditional pdfs of \\(X|Y=y\\) and \\(Y|X=x\\). \\[ f_{X|Y=y}(x)=\\frac{1}{1-\\frac{y}{2}}=\\frac{2}{2-y} \\] where \\(y/2 \\leq x \\leq 1\\). \\[ f_{Y|X=x}(y)=\\frac{1}{2x} \\] where \\(0\\leq y \\leq 2x\\). Find the following probabilities: \\(\\mbox{P}(X&lt;0.5)\\); \\(\\mbox{P}(Y&gt;1)\\); \\(\\mbox{P}(X&lt;0.5,Y\\leq 0.8)\\); \\(\\mbox{P}(X&lt;0.5|Y= 0.8)\\); Optional \\(\\mbox{P}(Y\\leq 1-X)\\). (It would probably help to draw some pictures.) \\[ \\mbox{P}(X&lt;0.5)=\\int_0^{0.5} 2x \\mathop{}\\!\\mathrm{d}x = x^2\\bigg|_0^{0.5}=0.25 \\] \\[ \\mbox{P}(Y&gt;1)=\\int_1^2 1-\\frac{y}{2}\\mathop{}\\!\\mathrm{d}y = y-\\frac{y^2}{4}\\bigg|_1^2 = 1-\\frac{3}{4}=0.25 \\] \\[ \\mbox{P}(X&lt;0.5,Y\\leq 0.8)=\\int_0^{0.4}\\int_0^{2x} 1 \\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x + \\int_{0.4}^{0.5}\\int_0^{0.8} 1 \\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = 0.16+0.08=0.24 \\] \\[ \\mbox{P}(X&lt;0.5|Y= 0.8)=\\int_{0.4}^{0.5} \\frac{2}{2-0.8}\\mathop{}\\!\\mathrm{d}x = \\frac{5x}{3}\\bigg|_{0.4}^{0.5}=0.1667 \\] \\[ \\mbox{P}(Y\\leq 1-X)=\\int_0^{1/3}\\int_0^{2x} 1 \\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x + \\int_{1/3}^1\\int_0^{1-x}1 \\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = \\int_0^{1/3}2x\\mathop{}\\!\\mathrm{d}x + \\int_{1/3}^1 1-x\\mathop{}\\!\\mathrm{d}x \\] \\[ =\\frac{1}{9}+\\left(x-\\frac{x^2}{2}\\right)_{1/3}^1=\\frac{1}{9}+\\frac{1}{2}-\\frac{1}{3}+\\frac{1}{18} = \\frac{1}{3} \\] "],["EST.html", "Chapter 15 Estimation Methods 15.1 Objectives 15.2 Homework", " Chapter 15 Estimation Methods 15.1 Objectives Obtain a method of moments estimate of a parameter or set of parameters. Given a random sample from a distribution, obtain the likelihood function. Obtain a maximum likelihood estimate of a parameter or set of parameters. Determine if an estimator is unbiased. 15.2 Homework 15.2.1 Problem 1 In the chapter notes, we found that if we take a sample from the uniform distribution \\(\\textsf{Unif}(0,\\theta)\\), the method of moments estimate of \\(\\theta\\) is \\(\\hat{\\theta}_{MoM}=2\\bar{x}\\). Suppose our sample consists of the following values: \\[ 0.2 \\hspace{0.4cm} 0.9 \\hspace{0.4cm} 1.9 \\hspace{0.4cm} 2.2 \\hspace{0.4cm} 4.7 \\hspace{0.4cm} 5.1 \\] What is \\(\\hat{\\theta}_{MoM}\\) for this sample? x&lt;-c(0.2,0.9,1.9,2.2,4.7,5.1) thetamom&lt;-2*mean(x) thetamom ## [1] 5 What is wrong with this estimate? For our distribution \\(\\theta\\) is the upper bound or largest value for the random variable. The estimate for \\(\\theta\\) is 5, which is an impossible value for \\(\\theta\\), since one of our observations (5.1) is beyond this value. Show that this estimator is unbiased. We need to show that \\[ E\\left( \\hat{\\theta}_{MoM} \\right) = \\theta \\] We proceed as follows \\[ E \\left(2\\bar{X} \\right) = 2 E\\left( \\sum{\\frac{X_i}{n}} \\right) = \\frac{2}{n} E\\left( \\sum{X_i} \\right) \\] \\[ =\\frac{2}{n} \\sum{E\\left(X_i \\right)} =\\frac{2}{n}\\sum{\\frac{\\theta}{2}}=\\frac{n\\theta}{n}=\\theta \\] Notice that in performing this derivation, we treated \\(X\\) as a random variable and not as \\(x\\), a data value. ADVANCED: Use simulation in R to find out how often the method of moment estimator is less the maximum observed value, (\\(\\hat{\\theta}_{MoM} &lt; \\max x\\)). Report an answer for various sizes of samples. You can just pick an arbitrary value for \\(\\theta\\) when you sample from the uniform. However, the minimum must be 0. Let’s start by writing code for one sample size and then generalize. This function will take as input the number of data points, sample that many points from a uniform with a max of 5, and then return a logical value comparing the method of moments estimate to the observed maximum. The choice of a maximum value for the uniform distribution is arbitrary. check &lt;- function(n=10){ temp&lt;-runif(n,max=5) 2*mean(temp)&lt;max(temp) } Let’s test the function. set.seed(3030) temp&lt;-runif(10,max=5) temp ## [1] 1.048619 2.370056 0.675677 2.434443 2.994015 3.930865 4.708367 1.645806 ## [9] 2.592365 3.664424 2*mean(temp) ## [1] 5.212927 Reset the seed and run the function, we should get FALSE. set.seed(3030) check(10) ## [1] FALSE Now let’s repeat the test 10000 find the proportion of times the method of moments estimator is unrealistic. (do(10000)*check(10)) %&gt;% summarize(mean(check)) %&gt;% pull() ## [1] 0.285 Let’s make check a vectorized function for we can run for many sample sizes. check &lt;- Vectorize(check) Run 1000 replicates for each sample size. The rest of the code gets my data frame in the proper shape. my_data&lt;-(do(1000)*check(seq(10,200,5))) %&gt;% summarise_all(mean) %&gt;% pivot_longer(everything(),names_to = &quot;Sample&quot;,values_to = &quot;Percent&quot;) %&gt;% mutate(Sample=seq(10,200,5)) A quick look at the data. head(my_data) ## # A tibble: 6 × 2 ## Sample Percent ## &lt;dbl&gt; &lt;dbl&gt; ## 1 10 0.282 ## 2 15 0.358 ## 3 20 0.362 ## 4 25 0.362 ## 5 30 0.398 ## 6 35 0.389 Now we can plot the results of sample size versus my_data %&gt;% gf_line(Percent~Sample,xlab=&quot;Sample Size&quot;,title=&quot;Percent When Estimator is Invalid&quot;) %&gt;% gf_theme(theme = theme_minimal()) Here is more traditional, old school R code. simn&lt;-function(n){ y&lt;-replicate(1000,{ x&lt;-runif(n) (2*mean(x))&lt;max(x) }) mean(y) } t&lt;-seq(10,200,4) persim&lt;-sapply(t,simn) plot(t,persim,type=&quot;l&quot;) 15.2.2 Problem 2 Let \\(x_1,x_2,...,x_n\\) be a simple random sample from an exponentially distributed population with parameter \\(\\lambda\\). Find \\(\\hat{\\lambda}_{MoM}\\). Recall that \\(\\mbox{E}(X)={1\\over \\lambda}\\). Setting this equal to the sample moment \\(\\bar{x}\\) and solving for \\(\\lambda\\) yields the method of moment estimator. Thus, \\[ \\hat{\\lambda}_{MoM}={1\\over \\bar{x}} \\] 15.2.3 Problem 3 Let \\(x_1,x_2,...,x_n\\) be an iid random sample from an exponentially distributed population with parameter \\(\\lambda\\). Find \\(\\hat{\\lambda}_{MLE}\\). Recall that \\[ f_X(x;\\lambda)=\\lambda e^{-\\lambda x} \\] So the likelihood function is: \\[ L(\\lambda;\\boldsymbol{x})=\\prod_{i=1}^n \\lambda e^{-\\lambda x_i}=\\lambda^n e^{-\\lambda\\sum x_i} \\] And the log-likelihood function is: \\[ l(\\lambda;\\boldsymbol{x})=n\\log \\lambda - \\lambda \\sum x_i \\] Taking the derivative with respect to \\(\\lambda\\) and setting equal to 0: \\[ {\\mathop{}\\!\\mathrm{d}l(\\lambda;\\boldsymbol{x})\\over \\mathop{}\\!\\mathrm{d}\\lambda}={n\\over \\lambda}-\\sum x_i =0 \\] Note that \\({\\mathop{}\\!\\mathrm{d}^2 l(\\lambda;\\boldsymbol{x})\\over\\mathop{}\\!\\mathrm{d}\\lambda^2}=-{n\\over \\lambda^2}&lt;0\\), so \\(l\\) is always concave down. Thus, any optimum found is a maximum. So, \\[ \\hat{\\lambda}_{MLE}={n\\over \\sum x_i}={1\\over \\bar{x}} \\] 15.2.4 Problem 4 It is mathematically difficult to determine if the estimators found in questions 2 and 3 are unbiased. Since the sample mean is in the denominator; mathematically we may have to work with the joint pdf. So instead, use simulation to get an sense of whether the method of moments estimator for the exponential distribution is unbaised. We need to sample data from an exponential and then compare the the reciprocal of the mean to the parameter. set.seed(630) 1/mean(rexp(1000,rate=10)) ## [1] 10.09422 This is close, maybe we just got lucky. Let’s repeat many times. (do(1000)*(1/mean(rexp(1000,rate=10)))) %&gt;% gf_boxplot(~result) Looks like it has the potential to be unbiased. We would need to investigate other values for \\(\\lambda\\). 15.2.5 Problem 5 Find a maximum likelihood estimator for \\(\\theta\\) when \\(X\\sim\\textsf{Unif}(0,\\theta)\\). Compare this to the method of moments estimator we found. Hint: Do not take the derivative of the likelihood function. \\[ L(\\theta;\\boldsymbol{x})=\\frac{1}{\\theta^n}, \\hspace{0.5cm} \\mbox{only if all }x_i\\leq \\theta \\] A better way to write this is so as to see the answer is to let \\(M =max(x_i)\\), then: \\[ L(\\theta;\\boldsymbol{x})=\\left\\{\\begin{array}{ll} 0, &amp; \\theta &lt; M= max(x_i) \\\\ \\frac{1}{\\theta^n}, &amp; \\theta \\geq M = max(x_i) \\\\ \\end{array}\\right. \\] Figure 15.1 is a plot of this likelihood function and you can see that the maximum occurs at the maximum observed data point. Figure 15.1: A graph of the likelihood function Recall that \\(f(x_i;\\theta)={1\\over \\theta}\\) if \\(x_i\\in [0,\\theta]\\) and 0 elsewhere. And since the likelihood function is simply the product of these pdfs, if any \\(x_i\\) is beyond \\(\\theta\\), then the entire likelihood function is 0. You can picture \\(L\\) as a decreasing function of \\(\\theta\\), but remembering that \\(L\\) takes the value 0 if \\(\\theta\\) is smaller than at least one \\(x_i\\). Thus, \\(L\\) achieves its maximum at \\(\\theta=\\max x_i\\). This estimate is more intuitive than the method of moments estimate (\\(2\\bar{x}\\)). The method of moments estimate is sometimes not feasible. Meanwhile, the MLE (\\(\\max x_i\\)) is always feasible. "],["CS3.html", "Chapter 16 Case Study 16.1 Objectives 16.2 Homework", " Chapter 16 Case Study 16.1 Objectives Define and use properly in context all new terminology. Conduct a hypothesis test using a permutation test to include all 4 steps. 16.2 Homework 16.2.1 Problem 1 Side effects of Avandia Rosiglitazone is the active ingredient in the controversial type~2 diabetes medicine Avandia and has been linked to an increased risk of serious cardiovascular problems such as stroke, heart failure, and death. A common alternative treatment is pioglitazone, the active ingredient in a diabetes medicine called Actos. In a nationwide retrospective observational study of 227,571 Medicare beneficiaries aged 65 years or older, it was found that 2,593 of the 67,593 patients using rosiglitazone and 5,386 of the 159,978 using pioglitazone had serious cardiovascular problems. These data are summarized in the contingency table below. \\[ \\begin{array}{ccc|cc|c} &amp; &amp; &amp;\\textbf{Cardiovascular} &amp; \\textbf{problems} &amp; \\\\&amp; &amp; &amp; Yes &amp; No &amp; Total \\\\ &amp;\\hline \\textbf{Treatment} &amp; \\textit{Rosiglitazone} &amp; 2,593 &amp; 65,000 &amp; 67,593 \\\\ &amp; &amp; \\textit{Pioglitazone} &amp; 5,386 &amp; 154,592 &amp; 159,978\\\\ &amp;\\hline &amp;Total &amp; 7,979 &amp; 219,592 &amp; 227,571 \\end{array} \\] Determine if each of the following statements is true or false. If false, explain why. The reasoning may be wrong even if the statement’s conclusion is correct. In such cases, the statement should be considered false. Since more patients on pioglitazone had cardiovascular problems (5,386 vs. 2,593), we can conclude that the rate of cardiovascular problems for those on a pioglitazone treatment is higher. False. Instead of comparing counts, we should compare percentages. The data suggest that diabetic patients who are taking rosiglitazone are more likely to have cardiovascular problems since the rate of incidence was (2,593 / 67,593 = 0.038) 3.8% for patients on this treatment, while it was only (5,386 / 159,978 = 0.034) 3.4% for patients on pioglitazone. True. The fact that the rate of incidence is higher for the rosiglitazone group proves that rosiglitazone causes serious cardiovascular problems. False. We cannot infer a causal relationship from an association in an observational study. However, we can say the drug a person is on affects his risk in this case, as he chose that drug and his choice may be associated with other variables, which is why part (b) is true. The difference in these statements is subtle but important. Based on the information provided so far, we cannot tell if the difference between the rates of incidences is due to a relationship between the two variables or due to chance. True. 16.2.2 Problem 2 Heart transplants The Stanford University Heart Transplant Study was conducted to determine whether an experimental heart transplant program increased lifespan. Each patient entering the program was designated an official heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart. Some patients got a transplant and some did not. The variable indicates which group the patients were in; patients in the treatment group got a transplant and those in the control group did not. Another variable called was used to indicate whether or not the patient was alive at the end of the study. In the study, of the 34 patients in the control group, 4 were alive at the end of the study. Of the 69 patients in the treatment group, 24 were alive. The contingency table below summarizes these results. \\[ \\begin{array}{ccc|cc|c} &amp; &amp; &amp;\\textbf{Group} &amp; &amp; \\\\&amp; &amp; &amp; Control &amp; Treatment &amp; Total \\\\ &amp;\\hline \\textbf{Outcome} &amp; \\textit{Alive} &amp; 4 &amp; 24 &amp; 28 \\\\ &amp; &amp; \\textit{Dead} &amp; 30 &amp; 45 &amp; 75\\\\ &amp;\\hline &amp;Total &amp; 34 &amp; 69 &amp; 103 \\end{array} \\] The data is in a file called Stanford_heart_study.csv. Read the data in and answer the following questions. heart&lt;-read_csv(&quot;data/Stanford_heart_study.csv&quot;) What proportion of patients in the treatment group and what proportion of patients in the control group died? inspect(heart) ## ## categorical variables: ## name class levels n missing ## 1 outcome character 2 103 0 ## 2 group character 2 103 0 ## distribution ## 1 Dead (72.8%), Alive (27.2%) ## 2 Treatment (67%), Control (33%) tally(~outcome+group,data=heart,margins = TRUE) ## group ## outcome Control Treatment Total ## Alive 4 24 28 ## Dead 30 45 75 ## Total 34 69 103 tally(outcome~group,data=heart,margins = TRUE,format=&quot;percent&quot;) ## group ## outcome Control Treatment ## Alive 11.76471 34.78261 ## Dead 88.23529 65.21739 ## Total 100.00000 100.00000 So 88.2% of patients in control group died and 65.2% in the treatment group. One approach for investigating whether or not the treatment is effective is to use a randomization technique. What are the claims being tested? Use the same null and alternative hypothesis notation used in the lesson notes. \\(H_0\\): Independence model. The variables group and outcome are independent. They have no relationship, and the difference in survival rates between the control and treatment groups was due to chance. In other words, heart transplant is not effective. \\(H_A\\): Alternative hypothesis. The variables group and outcome are not independent. The difference in survival rates between the control and treatment groups was not due to chance and the heart transplant is effective. The paragraph below describes the set up for such approach, if we were to do it without using statistical software. Fill in the blanks with a number or phrase, whichever is appropriate. We write alive on 28 cards representing patients who were alive at the end of the study, and dead on 75 cards representing patients who were not. Then, we shuffle these cards and split them into two groups: one group of size 69 representing treatment, and another group of size 34 representing control. We calculate the difference between the proportion of dead cards in the control and treatment groups (control - treatment), this is just so we have positive observed value, and record this value. We repeat this many times to build a distribution centered at zero. Lastly, we calculate the fraction of simulations where the simulated differences in proportions are 0.23 or greater. If this fraction of simulations, the empirical p-value, is low, we conclude that it is unlikely to have observed such an outcome by chance and that the null hypothesis should be rejected in favor of the alternative. Next we will perform the simulation and use results to decide the effectiveness of the transplant program. Find observed value of the test statistic, which we decided to use the difference in proportions. obs&lt;-diffprop(outcome~group,data=heart) obs ## diffprop ## 0.230179 Simulate 1000 values of the test statistic by using shuffle() on the variable group. set.seed(1213) results &lt;- do(1000)*diffprop(outcome~shuffle(group),data=heart) Plot distribution of results. Include a vertical line for the observed value. Clean up the plot as if you were presenting to a decision maker. results %&gt;% gf_histogram(~diffprop,xlab=&quot;Difference in proportions&quot;, ylab=&quot;Count&quot;, fill=&quot;cyan&quot;, color=&quot;black&quot;, title=&quot;Stanford Heart Study&quot;, subtitle=&quot;Distribution of difference between control and treatment groups&quot;) %&gt;% gf_vline(xintercept =obs ) %&gt;% gf_theme(theme_classic()) Find p-value. Think carefully about what more extreme would mean. results %&gt;% summarise(p_value = mean(diffprop&gt;=obs)) ## p_value ## 1 0.011 If we wanted to use a hypergeometric. We could use any of the cell in the table. We will use the upper left. More extreme would be 4 or fewer in the control group being alive. We get a similar p-value. tally(~outcome+group,data=heart,margins = TRUE) ## group ## outcome Control Treatment Total ## Alive 4 24 28 ## Dead 30 45 75 ## Total 34 69 103 phyper(4,34,69,28) ## [1] 0.01039537 Decide if the treatment is effective. Under the independence model, only 11 out of 1000 times (1.1%) did we get a difference of 0.23 or higher between the proportions of patients that died in the control and treatment groups. Since this is a low probability, we can reject the claim of independence in favor of the alternate model. There is convincing evidence to suggest that the transplant program is effective. "],["HYPOTEST.html", "Chapter 17 Hypothesis Testing 17.1 Objectives 17.2 Homework", " Chapter 17 Hypothesis Testing 17.1 Objectives Know and properly use the terminology of a hypothesis test. Conduct all four steps of a hypothesis test using randomization. Discuss and explain the ideas of decision errors, one-sided versus two-sided, and choice of statistical significance. 17.2 Homework 17.2.1 Problem 1 Repeat the analysis of the commercial length in the notes. This time use a different test statistic. ads&lt;-read_csv(&quot;data/ads.csv&quot;) ads ## # A tibble: 10 × 2 ## basic premium ## &lt;dbl&gt; &lt;dbl&gt; ## 1 6.95 3.38 ## 2 10.0 7.8 ## 3 10.6 9.42 ## 4 10.2 4.66 ## 5 8.58 5.36 ## 6 7.62 7.63 ## 7 8.23 4.95 ## 8 10.4 8.01 ## 9 11.0 7.8 ## 10 8.52 9.58 ads &lt;- ads %&gt;% pivot_longer(cols=everything(),names_to=&quot;channel&quot;,values_to = &quot;length&quot;) ads ## # A tibble: 20 × 2 ## channel length ## &lt;chr&gt; &lt;dbl&gt; ## 1 basic 6.95 ## 2 premium 3.38 ## 3 basic 10.0 ## 4 premium 7.8 ## 5 basic 10.6 ## 6 premium 9.42 ## 7 basic 10.2 ## 8 premium 4.66 ## 9 basic 8.58 ## 10 premium 5.36 ## 11 basic 7.62 ## 12 premium 7.63 ## 13 basic 8.23 ## 14 premium 4.95 ## 15 basic 10.4 ## 16 premium 8.01 ## 17 basic 11.0 ## 18 premium 7.8 ## 19 basic 8.52 ## 20 premium 9.58 favstats(length~channel,data=ads) ## channel min Q1 median Q3 max mean sd n missing ## 1 basic 6.950 8.30375 9.298 10.30000 11.016 9.2051 1.396126 10 0 ## 2 premium 3.383 5.05250 7.715 7.95975 9.580 6.8592 2.119976 10 0 State the null and alternative hypotheses. \\(H_0\\): Null hypothesis. The distribution of length of commercials in premium and basic channels is the same. \\(H_A\\): Alternative hypothesis. The distribution of length of commercials in premium and basic channels is different. Compute a test statistic. We will use the difference in means so we can use diffmeans() from mosiac. obs &lt;- diffmean(length~channel,data=ads) obs ## diffmean ## -2.3459 Determine the p-value. set.seed(4172) results &lt;- do(10000)*diffmean(length~shuffle(channel),data=ads) Next we create a plot of the empirical sampling distribution of the difference of means. results %&gt;% gf_histogram(~diffmean, fill=&quot;cyan&quot;, color=&quot;black&quot;) %&gt;% gf_vline(xintercept =obs) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Test statistic&quot;) Again, notice it is centered at zero and symmetrical. prop1(~(diffmean&lt;=obs),data=results) ## prop_TRUE ## 0.00459954 The p-value is much smaller! The test statistic matters in terms of efficiency of the testing procedure. Draw a conclusion. Based on our data, if there were really no difference in the distribution of lengths of commercials in 30 minute shows between basic and premium channels then the probability of finding our observed difference of means is 0.005. Since this is less than our significance level of 0.05, we reject the null in favor of the alternative that the basic channel has longer commercials. 17.2.2 Problem 2 Is yawning contagious? An experiment conducted by the MythBusters, a science entertainment TV program on the Discovery Channel, tested if a person can be subconsciously influenced into yawning if another person near them yawns. 50 people were randomly assigned to two groups: 34 to a group where a person near them yawned (treatment) and 16 to a group where there wasn’t a person yawning near them (control). The following table shows the results of this experiment. \\[ \\begin{array}{ccc|cc|c} &amp; &amp; &amp;\\textbf{Group} &amp; &amp; \\\\&amp; &amp; &amp; Treatment &amp; Control &amp; Total \\\\ &amp;\\hline \\textbf{Result} &amp; \\textit{Yawn} &amp; 10 &amp; 4 &amp; 14 \\\\ &amp; &amp; \\textit{No Yawn} &amp; 24 &amp; 12 &amp; 36\\\\ &amp;\\hline &amp;Total &amp; 34 &amp; 16 &amp; 50 \\end{array} \\] The data is in the file “yawn.csv.” yawn &lt;- read_csv(&quot;data/yawn.csv&quot;) glimpse(yawn) ## Rows: 50 ## Columns: 2 ## $ group &lt;chr&gt; &quot;treatment&quot;, &quot;treatment&quot;, &quot;control&quot;, &quot;treatment&quot;, &quot;treatment&quot;,… ## $ outcome &lt;chr&gt; &quot;no_yawn&quot;, &quot;no_yawn&quot;, &quot;no_yawn&quot;, &quot;no_yawn&quot;, &quot;no_yawn&quot;, &quot;yawn&quot;,… inspect(yawn) ## ## categorical variables: ## name class levels n missing ## 1 group character 2 50 0 ## 2 outcome character 2 50 0 ## distribution ## 1 treatment (68%), control (32%) ## 2 no_yawn (72%), yawn (28%) tally(outcome~group,data=yawn,margins = TRUE,format=&quot;proportion&quot;) ## group ## outcome control treatment ## no_yawn 0.7500000 0.7058824 ## yawn 0.2500000 0.2941176 ## Total 1.0000000 1.0000000 What are the hypotheses? \\(H_0\\): Yawning is not contagious, someone in the group yawning does not impact the percentage of the group that yawns. \\(p_c - p_t = 0\\) or equivalently \\(p_c = p_t\\) . \\(H_A\\): Yawning does have an impact, it is contagious. If someone yawns then you are more likely to yawn. \\(p_t &gt; p_c\\) or \\(p_c - p_t &lt; 0\\). Calculate the observed difference between the yawning rates under the two scenarios. Yes we are giving you the test statistic. obs &lt;- diffprop(outcome~group,data=yawn) obs ## diffprop ## -0.04411765 Notice that it is negative. If it had been positive, then we would not even need the next step; we would fail to reject the null because the p-value would be much larger than 0.05. Think about this and make sure you understand. Estimate the p-value using randomization. set.seed(56) results&lt;-do(10000)*diffprop(outcome~shuffle(group),data=yawn) prop1(~(diffprop&lt;=obs),data=results) ## prop_TRUE ## 0.5140486 This is a large p-value. Notice that if we were doing a two-sided hypothesis test, then doubling the p-value would exceed 1. Since a p-value is a probability, this is not possible and so we would report a p-value of approximately 1. The reason is that the sampling distribution is not symmetrical. We can see this if we plot the hypergeometric distribution for this problem. gf_dist(&quot;hyper&quot;,m=16,n=34,k=14) %&gt;% gf_theme(theme_classic()) %&gt;% gf_refine(scale_x_continuous(breaks=0:14)) Plot the empirical sampling distribution. results %&gt;% gf_histogram(~diffprop, fill=&quot;cyan&quot;, color=&quot;black&quot;) %&gt;% gf_vline(xintercept =obs ) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Test statistic&quot;) Determine the conclusion of the hypothesis test. Since p-value, 0.54, is high, larger than 0.05, we fail to reject the null hypothesis of yawning is not contagious. The data do not provide convincing evidence that people are more likely to yawn if a person near them yawns. The traditional belief is that yawning is contagious – one yawn can lead to another yawn, which might lead to another, and so on. In this exercise, there was the option of selecting a one-sided or two-sided test. Which would you recommend (or which did you choose)? Justify your answer in 1-3 sentences. I chose a one-sided test since as a researcher, I thought having someone in the group yawn would lead to more people in that group yawning. How did you select your level of significance? Explain in 1-3 sentences. Since there was no clear impact on one type of error being worse than the other, I stayed with the default of 0.05. "],["PVALUES.html", "Chapter 18 Empirical p-values 18.1 Objective 18.2 Homework", " Chapter 18 Empirical p-values 18.1 Objective Conduct all four steps of a hypothesis test using probability models. 18.2 Homework 18.2.1 Problem 1 Repeat the analysis of the yawning data from last lesson but this time use the hypergeometric distribution. Is yawning contagious? An experiment conducted by the MythBusters, a science entertainment TV program on the Discovery Channel, tested if a person can be subconsciously influenced into yawning if another person near them yawns. 50 people were randomly assigned to two groups: 34 to a group where a person near them yawned (treatment) and 16 to a group where there wasn’t a person yawning near them (control). The following table shows the results of this experiment. \\[ \\begin{array}{ccc|cc|c} &amp; &amp; &amp;\\textbf{Group} &amp; &amp; \\\\&amp; &amp; &amp; Treatment &amp; Control &amp; Total \\\\ &amp;\\hline \\textbf{Result} &amp; \\textit{Yawn} &amp; 10 &amp; 4 &amp; 14 \\\\ &amp; &amp; \\textit{No Yawn} &amp; 24 &amp; 12 &amp; 36\\\\ &amp;\\hline &amp;Total &amp; 34 &amp; 16 &amp; 50 \\end{array} \\] The data is in the file “yawn.csv.” What are the hypotheses? \\(H_0\\): Yawning is not contagious, someone in the group yawning does not impact the percentage of the group that yawns. \\(H_A\\): Yawning does have an impact, it is contagious. If someone yawns then you are more likely to yawn. Calculate the observed statistic, pick a cell. yawn &lt;- read_csv(&quot;data/yawn.csv&quot;) glimpse(yawn) ## Rows: 50 ## Columns: 2 ## $ group &lt;chr&gt; &quot;treatment&quot;, &quot;treatment&quot;, &quot;control&quot;, &quot;treatment&quot;, &quot;treatment&quot;,… ## $ outcome &lt;chr&gt; &quot;no_yawn&quot;, &quot;no_yawn&quot;, &quot;no_yawn&quot;, &quot;no_yawn&quot;, &quot;no_yawn&quot;, &quot;yawn&quot;,… inspect(yawn) ## ## categorical variables: ## name class levels n missing ## 1 group character 2 50 0 ## 2 outcome character 2 50 0 ## distribution ## 1 treatment (68%), control (32%) ## 2 no_yawn (72%), yawn (28%) tally(~outcome+group,data=yawn,margins = TRUE) ## group ## outcome control treatment Total ## no_yawn 12 24 36 ## yawn 4 10 14 ## Total 16 34 50 The random variable is the number of control patients that yawned from a population of 16 control patients, 34 treatment patients, where a total of 14 yawned. Find the p-value using the hypergeometric distribution. In this case we want to find \\(\\mbox{P}(X \\leq 4)\\) and double it since it is a two-sided test. phyper(4,16,34,14) ## [1] 0.5127818 Doubling this will take us above 1, which is not valid. You might have seen this when we did the randomization test for this problem in a previous lesson. Again, since the hypergeometric is not symmetrical, we can’t just double the p-value from the one-sided test. We could simply report the result as \\(\\approx 1\\). If we look at a plot of the pmf, see the figure, you see that \\(X=4\\) is the highest probability outcome. Thus the p-value is 1 if we sum all the values less than or equal to \\(P(X=4)\\). gf_dist(&quot;hyper&quot;,m=16,n=34,k=14) %&gt;% gf_hline(yintercept = dhyper(4,16,34,14),color=&quot;red&quot;) %&gt;% gf_labs(title=&quot;Hypergeometric pmf&quot;, subtitle=&quot;Red line is P(X=4)&quot;, y=&quot;Probability&quot;) %&gt;% gf_theme(theme_classic) ## Warning: geom_hline(): Ignoring `mapping` because `yintercept` was provided. fisher.test(tally(~group+outcome,data=yawn)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: tally(~group + outcome, data = yawn) ## p-value = 1 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.2790902 6.5930656 ## sample estimates: ## odds ratio ## 1.244531 temp&lt;-dhyper(0:14,16,34,14) sum(temp[temp&lt;=dhyper(4,16,34,14)]) ## [1] 1 Plot the the sampling distribution. gf_dist(&quot;hyper&quot;,m=16,n=34,k=14) %&gt;% gf_vline(xintercept =4,color=&quot;red&quot; ) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Hypergeometric pmf&quot;, subtitle=&quot;Red line is X=4&quot;, y=&quot;Probability&quot;) Determine the conclusion of the hypothesis test. Since p-value, 1, is high, larger than 0.05, we fail to reject the null hypothesis of yawning is not contagious. The data do not provide convincing evidence that people are more likely to yawn if a person near them yawns. Compare your results with the randomization test. This result is essentially the same as the randomization test. 18.2.2 Problem 2 Repeat the golf ball example using a different test statistic. Use a level of significance of 0.05. State the null and alternative hypotheses. We think that the numbers are not all equally likely. The question of one-sided versus two-sided is not relevant in this test, you will see this when we write the hypotheses. \\(H_0\\): All of the numbers are equally likely.\\(\\pi_1 = \\pi_2 = \\pi_3 = \\pi_4\\) or \\(\\pi_1 = \\frac{1}{4}, \\pi_2 =\\frac{1}{4}, \\pi_3 =\\frac{1}{4}, \\pi_4 =\\frac{1}{4}\\) \\(H_A\\): The is some other distribution of percentages in the population. At least one population proportion is not \\(\\frac{1}{4}\\). Compute a test statistic. golf_balls &lt;- read_csv(&quot;data/golf_balls.csv&quot;) inspect(golf_balls) ## ## quantitative variables: ## name class min Q1 median Q3 max mean sd n missing ## ...1 number numeric 1 1 2 3 4 2.366255 1.107432 486 0 tally(~number,data=golf_balls,format = &quot;proportion&quot;) ## number ## 1 2 3 4 ## 0.2818930 0.2839506 0.2201646 0.2139918 I will use the maximum deviation from the expected value obs &lt;- max(abs(tally(~number,data=golf_balls) -121.5)) obs ## [1] 17.5 Determine the p-value. set.seed(2517) results &lt;- do(10000)*max(abs(table(sample(1:4,size=486,replace=TRUE))-121.5)) results %&gt;% gf_histogram(~max,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_vline(xintercept = obs) %&gt;% gf_labs(title=&quot;Sampling Distribution of Maximum Deviation&quot;, subtitle=&quot;Multinomial with equal probability&quot;, x=&quot;Range&quot;) %&gt;% gf_theme(theme_classic) prop1(~(max&gt;=obs),data=results) ## prop_TRUE ## 0.2382762 Draw a conclusion. Since this p-value is larger than 0.05, we do not reject the null hypothesis. That is, based on our data, we do not find statistically significant evidence against the claim that the number on the golf balls are equally likely. 18.2.3 Problem 3 Body Temperature Shoemaker6 cites a paper from the American Medical Association7 that questions conventional wisdom that the average body temperature of a human is 98.6. One of the main points of the original article – the traditional mean of 98.6 is, in essence, 100 years out of date. The authors cite problems with Wunderlich’s original methodology, diurnal fluctuations (up to 0.9 degrees F per day), and unreliable thermometers. The authors believe the average temperature is less than 98.6. Test the hypothesis. State the null and alternative hypotheses. \\(H_0\\): The average body temperature is 98.6 \\(\\mu = 98.6\\) \\(H_A\\): The average body temperature is less than 98.6. \\(\\mu &lt; 98.6\\) State the significance level that will be used. There is no reason to believe one type of error is more important than another. \\(\\alpha = 0.05\\) Load the data from the file “temperature.csv” and generate summary statistics and a boxplot of the temperature data. We will not be using gender or heart rate for this problem. temperature &lt;- read_csv(&quot;data/temperature.csv&quot;) glimpse(temperature) ## Rows: 130 ## Columns: 3 ## $ temperature &lt;dbl&gt; 96.3, 96.7, 96.9, 97.0, 97.1, 97.1, 97.1, 97.2, 97.3, 97.4… ## $ gender &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ hr &lt;dbl&gt; 70, 71, 74, 80, 73, 75, 82, 64, 69, 70, 68, 72, 78, 70, 75… favstats(~temperature,data=temperature) ## min Q1 median Q3 max mean sd n missing ## 96.3 97.8 98.3 98.7 100.8 98.24923 0.7331832 130 0 temperature %&gt;% gf_boxplot(~temperature) %&gt;% gf_theme(theme_classic) Compute a test statistic. We are going to help you with this part. We cannot do a randomization test since we don’t have a second variable. It would be nice to use the mean as a test statistic but we don’t yet know the sampling distribution of the sample mean. Let’s get clever. If the distribution of the sample is symmetric, this is an assumption but look at the boxplot and summary statistics to determine if you are comfortable with it, then under the null hypothesis the observed values should be equally likely to either be greater or less than 98.6. Thus our test statistic is the number of cases that have a positive difference between the observed value and 98.6. This will be a binomial distribution with a probability of success of 0.5. You must also account for the possibility that there are observations of 98.6 in the data. First let’s find out how many data points are equal to 98.6. temperature %&gt;% count(temperature==98.6) ## # A tibble: 2 × 2 ## `temperature == 98.6` n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 120 ## 2 TRUE 10 We have ten observations equal to 98.6, we will split this and make 5 have a positive difference and 5 have a negative difference. Next determine that number of subjects that have a positive difference. temperature %&gt;% mutate(pos=(temperature-98.6)&gt;0) %&gt;% summarise(num_greater=sum(pos)) ## # A tibble: 1 × 1 ## num_greater ## &lt;int&gt; ## 1 39 Therefore we have a total 44 subjects whose temperature was greater than 98.6 and 86 who had a temperature less than 98.6 Determine the p-value. Out of 130 subjects, 86 had a temperature less than 98.6 and 44 had a temperature greater. We can use either number to determine a p-value. If the null hypothesis were true, then the probability of 86 or more, this is more extreme under that alternative hypothesis, is 1-pbinom(85,130,.5) ## [1] 0.0001447744 We could have also done 44 or less. pbinom(44,130,.5) ## [1] 0.0001447744 Our p-value is 0.00014. Some other choices are to drop the 10 data points equal to 98.6. We should always be careful about deleting data. We could be extremely conservative and move all 10 points to greater than 98.6 and if we still reject, we will comfortable about our conclusion. Finally, we could randomly assign each point to one side or the other. You can check the p-values for each of these methods. Draw a conclusion. Based on our data, if the true mean body temperature is 98.6, then the probability of having 86 or more subjects out of 130 with temperatures below this is 0.00014. This is too unlikely so we reject the hypothesis that the average body temperature is 98.6. This is a clever way to test the claim. Make sure you understand how we solved. In the coming lessons we will show you alternative ways to attack the problem. We made it into a binomial random variable. The only assumption is independence and symmetry. Notice that by descritizing this problem, we are taking information away. However, the p-value is still small. L. Shoemaker Allen (1996) What’s Normal? – Temperature, Gender, and Heart Rate, Journal of Statistics Education, 4:2↩︎ Mackowiak, P. A., Wasserman, S. S., and Levine, M. M. (1992), “A Critical Appraisal of 98.6 Degrees F, the Upper Limit of the Normal Body Temperature, and Other Legacies of Carl Reinhold August Wunderlich,” Journal of the American Medical Association, 268, 1578-1580.↩︎ "],["CLT.html", "Chapter 19 Central Limit Theorem 19.1 Objectives 19.2 Homework", " Chapter 19 Central Limit Theorem 19.1 Objectives Explain the central limit theorem and when you can use it for inference. Conduct hypothesis tests of a single mean and proportion using the CLT and R. Explain how the chi-squared and \\(t\\) distributions relate to the normal distribution, where we use them, and describe the impact on the shape of the distribution when the parameters are changed. 19.2 Homework 19.2.1 Problem 1 Suppose we roll a fair six-sided die and let \\(X\\) be the resulting number. The distribution of \\(X\\) is discrete uniform. (Each of the six discrete outcomes is equally likely.) Suppose we roll the fair die 5 times and record the value of \\(\\bar{X}\\), the mean of the resulting rolls. Under the central limit theorem, what should be the distribution of \\(\\bar{X}\\)? The mean of \\(X\\) is 3.5 and the variance of \\(X = \\frac{(b-a+1)^2-1}{12} = \\frac{35}{12}\\) is 2.9167. So, \\[ \\bar{X}\\overset{approx}{\\sim}\\textsf{Norm}(3.5,0.764) \\] Simulate this process in R. Plot the resulting empirical distribution of \\(\\bar{X}\\) and report the mean and standard deviation of \\(\\bar{X}\\). Was it what you expected? (HINT: You can simulate a die roll using the sample function. Be careful and make sure you use it properly.) set.seed(2003) results&lt;-do(10000)*mean(sample(6,5,replace=T)) results %&gt;% gf_histogram(~mean,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Test statistic&quot;) favstats(~mean,data=results) ## min Q1 median Q3 max mean sd n missing ## 1 3 3.6 4 6 3.51278 0.772254 10000 0 It appears to be roughly normally distributed with the mean and standard deviation we expected. Repeat parts a) and b) for \\(n=20\\) and \\(n=50\\). Describe what you notice. Make sure all three plots are plotted on the same \\(x\\)-axis scale. You can use facets if you combine your data into one tibble. When \\(n=20\\): \\[ \\bar{X}\\overset{approx}{\\sim}\\textsf{Norm}(3.5,0.382) \\] results2&lt;-do(10000)*mean(sample(6,20,replace=T)) results2 %&gt;% gf_histogram(~mean,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Test statistic&quot;) favstats(~mean,data=results2) ## min Q1 median Q3 max mean sd n missing ## 2.15 3.25 3.5 3.75 4.95 3.49896 0.3828754 10000 0 When \\(n=50\\): \\[ \\bar{X}\\overset{approx}{\\sim}\\textsf{Norm}(3.5,0.242) \\] results3&lt;-do(10000)*mean(sample(6,50,replace=T)) results3 %&gt;% gf_histogram(~mean,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Test statistic&quot;) favstats(~mean,data=results3) ## min Q1 median Q3 max mean sd n missing ## 2.54 3.34 3.5 3.66 4.36 3.49852 0.2423665 10000 0 Now let’s put them all together to make it easier to compare. final_results&lt;-rbind(cbind(results,n=10),cbind(results2,n=20),cbind(results3,n=50)) final_results %&gt;% gf_dhistogram(~mean|n,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Test statistic&quot;) favstats(~mean|n,data=final_results) %&gt;% select(mean,sd,n) ## mean sd n ## 1 3.51278 0.7722540 10 ## 2 3.49896 0.3828754 20 ## 3 3.49852 0.2423665 50 All results were as expected. As \\(n\\) increased, the variance of the sample mean decreased. 19.2.2 Problem 2 The nutrition label on a bag of potato chips says that a one ounce (28 gram) serving of potato chips has 130 calories and contains ten grams of fat, with three grams of saturated fat. A random sample of 35 bags yielded a sample mean of 134 calories with a standard deviation of 17 calories. Is there evidence that the nutrition label does not provide an accurate measure of calories in the bags of potato chips? The conditions necessary for applying the normal model have been checked and are satisfied. The question has been framed in terms of two possibilities: the nutrition label accurately lists the correct average calories per bag of chips or it does not, which may be framed as a hypothesis test. Write the null and alternative hypothesis. \\(H_0\\): The average is listed correctly. \\(\\mu = 130\\) \\(H_A\\): The nutrition label is incorrect. \\(\\mu \\neq 130\\) What level of significance are you going to use? I am going to use \\(\\alpha = 0.05\\). What is the distribution of the test statistic \\({\\bar{X}-\\mu\\over S/\\sqrt{n}}\\)? Calculate the observed value. The distribution of the test statistic is \\(t\\) with 34 degrees of freedom. The observed average is \\(\\bar{x} = 134\\) and the standard error may be calculated as \\(SE = \\frac{17}{\\sqrt{35}} = 2.87\\). We can compute a test statistic as the t score: \\[ t = \\frac{134 - 130}{2.87} = 1.39 \\] d. Calculate a p-value. The upper-tail area is 0.0823, pt(1.39,34,lower.tail = F) ## [1] 0.08678153 or 1-pt(1.39,34) ## [1] 0.08678153 so the p-value is \\(2 \\times 0.0823 = 0.1646\\). Draw a conclusion. Since the p-value is larger than 0.05, we do not reject the null hypothesis. That is, there is not enough evidence to show the nutrition label has incorrect information. 19.2.2.1 Extra material If we had used a normal model based on the CLT our p-value would have been close to the value from the \\(t\\) because our sample size is large. pnorm(1.39,lower.tail = F) ## [1] 0.08226444 19.2.3 Problem 3 Exploration of the chi-squared and \\(t\\) distributions. In R, plot the pdf of a random variable with the chi-squared distribution with 1 degree of freedom. On the same plot, include the pdfs with degrees of freedom of 5, 10 and 50. Describe how the behavior of the pdf changes with increasing degrees of freedom. gf_dist(&quot;chisq&quot;,df=1,col=1) %&gt;% gf_dist(&quot;chisq&quot;,df=5,col=2) %&gt;% gf_dist(&quot;chisq&quot;,df=10,col=3) %&gt;% gf_dist(&quot;chisq&quot;,df=50,col=4) %&gt;% gf_lims(y=c(0,.25)) %&gt;% gf_labs(y=&quot;f(x)&quot;) %&gt;% gf_theme(theme_classic()) The “bump” moves to the rights as the degrees of freedom increase. The plot should have a legend, but I could not find a way to do it within ggformula so here it is in ggplot. ggplot(data = data.frame(x = c(0, 75)), aes(x)) + stat_function(fun = dchisq, n = 101, args = list(df = 1), mapping=aes(col=&quot;myline1&quot;)) + stat_function(fun = dchisq, n = 101, args = list(df = 5), mapping=aes(col=&quot;myline2&quot;)) + stat_function(fun = dchisq, n = 101, args = list(df = 10), mapping=aes(col=&quot;myline3&quot;)) + stat_function(fun = dchisq, n = 101, args = list(df = 50), mapping=aes(col=&quot;myline4&quot;)) + ylab(&quot;&quot;) + scale_y_continuous(breaks = NULL) + theme_classic()+ scale_colour_manual(name=&quot;Legend&quot;, values=c(myline1=&quot;black&quot;, myline2=&quot;red&quot;, myline3=&quot;green&quot;, myline4=&quot;blue&quot;), labels=c(&quot;df=1&quot;,&quot;df=5&quot;,&quot;df=10&quot;,&quot;df=50&quot;)) Repeat part (a) with the \\(t\\) distribution. Add the pdf of a standard normal random variable as well. What do you notice? gf_dist(&quot;t&quot;,df=1,col=&quot;black&quot;) %&gt;% gf_dist(&quot;t&quot;,df=5,col=&quot;red&quot;) %&gt;% gf_dist(&quot;t&quot;,df=10,col=&quot;green&quot;) %&gt;% gf_dist(&quot;t&quot;,df=50,col=&quot;blue&quot;) %&gt;% gf_dist(&quot;norm&quot;,lty=2,lwd=1.5) %&gt;% gf_lims(x=c(-4,4)) %&gt;% gf_labs(y=&quot;f(x)&quot;) %&gt;% gf_theme(theme_classic()) As degrees of freedom increases, the \\(t\\)-distribution approaches the standard normal distribution. ggplot(data = data.frame(x = c(-4, 4)), aes(x)) + stat_function(fun = dt, n = 101, args = list(df = 1), mapping=aes(col=&quot;myline1&quot;)) + stat_function(fun = dt, n = 101, args = list(df = 5), mapping=aes(col=&quot;myline2&quot;)) + stat_function(fun = dt, n = 101, args = list(df = 10), mapping=aes(col=&quot;myline3&quot;)) + stat_function(fun = dt, n = 101, args = list(df = 50), mapping=aes(col=&quot;myline4&quot;)) + stat_function(fun = dnorm, n = 101, args = list(mean=0,sd=1), linetype=&quot;dashed&quot;, mapping=aes(col=&quot;myline5&quot;)) + ylab(&quot;&quot;) + scale_y_continuous(breaks = NULL) + theme_classic()+ scale_colour_manual(name=&quot;Legend&quot;, values=c(myline1=&quot;black&quot;, myline2=&quot;red&quot;, myline3=&quot;green&quot;, myline4=&quot;blue&quot;, myline5=&quot;grey&quot;), labels=c(&quot;df=1&quot;,&quot;df=5&quot;,&quot;df=10&quot;,&quot;df=50&quot;,&quot;Normal&quot;)) 19.2.4 Problem 4 . In this lesson, we have used the expression degrees of freedom a lot. What does this expression mean? When we have sample of size \\(n\\), why are there \\(n-1\\) degrees of freedom for the \\(t\\) distribution? Give a short concise answer (about one paragraph). You will likely have to do a little research on your own. Answers will vary. One possible explanation is that the degrees of freedom represents the number of independent pieces of information. For example, you’ll notice that in order to get an unbiased estimate of \\(\\sigma^2\\), we have to divide by \\(n-1\\). This is because in order to estimate \\(\\sigma^2\\), we need to first estimate \\(\\mu\\), which is done by obtaining the sample mean. Once we know the sample mean, we only have \\(n-1\\) pieces of independent information. For example, suppose we have a sample of size 10, and we know the sample mean. Once we are given the first 9 observations, we know exactly what the 10th observation must be. 19.2.5 Problem 5 . Deborah Toohey is running for Congress, and her campaign manager claims she has more than 50% support from the district’s electorate. Ms. Toohey’s opponent claimed that Ms. Toohey has less than 50%. Set up a hypothesis test to evaluate who is right. Should we run a one-sided or two-sided hypothesis test? We should run a two-sided. She could be greater than 50% regardless of what the opponent claims. Write the null and alternative hypothesis. \\(H_0\\): Ms. Toohey’s support is 50%. \\(p = 0.50\\). \\(H_A\\): Ms. Toohey’s support is either above or below 50%. \\(p \\neq 0.50\\). What level of significance are you going to use? \\(\\alpha = 0.05\\) What are the assumptions of this test? The observations are independent. There are at least 10 votes for and 10 against. Because this is a simple random sample that includes fewer than 10% of the population, the observations are independent. In a single proportion hypothesis test, the success-failure condition is checked using the null proportion, \\(p_0=0.5\\): \\(np_0 = n(1-p_0) = 500\\times 0.5 = 250 \\geq 10\\). With these conditions verified, the normal model based on the CLT may be applied to \\(\\hat{p}\\). Calculate the test statistic. A newspaper collects a simple random sample of 500 likely voters in the district and estimates Toohey’s support to be 52%. The test statistic is \\(\\bar{x}=0.52\\) Calculate a p-value. Based on the normal model, we can compute a one-sided p-value and then double to get the correct p-value. The standard error can be computed. The null value is used again here, because this is a hypothesis test for a single proportion with the specified value for the probability of success. \\[SE = \\sqrt{\\frac{p_0\\times (1-p_0)}{n}} = \\sqrt{\\frac{0.5\\times (1-0.5)}{500}} = 0.022\\] 2*pnorm(.52,mean=.5,sd=0.022,lower.tail = FALSE) ## [1] 0.3633021 Draw a conclusion. Because the p-value is larger than 0.05, we do not reject the null hypothesis, and we do not find convincing evidence to support the campaign manager’s claim. "],["CI.html", "Chapter 20 Confidence Intervals 20.1 Objectives 20.2 Homework", " Chapter 20 Confidence Intervals 20.1 Objectives Using asymptotic methods based on the normal distribution, construct and interpret a confidence interval for an unknown parameter. Describe the relationships between confidence intervals, confidence level, and sample size. For proportions, be able to calculate the three different approaches for confidence intervals using R. 20.2 Homework 20.2.1 Problem 1 Chronic illness In 2013, the Pew Research Foundation reported that “45% of U.S. adults report that they live with one or more chronic conditions.”8 However, this value was based on a sample, so it may not be a perfect estimate for the population parameter of interest on its own. The study reported a standard error of about 1.2%, and a normal model may reasonably be used in this setting. Create a 95% confidence interval for the proportion of U.S. adults who live with one or more chronic conditions. Also interpret the confidence interval in the context of the study. \\(0.45 \\pm 1.96 \\times 0.012 = (0.426, 0.474)\\) We are 95% confident that 42.6% to 47.4% of U.S. adults live with one or more chronic conditions. Create a 99% confidence interval for the proportion of U.S. adults who live with one or more chronic conditions. Also interpret the confidence interval in the context of the study. qnorm(.995) ## [1] 2.575829 0.45 +c(-1,1)*qnorm(.995)*0.012 ## [1] 0.41909 0.48091 We are 99% confident that 41.9% to 48.1% of U.S. adults live with one or more chronic conditions. Identify each of the following statements as true or false. Provide an explanation to justify each of your answers. We can say with certainty that the confidence interval from part a contains the true percentage of U.S. adults who suffer from a chronic illness. False, we’re only 95% confident. If we repeated this study 1,000 times and constructed a 95% confidence interval for each study, then approximately 950 of those confidence intervals would contain the true fraction of U.S. adults who suffer from chronic illnesses. True, this is the definition of the confidence level. The poll provides statistically significant evidence (at the \\(\\alpha = 0.05\\) level) that the percentage of U.S. adults who suffer from chronic illnesses is not 50%. True, the equivalent significance level of a two-sided hypothesis test for a 95% confidence interval is indeed 5%, and since the interval lies below 50% this statement is correct. Since the standard error is 1.2%, only 1.2% of people in the study communicated uncertainty about their answer. False, the 1.2% measures the uncertainty associated with the sample proportion (the point estimate) not the uncertainty of individual observations, uncertainty in the sense of not being sure of one’s answer to a survey question. Suppose the researchers had formed a one-sided hypothesis, they believed that the true proportion is less than 50%. We could find an equivalent one-sided 95% confidence interval by taking the upper bound of our two-sided 95% confidence interval. False. In constructing a one-sided confidence interval we need \\(\\alpha\\) to be in one tail. Only taking the upper value of a two-sided 95% confidence interval leads to a 97.5% one-sided confidence interval, a more conservative value. qnorm(.95) ## [1] 1.644854 0.45 +qnorm(.95)*0.012 ## [1] 0.4697382 Notice that 46.9 is smaller than 47.4. 20.2.2 Problem 2 Vegetarian college students Suppose that 8% of college students are vegetarians. Determine if the following statements are true or false, and explain your reasoning. The distribution of the sample proportions of vegetarians in random samples of size 60 is approximately normal since \\(n \\ge 30\\). FALSE. For the distribution of \\(\\hat{p}\\) to be approximately normal, we need to have at least 10 successes and 10 failures in our sample. The distribution of the sample proportions of vegetarian college students in random samples of size 50 is right skewed. TRUE. The success-failure condition is not satisfied \\[np = 50 \\times 0.08 = 4\\] and \\[n(1 - p) = 50 \\times 0.92 = 46\\] therefore we know that the distribution of \\(\\hat{p}\\) is not approximately normal. In most samples we would expect \\(\\hat{p}\\) to be close to 0.08, the true population proportion. While \\(\\hat{p}\\) can be as high as 1 (though we would expect this to effectively never happen), it can only go as low as 0. Therefore the distribution would probably take on a right-skewed shape. Plotting the sampling distribution would confirm this suspicion. A random sample of 125 college students where 12% are vegetarians would be considered unusual. FALSE. \\[ SE_{\\hat{p}} \\approx \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\] \\[=\\sqrt{\\frac{.08(.92)}{125}} = 0.0243\\] A \\(\\hat{p}\\) of 0.12 is only \\(\\frac{0.12 - 0.08}{0.0243} = 1.65\\) standard errors away from the mean, which would not be considered unusual. The p-value is: 2*(1-pnorm(1.65)) ## [1] 0.09894294 A random sample of 250 college students where 12% are vegetarians would be considered unusual. TRUE. \\[ SE_{\\hat{p}} \\approx \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\] \\[=\\sqrt{\\frac{.08(.92)}{250}} = 0.0172\\] Notice that doubling the sample size only reduced the standard error by \\(\\sqrt{2}\\). A \\(\\hat{p}\\) of 0.12 is \\(\\frac{0.12 - 0.08}{0.0172} = 2.32\\) standard errors away from the mean, which might be considered unusual. The p-value is: 2*(1-pnorm(2.32)) ## [1] 0.02034088 The standard error would be reduced by one-half if we increased the sample size from 125 to 250. FALSE. Since n appears under the square root sign in the formula for the standard error, increasing the sample size from 125 to 250 would decrease the standard error of the sample proportion only by a factor of \\(\\sqrt{2}\\). A 99% confidence will be wider than a 95% because to have a higher confidence level requires a wider interval. TRUE. The width is a function of the margin of error. Keeping all else the same, the critical values for 95% and 99% are temp&lt;-qnorm(c(.985,.995)) names(temp)&lt;-c(&quot;95%&quot;,&quot;99%&quot;) temp ## 95% 99% ## 2.170090 2.575829 20.2.3 Problem 3 Orange tabbies Suppose that 90% of orange tabby cats are male. Determine if the following statements are true or false, and explain your reasoning. The distribution of sample proportions of random samples of size 30 is left skewed. TRUE. The success-failure condition is not satisfied \\[n\\hat{p} = 30 \\times 0.90 = 27\\] and \\[n(1-\\hat{p}) = 30 \\times 0.10 = 3;\\] therefore we know that the distribution of \\(\\hat{p}\\) is not nearly normal. In most samples we would expect \\(\\hat{p}\\) to be close to 0.90, the true population proportion. While \\(\\hat{p}\\) can be as low as 0 (though we would expect this to happen very rarely), it can only go as high as 1. Therefore the distribution would probably take on a left-skewed shape. Plotting the sampling distribution would confirm this suspicion. Using a sample size that is 4 times as large will reduce the standard error of the sample proportion by one-half. TRUE. Since \\(n\\) appears in a square root for \\(SE\\), using a sample size that is 4 times as large will reduce the \\(SE\\) by half. The distribution of sample proportions of random samples of size 140 is approximately normal. TRUE. The success-failure condition is satisfied \\[n\\hat{p} = 140 \\times 0.90 = 126\\] and \\[n(1-\\hat{p}) = 140 \\times 0.10 = 14;\\] therefore the distribution of \\(\\hat{p}\\) is nearly normal. 20.2.4 Problem 4 Working backwards A 90% confidence interval for a population mean is (65,77). The population distribution is approximately normal and the population standard deviation is unknown. This confidence interval is based on a simple random sample of 25 observations. Calculate the sample mean, the margin of error, and the sample standard deviation. The sample mean is the midpoint of the confidence interval: \\[\\bar{x} = \\frac{65+77}{2} = 71\\] The margin of error is half the width of the confidence interval: \\[ME = \\frac{ \\left(77 - 65 \\right)}{2} = 6 \\] Using df = 25 - 1 = 24 and the confidence level of 90% we can find the critical value from the t-table distribution. qt(.95,24) ## [1] 1.710882 Lastly, using the margin of error and the critical value we can solve for \\(s\\): \\[ ME = t_{24}\\times \\frac{s}{\\sqrt{n}}\\] \\[6 = 1.71\\times \\frac{s}{\\sqrt{25}}\\] \\[s = 17.54\\] 20.2.5 Problem 5 Find the p-value An independent random sample is selected from an approximately normal population with an unknown standard deviation. Find the p-value for the given set of hypotheses and \\(T\\) test statistic. Also determine if the null hypothesis would be rejected at \\(\\alpha = 0.05\\). \\(H_{A}: \\mu &gt; \\mu_{0}\\), \\(n = 11\\), \\(T = 1.91\\) 1-pt(1.91,10) ## [1] 0.04260244 The p-value is less than 0.05, reject the null hypothesis. \\(H_{A}: \\mu &lt; \\mu_{0}\\), \\(n = 17\\), \\(T = - 3.45\\) pt(-3.45,16) ## [1] 0.001646786 The p-value is less than 0.05, reject the null hypothesis. \\(H_{A}: \\mu \\ne \\mu_{0}\\), \\(n = 7\\), \\(T = 0.83\\) 2*(1-pt(0.83,6)) ## [1] 0.4383084 The p-value is greater than 0.05, fail to reject the null hypothesis. \\(H_{A}: \\mu &gt; \\mu_{0}\\), \\(n = 28\\), \\(T = 2.13\\) 1-pt(2.13,27) ## [1] 0.02121769 The p-value is less than 0.05, reject the null hypothesis. 20.2.6 Problem 6 Sleep habits of New Yorkers New York is known as “the city that never sleeps.” A random sample of 25 New Yorkers were asked how much sleep they get per night. Statistical summaries of these data are shown below. Do these data provide strong evidence that New Yorkers sleep less than 8 hours a night on average? \\[ \\begin{array}{cccccc} &amp; &amp; &amp; &amp; &amp; \\\\&amp; n &amp; \\bar{x} &amp; s &amp; min &amp; max \\\\ &amp;\\hline 25 &amp; 7.73 &amp; 0.77 &amp; 6.17 &amp; 9.78 \\\\ &amp; &amp; &amp; &amp; &amp; \\end{array} \\] Write the hypotheses in symbols and in words. \\(H_0: \\mu = 8\\) New Yorkers sleep 8 hrs per night on average. \\(H_A: \\mu &lt; 8\\) New Yorkers sleep less than 8 hrs per night on average. Check conditions, then calculate the test statistic, \\(T\\), and the associated degrees of freedom. Independence: The sample is random and 25 is less than 10% of all New Yorkers, so it seems reasonable that the observations are independent. Sample size: Sample size is less than 30, therefore we use a t-test which implies the population must be normal. Skew: We don’t have the data to look at a qq plot so we will make some guesses. All observations are within three standard deviations of the mean. For now we will proceed while acknowledging that we are assuming the skew is perhaps moderate or less (moderate skew would be acceptable for this sample size). The test statistic and degrees of freedom can be calculated as follows: \\[T = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}} = \\] \\[\\frac{7.73 - 8}{\\frac{0.77}{\\sqrt{25}}} = - 1.75\\] \\(df = 25 - 1 = 24\\). Find and interpret the p-value in this context. pt(-1.75,24) ## [1] 0.04644754 If in fact the true population mean of the amount New Yorkers sleep per night was 8 hours, the probability of getting a random sample of 25 New Yorkers where the average amount of sleep is 7.73 hrs per night or less is 0.046. This p-value is close to 0.05. What is the conclusion of the hypothesis test? Since the p-value is less than 0.05, we reject the null hypothesis that New Yorkers sleep an average of 8 hours per night in favor of the alternative that they sleep less than 8 hours per night on average. However, the p-value is close to the significance level and we may want to run the study again and/or look at sample sizes to determine how big of a difference from 8 hours is important from a practical standpoint. Let’s look at the power of this test. Remember that power is the probability of rejecting the null when the alternative is true. Since the alternative specifies a range of values for the parameter, we must specify a value. This is done by subject matter experts. How much of a difference in average sleep is needed from a practical standpoint to say it is different? Let’s say that a half an hour is important to detect. We will use the function power.t.test() to determine the power of our test. power.t.test(n=25,delta=.5,sd=.77,alternative = &quot;one.sided&quot;,type=&quot;one.sample&quot;) ## ## One-sample t test power calculation ## ## n = 25 ## delta = 0.5 ## sd = 0.77 ## sig.level = 0.05 ## power = 0.9342637 ## alternative = one.sided This is a high level of power, the typical value used by researchers is 80%. Let’s see what the power is for a 15 minute difference. power.t.test(n=25,delta=.25,sd=.77,alternative = &quot;one.sided&quot;,type=&quot;one.sample&quot;) ## ## One-sample t test power calculation ## ## n = 25 ## delta = 0.25 ## sd = 0.77 ## sig.level = 0.05 ## power = 0.4731184 ## alternative = one.sided This is a much lower power and thus not an effective test with that sample size for finding that small of a difference. We can turn the problem around and ask what sample size we need for 80% power? power.t.test(power=.8,delta=.25,sd=.77,alternative = &quot;one.sided&quot;,type=&quot;one.sample&quot;) ## ## One-sample t test power calculation ## ## n = 60.02642 ## delta = 0.25 ## sd = 0.77 ## sig.level = 0.05 ## power = 0.8 ## alternative = one.sided We need 60 subjects in the study. Construct a 95% confidence interval that corresponded to this hypothesis test, would you expect 8 hours to be in the interval? We need an 95% upper bound on the confidence interval. \\[\\bar{x} + t_{24,0.95}{\\frac{s}{\\sqrt{n}}} = \\] The critical value is qt(.95,24) ## [1] 1.710882 So the upper confidence bound is 7.73+qt(.95,24)*0.77/sqrt(25) ## [1] 7.993476 The value of 8 is not included in the interval, but it is close. We need more data or another study to be confident in our results. 20.2.7 Problem 7 Vegetarian college students II From problem 2 part c, suppose that it has been reported that 8% of college students are vegetarians. We think USAFA is not typical because of their fitness and health awareness, we think there are more vegetarians. We collect a random sample of 125 cadets and find 12% claimed they are vegetarians. Is there enough evidence to claim that USAFA cadets are different? Use binom.test() to conduct the hypothesis test and find a confidence interval. binom.test(x=15,n=125,p=.08,alternative = &quot;greater&quot;) ## ## ## ## data: 15 out of 125 ## number of successes = 15, number of trials = 125, p-value = 0.07483 ## alternative hypothesis: true probability of success is greater than 0.08 ## 95 percent confidence interval: ## 0.07544411 1.00000000 ## sample estimates: ## probability of success ## 0.12 We fail to reject. Notice 0.08 is in the interval. Use prop.test() with correct=FALSE to conduct the hypothesis test and find a confidence interval. prop.test(x=15,n=125,p=.08,alternative = &quot;greater&quot;,correct=FALSE) ## ## 1-sample proportions test without continuity correction ## ## data: 15 out of 125 ## X-squared = 2.7174, df = 1, p-value = 0.04963 ## alternative hypothesis: true p is greater than 0.08 ## 95 percent confidence interval: ## 0.08007111 1.00000000 ## sample estimates: ## p ## 0.12 We reject, what is going on? Use prop.test() with correction=TRUE to conduct the hypothesis test and find a confidence interval. prop.test(x=15,n=125,p=.08,alternative = &quot;greater&quot;,correct=TRUE) ## ## 1-sample proportions test with continuity correction ## ## data: 15 out of 125 ## X-squared = 2.2011, df = 1, p-value = 0.06896 ## alternative hypothesis: true p is greater than 0.08 ## 95 percent confidence interval: ## 0.07682087 1.00000000 ## sample estimates: ## p ## 0.12 We fail to reject. Which test should you use? Go into the help for binom.test() in R and it explains these intervals. The way to compare these is to look at coverage. By this we mean for a 95% confidence interval, does the interval include the true parameter 95% of the time? This can be checked by simulation, but mathematics are really needed for a more definitive answer. The first test is an exact test and as stated in the help, it guarantees that the coverage rate is at least 95%. This means the interval tends to be too large. This is the largest interval and tends to be conservative. The second test is the Score test, also called the Wilson. It is found by inverting the p-value, beyond the scope of this class. This interval is a nice compromise in its coverage. The third is still a score but in the calculation of the p-value, a continuity correction is applied. This correction takes the limits on the binomial and extends it 0.5 in each direction. This is done to give the discrete binomial a better approximation to the continuous normal in the CLT. This interval is the default in R via prop.test(). None of these are our simple confidence interval based on the normal approximation. Here is the code for it: binom.test(x=15,n=125,p=.08,alternative = &quot;greater&quot;,ci.method = &quot;Wald&quot;) ## ## Exact binomial test (Wald CI) ## ## data: 15 out of 125 ## number of successes = 15, number of trials = 125, p-value = 0.07483 ## alternative hypothesis: true probability of success is greater than 0.08 ## 95 percent confidence interval: ## 0.0721916 1.0000000 ## sample estimates: ## probability of success ## 0.12 15/125-qnorm(.95)*sqrt(.12*.88/125) ## [1] 0.0721916 This interval is not very good. The coverage is poor especially for small sample sizes. To learn more, read Approximate is better then exact for interval estimation of binomial proportions. A. Agresti and B. A. Coull, American Statistician 52, 1998, 119-126. http://pewinternet.org/Reports/2013/The-Diagnosis-Difference.aspx The Diagnosis Difference. November 26, 2013. Pew Research.↩︎ "],["BOOT.html", "Chapter 21 Bootstrap 21.1 Objectives 21.2 Homework", " Chapter 21 Bootstrap 21.1 Objectives Use the bootstrap to estimate the standard error, the standard deviation, of the sample statistic. Using bootstrap methods, obtain and interpret a confidence interval for an unknown parameter, based on a random sample. Describe the advantages, disadvantages, and assumptions behind using bootstrapping for confidence intervals. 21.2 Homework 21.2.1 Problem 1 Poker An aspiring poker player recorded her winnings and losses over 50 evenings of play, the data is in the openintro package in the object poker. The poker player would like to better understand the volatility in her long term play. Load the data and plot a histogram. poker&lt;-read_csv(&quot;data/poker.csv&quot;) poker %&gt;% gf_histogram(~winnings,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Winnings&quot;) Find the summary statistics. favstats(~winnings,data=poker) ## min Q1 median Q3 max mean sd n missing ## -1000 -187 11 289 3712 90.08 703.6835 50 0 Mean absolute deviation or MAD is a more intuitive measure of spread than variance. It directly measures the average distance from the mean. It is found by the formula: \\[mad = \\sum_{i=1}^{n}\\frac{\\left| x_{i} - \\bar{x} \\right|}{n}\\] Write a function and find the MAD of the data. mad&lt;-function(x){ xbar&lt;-mean(x) sum(abs(x-xbar))/length(x) } obs&lt;-mad(poker$winnings) obs ## [1] 394.1792 Find the bootstrap distribution of the MAD using 1000 replicates. set.seed(1122) results&lt;-do(1000)*mad(resample(poker$winnings)) Plot a histogram of the bootstrap distribution. results %&gt;% gf_histogram(~mad,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Mean absolute deviation&quot;) Report a 95% confidence interval on the MAD. cdata(~mad,data=results) ## lower upper central.p ## 2.5% 243.9448 636.0925 0.95 ADVANCED: Do you think sample MAD is an unbiased estimator of population MAD? Why or why not? We don’t know without doing some math. We do know that the sample standard deviation is biased and part of that is because we have to use the sample mean in its calculation. We are doing the same thing here, so our estimate might also be biased for the same reason. 21.2.2 Problem 2 Bootstrap hypothesis testing Bootstrap hypothesis testing is relatively undeveloped, and is generally not as accurate as permutation testing. Therefore in general avoid it. But for our problem in the reading, it may work. We will sample in a way that is consistent with the null hypothesis, then calculate a p-value as a tail probability like we do in permutation tests. This example does not generalize well to other applications like relative risk, correlation, regression, or categorical data. Using the HELPrct data set, store the observed value of the difference of means for male and female. I am going to just select the two columns I need. HELP_sub &lt;- HELPrct %&gt;% select(age,sex) obs &lt;- diffmean(age~sex,data=HELP_sub) obs ## diffmean ## -0.7841284 The null hypothesis requires the means of each group to be equal. Pick one group to adjust, either male or female. First zero the mean of the selected group by subtracting the sample mean of this group from data points only in this group. Then add the sample mean of the other group to each data point in the selected group. Store in a new object called HELP_null. This is tricky, we are doing some data wrangling here. means&lt;-mean(age~sex,data=HELP_sub) means ## female male ## 36.25234 35.46821 means[&#39;female&#39;] ## female ## 36.25234 Let’s get all the female observations and adjust the mean to equal that of the males. H_female &lt;- HELP_sub %&gt;% filter(sex==&quot;female&quot;) %&gt;% mutate(age=age-means[&#39;female&#39;]+means[&#39;male&#39;]) mean(~age,data=H_female) ## [1] 35.46821 Combine back into one data set. HELP_sub_new&lt;-HELP_sub %&gt;% filter(sex==&quot;male&quot;) %&gt;% rbind(H_female) Run favstats() to check that the means are equal. favstats(age~sex,data=HELP_sub_new) ## sex min Q1 median Q3 max mean sd n ## 1 female 20.21587 30.21587 34.21587 39.71587 57.21587 35.46821 7.584858 107 ## 2 male 19.00000 30.00000 35.00000 40.00000 60.00000 35.46821 7.750110 346 ## missing ## 1 0 ## 2 0 On this new adjusted data set, generate a bootstrap distribution of the difference in sample means. set.seed(1159) results&lt;-do(1000)*diffmean(age~sex,data=resample(HELP_sub_new)) Plot the bootstrap distribution and a line at the observed difference in sample means. results %&gt;% gf_histogram(~diffmean,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_vline(xintercept=obs) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Difference in means&quot;) ## Warning: geom_vline(): Ignoring `mapping` because `xintercept` was provided. Find a p-value. 2*prop1(~(diffmean&lt;=obs),data=results) ## prop_TRUE ## 0.3476523 How does the p-value compare with those in the reading. This is a similar p-value. 21.2.3 Problem 3 Paired data Are textbooks actually cheaper online? Here we compare the price of textbooks at the University of California, Los Angeles’ (UCLA’s) bookstore and prices at Amazon.com. Seventy-three UCLA courses were randomly sampled in Spring 2010, representing less than 10% of all UCLA courses. When a class had multiple books, only the most expensive text was considered. The data is in the file textbooks.csv under the data folder. textbooks&lt;-read_csv(&quot;data/textbooks.csv&quot;) ## Rows: 73 Columns: 7 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): dept_abbr, course, isbn, more ## dbl (3): ucla_new, amaz_new, diff ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(textbooks) ## # A tibble: 6 × 7 ## dept_abbr course isbn ucla_new amaz_new more diff ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Am Ind C170 978-0803272620 27.7 28.0 Y -0.28 ## 2 Anthro 9 978-0030119194 40.6 31.1 Y 9.45 ## 3 Anthro 135T 978-0300080643 31.7 32 Y -0.32 ## 4 Anthro 191HB 978-0226206813 16 11.5 Y 4.48 ## 5 Art His M102K 978-0892365999 19.0 14.2 Y 4.74 ## 6 Art His 118E 978-0394723693 15.0 10.2 Y 4.78 Each textbook has two corresponding prices in the data set: one for the UCLA bookstore and one for Amazon. Therefore, each textbook price from the UCLA bookstore has a natural correspondence with a textbook price from Amazon. When two sets of observations have this special correspondence, they are said to be paired. To analyze paired data, it is often useful to look at the difference in outcomes of each pair of observations. In textbooks, we look at the difference in prices, which is represented as the diff variable. It is important that we always subtract using a consistent order; here Amazon prices are always subtracted from UCLA prices. Is this data tidy? Explain. Yes, because each row is a textbook and each column is a variable. Make a scatterplot of the UCLA price versus the Amazon price. Add a 45 degree line to the plot. textbooks %&gt;% gf_point(ucla_new~amaz_new) %&gt;% gf_abline(slope=1,intercept = 0,color=&quot;darkblue&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(x=&quot;Amazon&quot;,y=&quot;UCLA&quot;) It appears the books at the UCLA bookstore are more expensive. One way to test this is with a regression model; we will learn about in the next block. Make a histogram of the differences in price. textbooks %&gt;% gf_histogram(~diff,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Distribution of price differences&quot;, x=&quot;Price difference between UCLA and Amazon&quot;) The distribution is skewed. The hypotheses are: \\(H_0\\): \\(\\mu_{diff}=0\\). There is no difference in the average textbook price. \\(H_A\\): \\(\\mu_{diff} \\neq 0\\). There is a difference in average prices. To use a \\(t\\) distribution, the variable diff has to be independent and normally distributed. Since the 73 books represent less than 10% of the population, the assumption that the random sample is independent is reasonable. Check normality using qqnorsim() from the openintro package. It generates 8 qq plots of simulated normal data that you can use to judge the diff variable. qqnormsim(diff,textbooks) The normality assumption is suspect but we have a large sample so it should be acceptable to use the \\(t\\). Run a \\(t\\) test on the diff variable. Report the p-value and conclusion. t_test(~diff,textbooks) ## ## One Sample t-test ## ## data: diff ## t = 7.6488, df = 72, p-value = 6.928e-11 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 9.435636 16.087652 ## sample estimates: ## mean of x ## 12.76164 We did not have to use the paired option since we already took the difference. Here is an example of using the paired option. t_test(textbooks$ucla_new,textbooks$amaz_new,paired=TRUE) ## ## Paired t-test ## ## data: textbooks$ucla_new and textbooks$amaz_new ## t = 7.6488, df = 72, p-value = 6.928e-11 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 9.435636 16.087652 ## sample estimates: ## mean of the differences ## 12.76164 The p-value is so small that we don’t believe the average price of the books from the UCLA bookstore and Amazon are the same. Create a bootstrap distribution and generate a 95% confidence interval on the mean of the differences, the diff column. textbooks %&gt;% summarise(obs_diff=mean(diff)) ## # A tibble: 1 × 1 ## obs_diff ## &lt;dbl&gt; ## 1 12.8 We need to just pull the difference. obs_stat&lt;- textbooks %&gt;% summarise(obs_diff=mean(diff)) %&gt;% pull(obs_diff) obs_stat ## [1] 12.76164 Next a bootstrap distribution. set.seed(843) results&lt;-do(1000)*mean(~diff,data=resample(textbooks)) results %&gt;% gf_dhistogram(~mean,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_dist(&quot;norm&quot;,mean=12.76,sd=14/sqrt(72),color=&quot;red&quot;) %&gt;% gf_vline(xintercept = obs_stat) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Sampling distribution of the mean of differences in price&quot;, x=&quot;Mean of differences in price&quot;) cdata(~mean,data=results) ## lower upper central.p ## 2.5% 9.583829 16.05705 0.95 Not a bad solution for this problem. If there is really no differences between book sources, the variable more is a binomial and under the null the probably of success is \\(\\pi = 0.5\\). Run a hypothesis test using the variable more. inspect(textbooks) ## ## categorical variables: ## name class levels n missing ## 1 dept_abbr character 41 73 0 ## 2 course character 66 73 0 ## 3 isbn character 73 73 0 ## 4 more character 2 73 0 ## distribution ## 1 Mgmt (8.2%), Pol Sci (6.8%) ... ## 2 10 (4.1%), 101 (2.7%), 180 (2.7%) ... ## 3 978-0030119194 (1.4%) ... ## 4 Y (61.6%), N (38.4%) ## ## quantitative variables: ## name class min Q1 median Q3 max mean sd n ## ...1 ucla_new numeric 10.50 24.70 43.56 116.00 214.5 72.22192 59.65913 73 ## ...2 amaz_new numeric 8.60 20.21 34.95 88.09 176.0 59.46027 48.99557 73 ## ...3 diff numeric -9.53 3.80 8.23 17.59 66.0 12.76164 14.25530 73 ## missing ## ...1 0 ## ...2 0 ## ...3 0 We have 45 books that were more expensive out of the total of 73. prop_test(45,73,p=0.5) ## ## 1-sample proportions test with continuity correction ## ## data: 45 out of 73 ## X-squared = 3.5068, df = 1, p-value = 0.06112 ## alternative hypothesis: true p is not equal to 0.5 ## 95 percent confidence interval: ## 0.4948968 0.7256421 ## sample estimates: ## p ## 0.6164384 Notice that this test failed to reject the null hypothesis. In the paired test, the evidence was so strong but in the binomial model it is not. There is a loss of information making a discrete variable out of a continuous one. Could you use a permutation test on this example? Explain. Yes, but you have to be careful because you want to keep the pairing so you can’t just shuffle the names. You have to shuffle the names within the paired values. This means to simply randomly switch the names within a row. This is easier to do by just multiplying the diff column by a random choice of -1 and 1. sample(c(-1,1),size=73,replace = TRUE) ## [1] 1 -1 -1 1 -1 -1 -1 1 -1 -1 1 -1 -1 1 -1 1 1 1 -1 -1 -1 -1 1 -1 -1 ## [26] -1 -1 1 1 1 1 1 -1 1 1 1 1 1 1 -1 -1 1 1 -1 1 1 1 1 -1 -1 ## [51] -1 -1 -1 1 -1 1 -1 -1 1 1 -1 1 1 -1 -1 -1 1 1 -1 1 -1 1 1 set.seed(406) results &lt;- do(1000)*mean((~diff*sample(c(-1,1),size=73,replace = TRUE)),data=textbooks) results %&gt;% gf_histogram(~mean,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Randomization sampling distribution of mean of differences in price&quot;, x=&quot;Mean of price difference&quot;) prop1((~mean&gt;=obs_stat),data=results) ## prop_TRUE ## 0.000999001 None of the permuted values is at or greater that the observed value. "],["ADDTESTS.html", "Chapter 22 Additional Hypothesis Tests 22.1 Objectives 22.2 Homework", " Chapter 22 Additional Hypothesis Tests 22.1 Objectives Conduct and interpret a hypothesis test for equality of two or more means using both permutation and the \\(F\\) distribution. Conduct and interpret a goodness of fit test using both Pearson’s chi-squared and randomization to evaluate the independence between two categorical variables. Conduct and interpret a hypothesis test for the equality of two variances. Know and check assumptions for the tests in this lesson. 22.2 Homework 22.2.1 Problem 1 Golf balls Repeat the analysis of the golf ball problem from earlier this semester. Load the data and tally the data into a table. The data is in golf_balls.csv. golf_balls &lt;- read_csv(&quot;data/golf_balls.csv&quot;) head(golf_balls) ## # A tibble: 6 × 1 ## number ## &lt;dbl&gt; ## 1 3 ## 2 2 ## 3 1 ## 4 4 ## 5 4 ## 6 3 tally(~number,data=golf_balls) ## number ## 1 2 3 4 ## 137 138 107 104 Using the function chisq.test conduct a hypothesis test of equally likely distribution of balls. You may have to read the help menu. chisq.test(tally(~number,data=golf_balls),p=c(.25,.25,.25,.25)) ## ## Chi-squared test for given probabilities ## ## data: tally(~number, data = golf_balls) ## X-squared = 8.4691, df = 3, p-value = 0.03725 Repeat part b. but assume balls with the numbers 1 and 2 occur 30% of the time and balls with 3 and 4 occur 20%. chisq.test(tally(~number,data=golf_balls),p=c(.3,.3,.2,.2)) ## ## Chi-squared test for given probabilities ## ## data: tally(~number, data = golf_balls) ## X-squared = 2.4122, df = 3, p-value = 0.4914 22.2.2 Problem 2 Bootstrap hypothesis testing Repeat the analysis of the MLB data from the lesson but this time generate a bootstrap distribution of the \\(F\\) statistic. First, read in the data. mlb_obp &lt;- read_csv(&quot;data/mlb_obp.csv&quot;) Convert position to a factor. mlb_obp &lt;- mlb_obp %&gt;% mutate(position=as.factor(position)) Summarize the data. favstats(obp~position,data=mlb_obp) ## position min Q1 median Q3 max mean sd n missing ## 1 C 0.219 0.30000 0.3180 0.35700 0.405 0.3226154 0.04513175 39 0 ## 2 DH 0.287 0.31625 0.3525 0.36950 0.412 0.3477857 0.03603669 14 0 ## 3 IF 0.174 0.30800 0.3270 0.35275 0.437 0.3315260 0.03709504 154 0 ## 4 OF 0.265 0.31475 0.3345 0.35300 0.411 0.3342500 0.02944394 120 0 We need a function to resample the data, we will use the resample() from the mosaic package. library(broom) f_boot &lt;- function(x){ aov(obp~position,data=resample(x)) %&gt;% tidy() %&gt;% summarize(stat=meansq[1]/meansq[2]) %&gt;% pull() } set.seed(541) results&lt;-do(1000)*f_boot(mlb_obp) Let’s plot our sampling distribution. results %&gt;% gf_histogram(~f_boot,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Bootstrap sampling distribution of F test statistic&quot;, x=&quot;Test statistic&quot;) Now the confidence interval for the F-statistic is: cdata(~f_boot,data=results) ## lower upper central.p ## 2.5% 0.3546682 8.724895 0.95 We are 95% confident that the \\(F\\) statistic is in the interval \\((0.35,8.72)\\) which includes 1 so we fail to reject the null hypothesis of equal means. Remember under the null hypothesis the ratio of the variance between means to the pooled variance within categories should be 1. 22.2.3 Problem 3 Test of variance We have not performed a test of variance so we will create our own. Using the MLB from the lesson, subset on IF and OF. mlb_prob3 &lt;- mlb_obp %&gt;% filter(position==&quot;IF&quot;|position==&quot;OF&quot;) %&gt;% droplevels() summary(mlb_prob3) ## position obp ## IF:154 Min. :0.1740 ## OF:120 1st Qu.:0.3100 ## Median :0.3310 ## Mean :0.3327 ## 3rd Qu.:0.3530 ## Max. :0.4370 The function droplevels() gets rid of C and DH in the factor levels. Create a side-by-side boxplot. mlb_prob3 %&gt;% gf_boxplot(obp~position) %&gt;% gf_theme(theme_classic()) The hypotheses are: \\(H_0\\): \\(\\sigma^2_{IF}=\\sigma^2{OF}\\). There is no difference in the variance of on base percentage for infielders and outfielders. \\(H_A\\): \\(\\sigma^2_{IF}\\neq \\sigma^2_{OF}\\). There is a difference in variances. Use the differences in sample standard deviations as your test statistic. Using a permutation test, find the p-value and discuss your decision. mlb_prob3 %&gt;% group_by(position) %&gt;% summarize(stat=sd(obp)) ## # A tibble: 2 × 2 ## position stat ## &lt;fct&gt; &lt;dbl&gt; ## 1 IF 0.0371 ## 2 OF 0.0294 obs &lt;- mlb_prob3 %&gt;% summarize(stat=sd(obp[position==&quot;IF&quot;])-sd(obp[position==&quot;OF&quot;])) %&gt;% pull() obs ## [1] 0.007651101 Let’s write a function to shuffle the position. perm_stat &lt;- function(x){ x %&gt;% mutate(position=shuffle(position)) %&gt;% summarize(stat=sd(obp[position==&quot;IF&quot;])-sd(obp[position==&quot;OF&quot;])) %&gt;% pull() } set.seed(443) results&lt;-do(1000)*perm_stat(mlb_prob3) results %&gt;% gf_histogram(~perm_stat,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_vline(xintercept=obs,color=&quot;red&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Sampling distribution of difference in variances&quot;, subtitle=&quot;Randomization permutation test&quot;, x=&quot;Test statistic&quot;) The p-value is 2*prop1(~(perm_stat&gt;=obs),data=results) ## prop_TRUE ## 0.04395604 This is a two sided test since we did not know in advance which variance would be larger. We reject the hypothesis of equal variance but the p-value is too close to the significance level. The conclusion is suspect. We need more data. Create a bootstrap distribution of the differences in sample standard deviations, and report a 95% confidence interval. Compare with part c.  Let’s write a function. var_stat &lt;- function(x){ resample(x) %&gt;% summarize(stat=sd(obp[position==&quot;IF&quot;])-sd(obp[position==&quot;OF&quot;])) %&gt;% pull() } set.seed(827) results&lt;-do(1000)*var_stat(mlb_prob3) results %&gt;% gf_histogram(~var_stat,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_vline(xintercept=obs,color=&quot;red&quot;)%&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Bootstrap sampling of difference in variances&quot;, x=&quot;Difference in variances&quot;) "],["CS4.html", "Chapter 23 Case Study 23.1 Objectives 23.2 Homework", " Chapter 23 Case Study 23.1 Objectives Using R, generate a linear regression model and use it to produce a prediction model. Using plots, check the assumptions of a linear regression model. 23.2 Homework 23.2.1 Problem 1 HFI Choose another freedom variable and a variable you think would strongly correlate with it. Note: even though some of the variables will appear to be quantitative, they don’t take on enough different values and thus appear to be categorical. So choose with some caution. The openintro package contains the data set hfi. Type ?openintro::hfi in the Console window in RStudio to learn more about the variables. hfi&lt;-read_csv(&quot;data/hfi.csv&quot;) Produce a scatterplot of the two variables. We selected pf_expression_influence as it is a measure of laws and regulations that influence media content. We kept pf_score because it is a measure of personal freedom in a country. Our thought is these should be correlated. gf_lm(pf_score~pf_expression_influence,data=hfi,color=&quot;black&quot;) %&gt;% gf_theme(theme_bw()) %&gt;% gf_point(alpha=0.3) %&gt;% gf_labs(title=&quot;Personal freedom score versus Control on media&quot;, x=&quot;Laws and regulations that influence media content&quot;, y=&quot;Personal freedom score&quot;) Quantify the strength of the relationship with the correlation coefficient. hfi %&gt;% summarise(cor(pf_expression_influence, pf_score, use = &quot;complete.obs&quot;)) ## # A tibble: 1 × 1 ## `cor(pf_expression_influence, pf_score, use = &quot;complete.obs&quot;)` ## &lt;dbl&gt; ## 1 0.787 Fit a linear model. At a glance, does there seem to be a linear relationship? m2 &lt;- lm(pf_score ~ pf_expression_influence, data = hfi) summary(m2) ## ## Call: ## lm(formula = pf_score ~ pf_expression_influence, data = hfi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9688 -0.5830 0.1681 0.5903 3.6730 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.06135 0.05064 99.95 &lt;2e-16 *** ## pf_expression_influence 0.41150 0.00869 47.36 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8482 on 1376 degrees of freedom ## (80 observations deleted due to missingness) ## Multiple R-squared: 0.6197, Adjusted R-squared: 0.6195 ## F-statistic: 2243 on 1 and 1376 DF, p-value: &lt; 2.2e-16 How does this relationship compare to the relationship between pf_expression_control and pf_score? Use the \\(R^2\\) values from the two model summaries to compare. Does your independent variable seem to predict your dependent one better? Why or why not? The adjusted \\(R^2\\) is a little smaller so the fit is not as good. Display the model diagnostics for the regression model analyzing this relationship. Linearity: ggplot(data = m2, aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + labs(x=&quot;Fitted values&quot;,y=&quot;Residuals&quot;,title=&quot;Residual analysis&quot;) + theme_bw() Figure 23.1: Fitted values versus residuals for diagnostics. There does appear to be some type of fluctuation so the linear model may not be appropriate. Nearly normal residuals: ggplot(data = m2, aes(x = .resid)) + geom_histogram(binwidth = .4,fill=&quot;cyan&quot;,color=&quot;black&quot;) + xlab(&quot;Residuals&quot;) + theme_bw() or a normal probability plot of the residuals. ggplot(data = m2, aes(sample = .resid)) + stat_qq() + theme_bw() + geom_abline(slope=1,intercept = 0) No, the sample is small but it appears the residual are skewed to the left. Constant variability: Based on Figure 23.1, the width of the plot seems constant with the exception of some extreme points. The constant variability assumption seems reasonable. Predict the response from your explanatory variable for a value between the median and third quartile. Is this an overestimate or an underestimate, and by how much? summary(hfi$pf_expression_influence) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.000 3.000 5.333 5.200 7.333 9.667 80 predict(m2,newdata=data.frame(pf_expression_influence=6)) ## 1 ## 7.53036 We thus predict a value of 7.53 for the pf_score. The observed value is 7.96, an average of 42 data points. We tend to underestimate the observed value. library(broom) augment(m2) %&gt;% filter(pf_expression_influence==6) %&gt;% summarize(ave=mean(pf_score),n=n()) ## # A tibble: 1 × 2 ## ave n ## &lt;dbl&gt; &lt;int&gt; ## 1 7.96 42 "],["LRBASICS.html", "Chapter 24 Linear Regression Basics 24.1 Objectives 24.2 Homework", " Chapter 24 Linear Regression Basics 24.1 Objectives Obtain parameter estimates of a simple linear regression model given a sample of data. Interpret the coefficients of a simple linear regression. Create a scatterplot with a regression line. Explain and check the assumptions of linear regression. Use and be able to explain all new terms. 24.2 Homework 24.2.1 Problem 1 Nutrition at Starbucks In the data folder is a file named starbucks.csv. Use it to answer the questions below. Create a scatterplot of number of calories and amount of carbohydrates. starbucks &lt;- read_csv(&quot;data/starbucks.csv&quot;) ## Rows: 77 Columns: 7 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): item, type ## dbl (5): calories, fat, carb, fiber, protein ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. glimpse(starbucks) ## Rows: 77 ## Columns: 7 ## $ item &lt;chr&gt; &quot;8-Grain Roll&quot;, &quot;Apple Bran Muffin&quot;, &quot;Apple Fritter&quot;, &quot;Banana… ## $ calories &lt;dbl&gt; 350, 350, 420, 490, 130, 370, 460, 370, 310, 420, 380, 320, 3… ## $ fat &lt;dbl&gt; 8, 9, 20, 19, 6, 14, 22, 14, 18, 25, 17, 12, 17, 21, 5, 18, 1… ## $ carb &lt;dbl&gt; 67, 64, 59, 75, 17, 47, 61, 55, 32, 39, 51, 53, 34, 57, 52, 7… ## $ fiber &lt;dbl&gt; 5, 7, 0, 4, 0, 5, 2, 0, 0, 0, 2, 3, 2, 2, 3, 3, 2, 3, 0, 2, 0… ## $ protein &lt;dbl&gt; 10, 6, 5, 7, 0, 6, 7, 6, 5, 7, 4, 6, 5, 5, 12, 7, 8, 6, 0, 10… ## $ type &lt;chr&gt; &quot;bakery&quot;, &quot;bakery&quot;, &quot;bakery&quot;, &quot;bakery&quot;, &quot;bakery&quot;, &quot;bakery&quot;, &quot;… starbucks %&gt;% gf_point(calories~carb) %&gt;% gf_labs(x=&quot;Carbohydrate Content (g)&quot;,y=&quot;Calories&quot;) %&gt;% gf_theme(theme_classic()) We put calories as the response. Describe the relationship in the graph. There is a positive, moderate, linear association between number of calories and amount of carbohydrates. In addition, the amount of carbohydrates is more variable for menu items with higher calories, indicating non-constant variance. There also appear to be two clusters of data: a patch of about a dozen observations in the lower left and a larger patch on the right side. There might be some natural groupings of the these points. For example, the points in the lower left might come from a light menu. In this scenario, what are the explanatory and response variables? Response: number of calories. Explanatory: amount of carbohydrates (in grams). Why might we want to fit a regression line to these data? With a regression line, we can predict the amount of calories for a given number of carbohydrates. This may be useful if you are concerned about your carb intake and its impact on calorie consumption. Typically you can get both on the menu so this model might not be that valuable. Create a scatterplot of number of calories and amount of carbohydrates with the regression line included. starbucks %&gt;% gf_point(calories~carb) %&gt;% gf_labs(x=&quot;Carbohydrate Content (g)&quot;,y=&quot;Calories&quot;) %&gt;% gf_lm() %&gt;% gf_theme(theme_classic()) Using ’lm()` fit a least squares line to the data. star_mod &lt;- lm(calories~carb,data=starbucks) summary(star_mod) ## ## Call: ## lm(formula = calories ~ carb, data = starbucks) ## ## Residuals: ## Min 1Q Median 3Q Max ## -151.962 -70.556 -0.636 54.908 179.444 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 146.0204 25.9186 5.634 2.93e-07 *** ## carb 4.2971 0.5424 7.923 1.67e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 78.26 on 75 degrees of freedom ## Multiple R-squared: 0.4556, Adjusted R-squared: 0.4484 ## F-statistic: 62.77 on 1 and 75 DF, p-value: 1.673e-11 Report and interpret the slope coefficient. The estimated slope is 4.297 so one additional gram of carbohydrates results in an average increase in calories of 4.297. For a menu item with 51 g of carbs, what is the estimated calorie count? 146.0204+4.2971*51 ## [1] 365.1725 Could we use the model for a menu item with 100 g of carbs? summary(starbucks) ## item calories fat carb ## Length:77 Min. : 80.0 Min. : 0.00 Min. :16.00 ## Class :character 1st Qu.:300.0 1st Qu.: 9.00 1st Qu.:31.00 ## Mode :character Median :350.0 Median :13.00 Median :45.00 ## Mean :338.8 Mean :13.77 Mean :44.87 ## 3rd Qu.:420.0 3rd Qu.:18.00 3rd Qu.:59.00 ## Max. :500.0 Max. :28.00 Max. :80.00 ## fiber protein type ## Min. :0.000 Min. : 0.000 Length:77 ## 1st Qu.:0.000 1st Qu.: 5.000 Class :character ## Median :2.000 Median : 7.000 Mode :character ## Mean :2.221 Mean : 9.481 ## 3rd Qu.:4.000 3rd Qu.:15.000 ## Max. :7.000 Max. :34.000 The maximum carb value is 80 so 100 is outside of the observed data. It would be suspect to extrapolate to that value. Does the assumption of constant variance seem reasonable for this problem? We are going to use the broom package to get the residuals and corresponding independent variable values. You could also get the residuals from the model object and the independent variable values from the original dataframe. library(broom) augment(star_mod) %&gt;% gf_point(.resid~carb) %&gt;% gf_hline(yintercept = 0) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(title=&quot;Residual plot&quot;,x=&quot;Carbohydrates&quot;,y=&quot;Residual&quot;) It seems that the variance in the second group is larger that the first, so it may not be a reasonable assumption. Also note that the linearity assumption is also questionable. Verify that the line passes through the mean carb and mean calories, do this mathematically. 146.0204+4.2971*44.87 ## [1] 338.8313 It checks. What is the estimate of the standard deviation of the residuals? How could you use this information? The estimate is 78.26. If the normal assumption is accurate, we would expect a majority of observations to be within \\(\\pm\\) 78.26 calories of the line. "],["LRINF.html", "Chapter 25 Linear Regression Inference 25.1 Objectives 25.2 Homework", " Chapter 25 Linear Regression Inference 25.1 Objectives Given a simple linear regression model, conduct inference on the coefficients \\(\\beta_0\\) and \\(\\beta_1\\). Given a simple linear regression model, calculate the predicted response for a given value of the predictor. Build and interpret confidence and prediction intervals for values of the response variable. 25.2 Homework 25.2.1 Problem 1 We noticed that the 95% prediction interval was much wider than the 95% confidence interval. In words, explain why this is. The two intervals are describing different parameters. A 95% confidence interval is describing the mean value of the response at a particular value of the predictor. On the other hand, a 95% prediction interval is describing an individual value of the response at a particular value of the predictor. There will be more uncertainty around an individual value than the overall mean. 25.2.2 Problem 2 Beer and blood alcohol content Many people believe that gender, weight, drinking habits, and many other factors are much more important in predicting blood alcohol content (BAC) than simply considering the number of drinks a person consumed. Here we examine data from sixteen student volunteers at Ohio State University who each drank a randomly assigned number of cans of beer. These students were evenly divided between men and women, and they differed in weight and drinking habits. Thirty minutes later, a police officer measured their blood alcohol content (BAC) in grams of alcohol per deciliter of blood. The data is in the bac.csv file under the data folder. Create a scatterplot for cans of beer and blood alcohol level. bac &lt;- read_csv(&quot;data/bac.csv&quot;) bac %&gt;% gf_point(bac~beers) %&gt;% gf_labs(x=&quot;Number of cans of beer&quot;,y=&quot;BAC&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_refine(scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9))) We put BAC as the response since it is more natural to predict BAC from the number of cans of beer consumed. Also notice that the number of cans of beers is really a discrete variable as we can only have whole numbers. Describe the relationship between the number of cans of beer and BAC. The relationship appears to be strong, positive and linear. There is one potential outlier, the student who had 9 cans of beer. We will discuss outliers in the next lesson. Write the equation of the regression line. Interpret the slope and intercept in context. bac_mod &lt;- lm(bac~beers,data=bac) summary(bac_mod) ## ## Call: ## lm(formula = bac ~ beers, data = bac) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.027118 -0.017350 0.001773 0.008623 0.041027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.012701 0.012638 -1.005 0.332 ## beers 0.017964 0.002402 7.480 2.97e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02044 on 14 degrees of freedom ## Multiple R-squared: 0.7998, Adjusted R-squared: 0.7855 ## F-statistic: 55.94 on 1 and 14 DF, p-value: 2.969e-06 \\[\\text{BAC} = -0.0127 + 0.0180 \\times \\text{beers} \\] Slope: For each additional can of beer consumed, the model predicts an additional 0.0180 grams per deciliter BAC on average. Intercept: Students who don’t have any beer are expected to have a blood alcohol content of -0.0127. This value could be interpreted as how much BAC drops in the time between drinking the beers and when BAC is measured. But it also could be that the intercept is really zero but our estimate is different because of variability in our estimator. Do the data provide strong evidence that drinking more cans of beer is associated with an increase in blood alcohol? State the null and alternative hypotheses, report the p-value, and state your conclusion. \\(H_0\\): The true slope coefficient of number of beers is zero (\\(\\beta_1 = 0\\)). \\(H_a\\): The true slope coefficient of number of beers is greater than zero (\\(\\beta_1 &gt; 0\\)). The p-value for the two-sided alternative hypothesis (\\(\\beta_1 \\neq 0\\)) is approximately 0. (Note that this output doesn’t mean the p-value is exactly zero, only that when rounded to four decimal places it is zero.) Therefore the p-value for the one-sided hypothesis will also be very small, it is one half of the p-value reported in the R output. With such a small p-value, we reject \\(H_0\\) and conclude that the data provide convincing evidence that number of cans of beer consumed and blood alcohol content are positively correlated and the true slope parameter is indeed greater than 0. Build a 95% confidence interval for the slope and interpret it in the context of your hypothesis test from part d.  We need a lower confidence bound since the alternative is \\(\\beta_1 &gt; 0\\). For a 95% lower confidence bound, we need at 90% confidence interval and then just ignore the upper value. confint(bac_mod,level=0.9) ## 5 % 95 % ## (Intercept) -0.03495916 0.009557957 ## beers 0.01373362 0.022193906 We are 95% confident that the true value of the slope is greater than 0.014. This bound does not contain 0, so it appears that number of beers and BAC are linearly related in a positive direction. Notice the confidence interval for the intercept does include zero as we discussed above. Suppose we visit a bar, ask people how many drinks they have had, and also take their BAC. Do you think the relationship between number of drinks and BAC would be as strong as the relationship found in the Ohio State study? It would probably be weaker. This study had people of very similar ages, and they also had identical drinks. In bars and elsewhere, drinks vary widely in the amount of alcohol they contain. Predict the average BAC after two beers and build a 90% confidence interval around that prediction. new_bac &lt;- data.frame(beers=2) predict(bac_mod, newdata = new_bac, interval = &#39;confidence&#39;,level=0.9) ## fit lwr upr ## 1 0.02322692 0.008308537 0.0381453 Repeat except build a 90% prediction interval and interpret. predict(bac_mod, newdata = new_bac, interval = &#39;prediction&#39;,level=0.9) ## fit lwr upr ## 1 0.02322692 -0.0157444 0.06219824 We are 90% confident that the BAC of a student who has two beers will be between -.016 and 0.062. Notice that this interval contains an unrealistic level of BAC for the lower limit. If you were briefing this you would make sure to note this. This is because we are using a model based on the normal distribution. A bootstrap may not have this problem. If you truncate the lower level to 0, you have to be careful about claiming 90% coverage. Plot the data points with a regression line, confidence band, and prediction band. bac %&gt;% gf_point(bac~beers) %&gt;% gf_labs(x=&quot;Number of cans of beers&quot;,y=&quot;BAC&quot;) %&gt;% gf_lm(stat=&quot;lm&quot;,interval=&quot;confidence&quot;) %&gt;% gf_lm(stat=&quot;lm&quot;,interval=&quot;prediction&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_refine(scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9))) 25.2.3 Problem 3 Suppose I build a regression fitting a response variable to one predictor variable. I build a 95% confidence interval on \\(\\beta_1\\) and find that it contains 0, meaning that a slope of 0 is feasible. Does this mean that the response and the predictor are independent? No. It merely means that my best guess is that the two variables are linearly uncorrelated. They could be related another way (quadratically, for example), but still result in an estimated slope close to 0. "],["LRDIAG.html", "Chapter 26 Regression Diagnostics 26.1 Objectives 26.2 Homework", " Chapter 26 Regression Diagnostics 26.1 Objectives Obtain and interpret \\(R\\)-squared and the \\(F\\)-statistic. Use R to evaluate the assumptions of a linear model. Identify and explain outliers and leverage points. 26.2 Homework 26.2.1 Problem 1 Identify relationships For each of the six plots, identify the strength of the relationship (e.g. weak, moderate, or strong) in the data and whether fitting a linear model would be reasonable. When we ask about the strength of the relationship, we mean: is there a relationship between \\(x\\) and \\(y\\) and does that relationship explain most of the variance? Figure 26.1: Homework problem 1. Strong relationship, but a straight line would not fit the data. Strong relationship, and a linear fit would be reasonable. Weak relationship, and trying a linear fit would be reasonable. Moderate relationship, but a straight line would not fit the data. Strong relationship, and a linear fit would be reasonable. Weak relationship because a change in \\(x\\) does not cause a change in \\(y\\) even though the points would cluster around a horizontal line. Trying a linear fit would be reasonable. 26.2.2 Problem 2 Beer and blood alcohol content We will use the blood alcohol content data again. As a reminder this is the description of the data: Many people believe that gender, weight, drinking habits, and many other factors are much more important in predicting blood alcohol content (BAC) than simply considering the number of drinks a person consumed. Here we examine data from sixteen student volunteers at Ohio State University who each drank a randomly assigned number of cans of beer. These students were evenly divided between men and women, and they differed in weight and drinking habits. Thirty minutes later, a police officer measured their blood alcohol content (BAC) in grams of alcohol per deciliter of blood. The data is in the bac.csv file under the data folder. Obtain and interpret \\(R\\)-squared for this model. bac&lt;-read_csv(&quot;data/bac.csv&quot;) bac %&gt;% gf_point(bac~beers) %&gt;% gf_labs(x=&quot;Number of cans of beer&quot;,y=&quot;BAC&quot;) %&gt;% gf_theme(theme_classic()) %&gt;% gf_refine(scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9))) bac_mod &lt;- lm(bac~beers,data=bac) summary(bac_mod) ## ## Call: ## lm(formula = bac ~ beers, data = bac) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.027118 -0.017350 0.001773 0.008623 0.041027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.012701 0.012638 -1.005 0.332 ## beers 0.017964 0.002402 7.480 2.97e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.02044 on 14 degrees of freedom ## Multiple R-squared: 0.7998, Adjusted R-squared: 0.7855 ## F-statistic: 55.94 on 1 and 14 DF, p-value: 2.969e-06 The \\(R\\)-squared is 0.7998, this means that almost 80% of the variance in blood alcohol content is explained by the number of beers consumed. This is not surprising. The remaining variance may be due to measurement errors, differences in the students, and environmental impacts. Evaluate the assumptions of this model. Do we have anything to be concerned about? plot(bac_mod,1) The fit is pretty good, there is one data point that is an outlier, the student that drank 9 beers. plot(bac_mod,2) bac ## # A tibble: 16 × 3 ## student beers bac ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 5 0.1 ## 2 2 2 0.03 ## 3 3 9 0.19 ## 4 4 8 0.12 ## 5 5 3 0.04 ## 6 6 7 0.095 ## 7 7 3 0.07 ## 8 8 5 0.06 ## 9 9 3 0.02 ## 10 10 5 0.05 ## 11 11 4 0.07 ## 12 12 6 0.1 ## 13 13 5 0.085 ## 14 14 7 0.09 ## 15 15 1 0.01 ## 16 16 4 0.05 The 3rd, 7th, and 10th data points stand out. This data set is too small to make any decisions about normality. It is suspect though. plot(bac_mod,3) We have higher variance at the higher number of beers. Again this appears to be caused by the small number of data points and the influence of observation number 3. plot(bac_mod,5) The points we’re looking for(or not looking for) are values in the upper right or lower right corners, which are outside the red dashed Cook’s distance line. These are points that would be influential in the model and removing them would likely noticeably alter the regression results. Now we see that observation 3 has extreme leverage on the model. Removing it would potentially drastically alter the model. To learn more about measures of influence, see https://cran.r-project.org/web/packages/olsrr/vignettes/influence_measures.html 26.2.3 Problem 3 Outliers Identify the outliers in the scatterplots shown below and determine what type of outliers they are. Explain your reasoning. The labels are off so treat the bottom row as (d), (e), and (f). Figure 26.2: Homework problem 3. The outlier is located in the bottom right corner of the plot. If we were to exclude this point from the analysis, the slope of the regression line would be notably affected, which means this is a high-leverage and influential point. The outlier is located in the bottom right corner of the plot. It is a point with high leverage since it is horizontally away from the center of the data, but it is not influential since the regression line would change little if it was removed. It is also an outlier in the response but would have a small residual. The outlier is located in the center and top of the plot. Though the point is unlike the rest of the data, it is not a high-leverage point since it is not far on the x-axis from the center of the data. This also means it is not an influential point since its presence has little influence on the slope of the regression line. The outlier is in the upper-left corner. Since it is horizontally far from the center of the data, it is an high leverage point. Additionally, since the fit of the regression line is greatly influenced by this point, it is a influential point. The outlier is located in the lower-left corner. It is horizontally far from the rest of the data, so it is a high-leverage point. The regression line also would fall relatively far from this point if the fit excluded this point, meaning the outlier is influential. The outlier is in the upper-middle of the plot. Since it is near the horizontal center of the data, it is not a high-leverage point. This means it also will have little or no influence on the slope of the regression line. "],["LRSIM.html", "Chapter 27 Simulation Based Linear Regression 27.1 Objectives 27.2 Homework", " Chapter 27 Simulation Based Linear Regression 27.1 Objectives Using the bootstrap, generate confidence and estimates of standard error for parameter estimates from a linear regression model. Generate and interpret bootstrap confidence intervals for predicted values. Generate bootstrap samples from sampling rows of the data or sampling residuals. Explain why you might prefer one over the other. Interpret regression coefficients for a linear model with a categorical explanatory variable. 27.2 Homework We will use the loans data set again to create linear models. Remember this data set represents thousands of loans made through the Lending Club platform, which is a platform that allows individuals to lend to other individuals. 27.2.1 Problem 1 Loans In this exercise we will examine the relationship between interest rate and loan amount. Read in the data from loans.csv in the data folder. loans &lt;- read_csv(&quot;data/loans.csv&quot;) Create a subset of data with 200 with the following three variables interest_rate, loan_amount, and term. Change term into a factor and use a stratified sample to keep the proportion of loan term roughly the same as the original data. tally(~term,data=loans,format=&quot;percent&quot;) ## term ## 36 60 ## 69.7 30.3 set.seed(2111) loans200 &lt;- loans %&gt;% select(interest_rate,loan_amount,term) %&gt;% mutate(term=factor(term)) %&gt;% group_by(term) %&gt;% slice_sample(prop=0.02) %&gt;% ungroup() tally(~term,data=loans200,format=&quot;percent&quot;) ## term ## 36 60 ## 69.84925 30.15075 str(loans200) ## tibble [199 × 3] (S3: tbl_df/tbl/data.frame) ## $ interest_rate: num [1:199] 13.59 9.92 17.47 10.9 7.34 ... ## $ loan_amount : num [1:199] 13000 10000 10000 8400 4800 10000 6000 6300 10000 32000 ... ## $ term : Factor w/ 2 levels &quot;36&quot;,&quot;60&quot;: 1 1 1 1 1 1 1 1 1 1 ... Plot interest_rate versus loan_amount. We think interest_rate should be the response. It seems natural that you would want to predict interest rate from loan amount. ggplot(loans200,aes(x=loan_amount,y=interest_rate)) + geom_point() + labs(title=&quot;Lending Club&quot;,subtitle=&quot;Loan amount versus Interest rate&quot;, x=&quot;Loan Amount&quot;,y=&quot;Interest rate (percent)&quot;) + theme_bw() Fit a linear model to the data by regressing interest_rate on loan_amount. Is there a significant relationship between interest_rate and loan_amount? int_rate_mod &lt;- lm(interest_rate~loan_amount,data=loans200) summary(int_rate_mod) ## ## Call: ## lm(formula = interest_rate ~ loan_amount, data = loans200) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.1611 -4.6842 -0.9666 3.0685 18.6280 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.165e+01 7.337e-01 15.874 &lt;2e-16 *** ## loan_amount 2.748e-05 3.722e-05 0.738 0.461 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.423 on 197 degrees of freedom ## Multiple R-squared: 0.002759, Adjusted R-squared: -0.002303 ## F-statistic: 0.545 on 1 and 197 DF, p-value: 0.4613 To test if there a significant relationship between interest_rate and loan_amount, we test if \\(\\beta_1 = 0\\). The p-value for this is 0.4613, so we fail to reject that there is no relationship between interest_rate and loan_amount. Using the \\(t\\) distribution: Find a 95% confidence interval for the slope. Find and interpret a 90% confidence interval for a loan amount of $20000. confint(int_rate_mod) ## 2.5 % 97.5 % ## (Intercept) 1.019986e+01 1.309374e+01 ## loan_amount -4.592357e-05 1.008748e-04 We are 95% confident the true slope is between -4.592357e-05 and 1.008748e-04. predict(int_rate_mod,newdata = data.frame(loan_amount=20000), interval = &quot;confidence&quot;,level=0.90) ## fit lwr upr ## 1 12.19631 11.53105 12.86157 We are 90% confident that the average interest rate for a loan of $20000 is between 11.5% and 12.9%. Repeat part e using a bootstrap. set.seed(3011) results &lt;- do(1000)*lm(interest_rate ~ loan_amount,data=resample(loans200)) head(results) ## Intercept loan_amount sigma r.squared F numdf dendf .row ## 1 10.26651 6.837311e-05 4.893962 0.0184148594 3.69578466 1 197 1 ## 2 11.63858 2.341492e-05 5.147715 0.0021348812 0.42147139 1 197 1 ## 3 12.52116 6.096805e-06 5.138114 0.0001537970 0.03030266 1 197 1 ## 4 11.93790 -6.489195e-06 5.284324 0.0001655171 0.03261227 1 197 1 ## 5 11.38254 9.000912e-05 5.472851 0.0281911082 5.71475355 1 197 1 ## 6 12.35924 -2.568690e-05 5.520788 0.0025489432 0.50342501 1 197 1 ## .index ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 cdata(~loan_amount,data=results) ## lower upper central.p ## 2.5% -4.277717e-05 9.863203e-05 0.95 Or using the infer package: results2 &lt;- loans200 %&gt;% specify(interest_rate~loan_amount) %&gt;% generate(reps=1000,type=&quot;bootstrap&quot;) %&gt;% calculate(stat=&quot;slope&quot;) head(results2) ## Response: interest_rate (numeric) ## Explanatory: loan_amount (numeric) ## # A tibble: 6 × 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 -0.0000114 ## 2 2 0.0000198 ## 3 3 0.0000207 ## 4 4 0.00000481 ## 5 5 0.0000549 ## 6 6 0.0000531 get_confidence_interval(results2) ## Using `level = 0.95` to compute confidence interval. ## # A tibble: 1 × 2 ## lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.0000361 0.0000947 Now the confidence interval for average interest rate at a loan amount of 20000: results %&gt;% mutate(pred=Intercept+loan_amount*20000) %&gt;% cdata(~pred,data=.) ## lower upper central.p ## 2.5% 11.4061 13.01508 0.95 Again, close to what we had but slightly different. Maybe some of the assumptions such as normality are not appropriate. Check the assumptions of linear regression. plot(int_rate_mod) There appears to be a lack of normality as the residuals are skewed to the right, large positive residuals. The bootstrap would probably be more appropriate for this problem. 27.2.2 Problem 2 Loans II Using the loans data set of 200 observations from the previous exercise, use the variable term to determine if there is a difference in interest rates for the two different loan lengths. Build a set of side-by-side boxplots that summarize interest rate by term. Describe the relationship you see. Note: You will have to convert the term variable to a factor prior to continuing. loans200 %&gt;% gf_boxplot(interest_rate~term) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Lending Club&quot;,x=&quot;Length of Loan&quot;,y=&quot;Interest Rate&quot;) It looks like there is a difference in interest rate based on the length of the loan. It also appears both are skewed to the right, positive skew. Let’s plot the density and see what we find. loans200 %&gt;% gf_dens(~interest_rate,group=~term,color=~term) %&gt;% gf_theme(theme_classic()) %&gt;% gf_labs(title=&quot;Lending Club&quot;,x=&quot;Interest Rate&quot;,y=&quot;Density&quot;) Just as we thought. Build a linear model fitting interest rate against term. Does there appear to be a significant difference in mean interest rates by term? int_rate_mod2 &lt;- lm(interest_rate~term,data=loans200) summary(int_rate_mod2) ## ## Call: ## lm(formula = interest_rate ~ term, data = loans200) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.643 -3.993 -1.263 3.132 15.597 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.7031 0.4230 25.304 &lt; 2e-16 *** ## term60 4.6601 0.7703 6.049 7.19e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.987 on 197 degrees of freedom ## Multiple R-squared: 0.1567, Adjusted R-squared: 0.1524 ## F-statistic: 36.6 on 1 and 197 DF, p-value: 7.189e-09 There is a significant difference between the average interest rate based on the length of the loan. Write out the estimated linear model. In words, interpret the coefficient estimate. The intercept \\(\\beta_\\text{Intercept} = \\mu_\\text{term36}\\) is the average interest rate for a 36 month loan. And \\(\\beta_\\text{term60} = \\mu_\\text{term60} - \\mu_\\text{term36}\\) is the difference in average interest rates between loan length. In this case, a 60 month loan is 4.66 percentage points higher on average than a 36 month loan. Construct a bootstrap confidence interval on the coefficient. set.seed(331) results &lt;- do(1000)*lm(interest_rate ~ term,data=resample(loans200)) head(results) ## Intercept term60 sigma r.squared F numdf dendf .row .index ## 1 10.11890 4.903166 4.534598 0.20352136 50.33871 1 197 1 1 ## 2 10.91695 5.275981 5.220399 0.17564679 41.97523 1 197 1 2 ## 3 11.07592 4.064085 5.226607 0.11097921 24.59212 1 197 1 3 ## 4 11.14948 3.675445 5.250290 0.09818999 21.44956 1 197 1 4 ## 5 10.45978 4.941671 4.960234 0.17698753 42.36454 1 197 1 5 ## 6 11.02597 4.232862 4.891180 0.13743012 31.38729 1 197 1 6 cdata(~term60,data=results) ## lower upper central.p ## 2.5% 3.029483 6.242261 0.95 We are 95% confident the difference in average interest rates for loans of 60 month and 36 month is between 3.03% and 6.24%. Let’s check using the assumption of normally distributed errors. confint(int_rate_mod2) ## 2.5 % 97.5 % ## (Intercept) 9.868936 11.537251 ## term60 3.140928 6.179218 Close, but slightly narrower. Check model assumptions. plot(int_rate_mod2) Because of the discrete nature of the predictor, only the first two plots are of interest. The assumption of constant variable does seem reasonable but the assumption of normally distributed errors is not. We have a positive skewness. "],["LRMULTI.html", "Chapter 28 Multiple Linear Regression 28.1 Objectives 28.2 Homework", " Chapter 28 Multiple Linear Regression 28.1 Objectives Create and interpret a model with multiple predictors and check assumptions. Generate and interpret confidence intervals for estimates. Explain adjusted \\(R^2\\) and multi-collinearity. Interpret regression coefficients for a linear model with multiple predictors. Build and interpret models with higher order terms. 28.2 Homework 28.2.1 Problem 1 The mtcars dataset contains average mileage (mpg) and other information about specific makes and models of cars. (This dataset is built-in to R; for more information about this dataset, reference the documentation with ?mtcars). Build and interpret the coefficients of a model fitting mpg against displacement (disp), horsepower (hp), rear axle ratio (drat), and weight in 1000 lbs (wt). cars_mod&lt;-lm(mpg~disp+hp+drat+wt,data=mtcars) summary(cars_mod) ## ## Call: ## lm(formula = mpg ~ disp + hp + drat + wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.5077 -1.9052 -0.5057 0.9821 5.6883 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.148738 6.293588 4.631 8.2e-05 *** ## disp 0.003815 0.010805 0.353 0.72675 ## hp -0.034784 0.011597 -2.999 0.00576 ** ## drat 1.768049 1.319779 1.340 0.19153 ## wt -3.479668 1.078371 -3.227 0.00327 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.602 on 27 degrees of freedom ## Multiple R-squared: 0.8376, Adjusted R-squared: 0.8136 ## F-statistic: 34.82 on 4 and 27 DF, p-value: 2.704e-10 \\[ \\mbox{E}(\\text{mpg})=29.15+0.004*\\text{disp}-0.035*\\text{hp}+1.768*\\text{drat}-3.480*\\text{wt} \\] Each coefficient represents the expected increase in mpg for a unit increase in the respective variable, leaving all other variables constant. Given your model, what is the expected, average, mpg for a vehicle with a displacement of 170, a horsepower of 100, a drat of 3.80 and a wt of 2,900 lbs. Construct a 95% confidence interval and prediction interval for that expected mpg. predict(cars_mod,newdata=data.frame(disp=170,hp=100,drat=3.8,wt=2.9),interval=&quot;confidence&quot;) ## fit lwr upr ## 1 22.94652 21.76569 24.12735 predict(cars_mod,newdata=data.frame(disp=170,hp=100,drat=3.8,wt=2.9),interval=&quot;prediction&quot;) ## fit lwr upr ## 1 22.94652 17.47811 28.41494 Repeat part (b) with a bootstrap for the confidence interval. set.seed(732) results &lt;- do(1000)*lm(mpg~disp+hp+drat+wt,data=resample(mtcars)) head(results) ## Intercept disp hp drat wt sigma r.squared ## 1 20.28185 -0.0017783926 -0.02620557 2.8897199 -2.016023 2.300320 0.8210210 ## 2 33.66818 -0.0128734640 -0.01960700 0.3309526 -2.862143 2.102123 0.8760470 ## 3 25.54583 -0.0001983653 -0.03743247 2.1905290 -2.392723 2.814197 0.7947977 ## 4 33.22436 0.0112562822 -0.04151242 1.4796584 -4.557451 2.699223 0.8523512 ## 5 25.96975 -0.0002882344 -0.03093247 2.3689688 -2.861715 2.670069 0.8643060 ## 6 34.17742 0.0054509451 -0.04085537 0.9019272 -3.975107 2.562486 0.8377108 ## F numdf dendf .row .index ## 1 30.96393 4 27 1 1 ## 2 47.70611 4 27 1 2 ## 3 26.14436 4 27 1 3 ## 4 38.96659 4 27 1 4 ## 5 42.99429 4 27 1 5 ## 6 34.84241 4 27 1 6 results %&gt;% mutate(pred=Intercept+disp*170+hp*100+drat*3.8+wt*2.9) %&gt;% cdata(~pred,data=.) ## lower upper central.p ## 2.5% 21.76703 24.15803 0.95 28.2.2 Problem 2 Is that the best model for predicting mpg? Try a variety of different models. You could explore higher order terms or even interactions. One place to start is by using the pairs() function on mtcars to plot a large pairwise scatterplot. How high could you get adjusted \\(R\\)-squared? Keep in mind that is only one measure of fit. Answers will vary, but we tried this and got 0.8694. summary(lm(mpg~disp+I(disp^2)+hp+I(hp^2)+wt,data=mtcars)) ## ## Call: ## lm(formula = mpg ~ disp + I(disp^2) + hp + I(hp^2) + wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.1591 -1.4907 -0.3903 1.5851 3.7795 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.440e+01 2.639e+00 16.823 1.71e-15 *** ## disp -4.532e-02 2.131e-02 -2.127 0.043100 * ## I(disp^2) 8.844e-05 3.315e-05 2.668 0.012967 * ## hp -8.652e-02 3.813e-02 -2.269 0.031813 * ## I(hp^2) 1.585e-04 8.932e-05 1.775 0.087666 . ## wt -3.517e+00 8.874e-01 -3.963 0.000515 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.178 on 26 degrees of freedom ## Multiple R-squared: 0.8904, Adjusted R-squared: 0.8694 ## F-statistic: 42.26 on 5 and 26 DF, p-value: 1.129e-11 "],["LOGREG.html", "Chapter 29 Logistic Regression 29.1 Objectives 29.2 Homework Problems", " Chapter 29 Logistic Regression 29.1 Objectives Using R, conduct logistic regression and interpret the output and perform model selection. Write the logistic regression model and predict outputs for given inputs. Find confidence intervals for parameter estimates and predictions. 29.2 Homework Problems 29.2.1 Problem 1 Possum classification Let’s investigate the possum data set again. This time we want to model a binary outcome variable. As a reminder, the common brushtail possum of the Australia region is a bit cuter than its distant cousin, the American opossum. We consider 104 brushtail possums from two regions in Australia, where the possums may be considered a random sample from the population. The first region is Victoria, which is in the eastern half of Australia and traverses the southern coast. The second region consists of New South Wales and Queensland, which make up eastern and northeastern Australia. We use logistic regression to differentiate between possums in these two regions. The outcome variable, called pop, takes value Vic when a possum is from Victoria and other when it is from New South Wales or Queensland. We consider five predictors: sex, head_l, skull_w, total_l, and tail_l. Explore the data by making histograms of the quantitative variables, and bar charts of the discrete variables. Are there any outliers that are likely to have a very large influence on the logistic regression model? possum &lt;- read_csv(&quot;data/possum.csv&quot;) %&gt;% select(pop,sex,head_l,skull_w,total_l,tail_l) %&gt;% mutate(pop=factor(pop),sex=factor(sex)) inspect(possum) ## ## categorical variables: ## name class levels n missing distribution ## 1 pop factor 2 104 0 other (55.8%), Vic (44.2%) ## 2 sex factor 2 104 0 m (58.7%), f (41.3%) ## ## quantitative variables: ## name class min Q1 median Q3 max mean sd n ## ...1 head_l numeric 82.5 90.675 92.80 94.725 103.1 92.60288 3.573349 104 ## ...2 skull_w numeric 50.0 54.975 56.35 58.100 68.6 56.88365 3.113426 104 ## ...3 total_l numeric 75.0 84.000 88.00 90.000 96.5 87.08846 4.310549 104 ## ...4 tail_l numeric 32.0 35.875 37.00 38.000 43.0 37.00962 1.959518 104 ## missing ## ...1 0 ## ...2 0 ## ...3 0 ## ...4 0 possum %&gt;% gf_props(~pop,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;Population&quot;) possum %&gt;% gf_props(~sex,fill=&quot;cyan&quot;,color=&quot;black&quot;) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(x=&quot;Gender&quot;) possum %&gt;% gf_boxplot(~head_l) %&gt;% gf_theme(theme_bw()) possum %&gt;% gf_boxplot(~skull_w) %&gt;% gf_theme(theme_bw()) possum %&gt;% gf_boxplot(~total_l) %&gt;% gf_theme(theme_bw()) possum %&gt;% gf_boxplot(~tail_l) %&gt;% gf_theme(theme_bw()) There are some potential outliers for skull width but otherwise not much concern. pairs(possum[,3:6],lower.panel = panel.smooth) We can see that head_l is correlated with the other three variables. This will cause some multicollinearity problems. Build a logistic regression model with all the variable. Report a summary of the model. possum_mod &lt;- glm(pop==&quot;Vic&quot;~.,data=possum,family=&quot;binomial&quot;) summary(possum_mod) ## ## Call: ## glm(formula = pop == &quot;Vic&quot; ~ ., family = &quot;binomial&quot;, data = possum) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6430 -0.5514 -0.1182 0.3760 2.8501 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 39.2349 11.5368 3.401 0.000672 *** ## sexm -1.2376 0.6662 -1.858 0.063195 . ## head_l -0.1601 0.1386 -1.155 0.248002 ## skull_w -0.2012 0.1327 -1.517 0.129380 ## total_l 0.6488 0.1531 4.236 2.27e-05 *** ## tail_l -1.8708 0.3741 -5.001 5.71e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 142.787 on 103 degrees of freedom ## Residual deviance: 72.155 on 98 degrees of freedom ## AIC: 84.155 ## ## Number of Fisher Scoring iterations: 6 confint(possum_mod) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 18.8530781 64.66444839 ## sexm -2.6227018 0.02472167 ## head_l -0.4428559 0.10865739 ## skull_w -0.4933140 0.04479826 ## total_l 0.3768179 0.98455786 ## tail_l -2.7170468 -1.23231969 Using the p-values decide if you want to remove a variable(S) and if so build that model. Let’s remove head_l first. possum_mod_red &lt;- glm(pop==&quot;Vic&quot;~sex+skull_w+total_l+tail_l,data=possum,family=&quot;binomial&quot;) summary(possum_mod_red) ## ## Call: ## glm(formula = pop == &quot;Vic&quot; ~ sex + skull_w + total_l + tail_l, ## family = &quot;binomial&quot;, data = possum) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8102 -0.5683 -0.1222 0.4153 2.7599 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 33.5095 9.9053 3.383 0.000717 *** ## sexm -1.4207 0.6457 -2.200 0.027790 * ## skull_w -0.2787 0.1226 -2.273 0.023053 * ## total_l 0.5687 0.1322 4.302 1.69e-05 *** ## tail_l -1.8057 0.3599 -5.016 5.26e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 142.787 on 103 degrees of freedom ## Residual deviance: 73.516 on 99 degrees of freedom ## AIC: 83.516 ## ## Number of Fisher Scoring iterations: 6 Since head_l was correlated with the other variables, removing it has increased the precision, decreased the standard error, of the other predictors. There p-values are all now less than 0.05. For any variable you decide to remove, build a 95% confidence interval for the parameter. confint(possum_mod) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 18.8530781 64.66444839 ## sexm -2.6227018 0.02472167 ## head_l -0.4428559 0.10865739 ## skull_w -0.4933140 0.04479826 ## total_l 0.3768179 0.98455786 ## tail_l -2.7170468 -1.23231969 We are 95% confident that the true slope coefficient for head_l is between -0.44 and 0.108. The bootstrap is not working for this problem. It may be that we have convergence issues when we resample the data. This is a reminder that we need to be careful and not just run methods without checking results. Here is the code: set.seed(952) results&lt;-do(1000)*glm(pop==&quot;Vic&quot;~.,data=resample(possum),family=&quot;binomial&quot;) head(results[,1:5]) ## Intercept sexm head_l skull_w total_l ## 1 -1184.61865 2.122389e+01 3.861560e+00 2.749263e+00 7.274005e+00 ## 2 6371.55494 -1.301513e+02 1.023732e+01 -2.738815e+01 -1.076913e+01 ## 3 -9612.61859 -2.392899e+03 -1.875252e+02 5.782026e+02 -2.820691e+02 ## 4 -25.18662 -1.852185e+01 2.097593e+01 -1.353619e+01 1.483815e+01 ## 5 -26.56607 2.167890e-13 -4.396307e-15 5.075768e-15 4.484497e-14 ## 6 -1025.00035 6.159665e+01 2.526181e+01 -2.032143e+01 1.438639e+01 results %&gt;% gf_histogram(~head_l,fill=&quot;cyan&quot;,color = &quot;black&quot;) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(title=&quot;Bootstrap sampling distribtuion&quot;, x=&quot;sex paramater estimate&quot;) cdata(~head_l,data=results) ## lower upper central.p ## 2.5% -187.5252 63.61867 0.95 This interval is too large. The resampling process has too much variation in it. This could be due to the small sample size and the multicollinearity. Explain why the remaining parameter estimates change between the two models. When coefficient estimates are sensitive to which variables are included in the model, this typically indicates that some variables are collinear. For example, a possum’s gender may be related to its head length, which would explain why the coefficient (and p-value) for sex male changed when we removed the head length variable. Likewise, a possum’s skull width is likely to be related to its head length, probably even much more closely related than the head length was to gender. Write out the form of the model. Also identify which of the following variables are positively associated (when controlling for other variables) with a possum being from Victoria: head_l, skull_w, total_l, and tail_l. We dropped head_l from the model. Here is the equation: \\[ \\log_{e}\\left( \\frac{p_i}{1-p_i} \\right) = 33.5 - 1.42 \\text{ sex} -0.28 \\text{ skull width} + 0.57 \\text{ total length} - 1.81 \\text{ tail length} \\] Only total_l is positively association with the probability of being from Victoria. Suppose we see a brushtail possum at a zoo in the US, and a sign says the possum had been captured in the wild in Australia, but it doesn’t say which part of Australia. However, the sign does indicate that the possum is male, its skull is about 63 mm wide, its tail is 37 cm long, and its total length is 83 cm. What is the reduced model’s computed probability that this possum is from Victoria? How confident are you in the model’s accuracy of this probability calculation? Let’s predict the outcome. We use response for the type to put the answer in the form of a probability. See the help menu on predict.glm for more information. predict(possum_mod_red,newdata = data.frame(sex=&quot;m&quot;,skull_w=63,tail_l=37,total_l=83), type=&quot;response&quot;,se.fit = TRUE) ## $fit ## 1 ## 0.006205055 ## ## $se.fit ## 1 ## 0.008011468 ## ## $residual.scale ## [1] 1 While the probability, 0.006, is very near zero, we have not run diagnostics on the model. We should also have a little skepticism that the model will hold for a possum found in a US zoo. However, it is encouraging that the possum was caught in the wild. As a rough sense of the accuracy, we will use the standard error. The errors are really binomial but we are trying to use a normal approximation. If you remember back to our block on probability, with such a low probability, this assumption of normality is suspect. However, we will use it to give us an upper bound. 0.0062+c(-1,1)*1.96*.008 ## [1] -0.00948 0.02188 So at most, the probability of the possum being from Victoria is 2%. 29.2.2 Problem 2 Medical school admission The file MedGPA.csv in the data folder has information on medical school admission status and GPA and standardized test scores gathered on 55 medical school applicants from a liberal arts college in the Midwest. The variables are: Accept Status: A=accepted to medical school or D=denied admission Acceptance: Indicator for Accept: 1=accepted or 0=denied Sex: F=female or M=male BCPM: Bio/Chem/Physics/Math grade point average GPA: College grade point average VR: Verbal reasoning (subscore) PS: Physical sciences (subscore) WS: Writing sample (subcore) BS: Biological sciences (subscore) MCAT: Score on the MCAT exam (sum of CR+PS+WS+BS) Apps: Number of medical schools applied to Build a logistic regression model to predict Acceptance from GPA and `Sex. MedGPA &lt;- read_csv(&quot;data/MedGPA.csv&quot;) glimpse(MedGPA) ## Rows: 55 ## Columns: 11 ## $ Accept &lt;chr&gt; &quot;D&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;D&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;,… ## $ Acceptance &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,… ## $ Sex &lt;chr&gt; &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;,… ## $ BCPM &lt;dbl&gt; 3.59, 3.75, 3.24, 3.74, 3.53, 3.59, 3.85, 3.26, 3.74, 3.86,… ## $ GPA &lt;dbl&gt; 3.62, 3.84, 3.23, 3.69, 3.38, 3.72, 3.89, 3.34, 3.71, 3.89,… ## $ VR &lt;dbl&gt; 11, 12, 9, 12, 9, 10, 11, 11, 8, 9, 11, 11, 8, 9, 11, 12, 8… ## $ PS &lt;dbl&gt; 9, 13, 10, 11, 11, 9, 12, 11, 10, 9, 9, 8, 10, 9, 8, 8, 8, … ## $ WS &lt;dbl&gt; 9, 8, 5, 7, 4, 7, 6, 8, 6, 6, 8, 4, 7, 4, 6, 8, 8, 9, 5, 8,… ## $ BS &lt;dbl&gt; 9, 12, 9, 10, 11, 10, 11, 9, 11, 10, 11, 8, 10, 10, 7, 10, … ## $ MCAT &lt;dbl&gt; 38, 45, 33, 40, 35, 36, 40, 39, 35, 34, 39, 31, 35, 32, 32,… ## $ Apps &lt;dbl&gt; 5, 3, 19, 5, 11, 5, 5, 7, 5, 11, 6, 9, 5, 8, 15, 6, 6, 1, 5… med_mod&lt;-glm(Accept==&quot;D&quot;~GPA+Sex,data=MedGPA,family=binomial) summary(med_mod) ## ## Call: ## glm(formula = Accept == &quot;D&quot; ~ GPA + Sex, family = binomial, data = MedGPA) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4623 -0.7194 -0.2978 0.9753 1.8171 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 21.0680 6.4025 3.291 0.001000 *** ## GPA -6.1324 1.8283 -3.354 0.000796 *** ## SexM 1.1697 0.7178 1.629 0.103210 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 75.791 on 54 degrees of freedom ## Residual deviance: 53.945 on 52 degrees of freedom ## AIC: 59.945 ## ## Number of Fisher Scoring iterations: 5 Generate a 95% confidence interval for the coefficient associated with GPA. confint(med_mod) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 10.0651173 35.579760 ## GPA -10.2977983 -3.007544 ## SexM -0.1720454 2.697436 Let’s try the bootstrap for this problem. set.seed(819) results_med &lt;- do(1000)*glm(Accept==&quot;D&quot;~GPA+Sex,data=resample(MedGPA),family=binomial) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred results_med %&gt;% gf_histogram(~GPA,fill=&quot;cyan&quot;,color = &quot;black&quot;) %&gt;% gf_theme(theme_bw()) %&gt;% gf_labs(title=&quot;Bootstrap sampling distribtuion&quot;, x=&quot;GPA paramater estimate&quot;) cdata(~GPA,data=results_med) ## lower upper central.p ## 2.5% -14.2047 -3.213036 0.95 This is not so bad. It appears that the distribution of GPA is skewed to the left. Fit a model with a polynomial of degree 2 in the GPA. Drop Sex from the model. Does a quadratic fit improve the model? med_mod2&lt;-glm(Accept==&quot;D&quot;~poly(GPA,2),data=MedGPA,family=binomial) summary(med_mod2) ## ## Call: ## glm(formula = Accept == &quot;D&quot; ~ poly(GPA, 2), family = binomial, ## data = MedGPA) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9553 -0.7830 -0.3207 0.8020 1.8363 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.3301 0.3582 -0.921 0.356822 ## poly(GPA, 2)1 -10.7293 3.0664 -3.499 0.000467 *** ## poly(GPA, 2)2 -3.4967 3.0987 -1.128 0.259133 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 75.791 on 54 degrees of freedom ## Residual deviance: 55.800 on 52 degrees of freedom ## AIC: 61.8 ## ## Number of Fisher Scoring iterations: 4 The quadratic term did not improve the model. Fit a model with just GPA and interpret the coefficient. tidy(glm(Accept==&quot;D&quot;~GPA,data=MedGPA,family=binomial)) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 19.2 5.63 3.41 0.000644 ## 2 GPA -5.45 1.58 -3.45 0.000553 exp(-5.454166) ## [1] 0.004278444 An increase of 1 in a student’s GPA decreases the odds of being denied acceptance by 0.00428. Remember that this is not a probability. As a reminder an odds of \\(\\frac{1}{2}\\) means the probability of success is \\(\\frac{1}{3}\\). An odds of 1 means a probability of success of \\(\\frac{1}{2}\\). Assume the initial odds are 1 if the odds are now 0.00428 smaller, then the probability of success is \\(\\frac{428}{10428}\\) or 0.04. The probability decreased by an order of magnitude. Try to add different predictors to come up with your best model. Do not use Acceptance and MCAT in the model. tidy(glm(Accept==&quot;D&quot;~.-Acceptance-MCAT,data=MedGPA,family=binomial)) %&gt;% mutate(p.adj=p.adjust(p.value)) %&gt;% select(term,p.value,p.adj) ## # A tibble: 9 × 3 ## term p.value p.adj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.00279 0.0251 ## 2 SexM 0.110 0.551 ## 3 BCPM 0.376 1 ## 4 GPA 0.150 0.600 ## 5 VR 0.799 1 ## 6 PS 0.0305 0.213 ## 7 WS 0.0491 0.295 ## 8 BS 0.00490 0.0392 ## 9 Apps 0.728 1 Let’s take out VR. tidy(glm(Accept==&quot;D&quot;~.-Acceptance-MCAT-VR,data=MedGPA,family=binomial)) %&gt;% mutate(p.adj=p.adjust(p.value)) %&gt;% select(term,p.value,p.adj) ## # A tibble: 8 × 3 ## term p.value p.adj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.00239 0.0191 ## 2 SexM 0.0681 0.272 ## 3 BCPM 0.362 0.723 ## 4 GPA 0.143 0.430 ## 5 PS 0.0280 0.168 ## 6 WS 0.0452 0.226 ## 7 BS 0.00476 0.0333 ## 8 Apps 0.742 0.742 Now let’s take out Apps. tidy(glm(Accept==&quot;D&quot;~.-Acceptance-MCAT-VR-Apps,data=MedGPA,family=binomial)) %&gt;% mutate(p.adj=p.adjust(p.value)) %&gt;% select(term,p.value,p.adj) ## # A tibble: 7 × 3 ## term p.value p.adj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.00147 0.0103 ## 2 SexM 0.0272 0.133 ## 3 BCPM 0.379 0.379 ## 4 GPA 0.120 0.240 ## 5 PS 0.0266 0.133 ## 6 WS 0.0385 0.133 ## 7 BS 0.00477 0.0286 Next, let’s remove BCPM. tidy(glm(Accept==&quot;D&quot;~.-Acceptance-MCAT-VR-Apps-BCPM, data=MedGPA,family=binomial)) %&gt;% mutate(p.adj=p.adjust(p.value)) %&gt;% select(term,p.value,p.adj) ## # A tibble: 6 × 3 ## term p.value p.adj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.00123 0.00739 ## 2 SexM 0.0142 0.0567 ## 3 GPA 0.0315 0.0901 ## 4 PS 0.0300 0.0901 ## 5 WS 0.0401 0.0901 ## 6 BS 0.00537 0.0269 Now WS. tidy(glm(Accept==&quot;D&quot;~.-Acceptance-MCAT-VR-Apps-BCPM-WS, data=MedGPA,family=binomial)) %&gt;% mutate(p.adj=p.adjust(p.value)) %&gt;% select(term,p.value,p.adj) ## # A tibble: 5 × 3 ## term p.value p.adj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.000584 0.00292 ## 2 SexM 0.0168 0.0504 ## 3 GPA 0.0366 0.0732 ## 4 PS 0.0634 0.0732 ## 5 BS 0.0100 0.0401 Maybe PS. tidy(glm(Accept==&quot;D&quot;~.-Acceptance-MCAT-VR-Apps-BCPM-WS-PS, data=MedGPA,family=binomial)) %&gt;% mutate(p.adj=p.adjust(p.value)) %&gt;% select(term,p.value,p.adj) ## # A tibble: 4 × 3 ## term p.value p.adj ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.000574 0.00230 ## 2 SexM 0.0306 0.0611 ## 3 GPA 0.0325 0.0611 ## 4 BS 0.00448 0.0134 I will stop there. There has to be a better way. Math 378 will explore how to select predictors and improve a model. Generate a confusion matrix for the best model you have developed. med_mod2&lt;-glm(Accept==&quot;D&quot;~Sex+GPA+BS, data=MedGPA,family=binomial) summary(med_mod2) ## ## Call: ## glm(formula = Accept == &quot;D&quot; ~ Sex + GPA + BS, family = binomial, ## data = MedGPA) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8388 -0.4707 -0.1347 0.5535 2.6411 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 27.4608 7.9721 3.445 0.000572 *** ## SexM 1.9885 0.9193 2.163 0.030543 * ## GPA -4.2900 2.0062 -2.138 0.032484 * ## BS -1.3752 0.4838 -2.843 0.004471 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 75.791 on 54 degrees of freedom ## Residual deviance: 40.835 on 51 degrees of freedom ## AIC: 48.835 ## ## Number of Fisher Scoring iterations: 6 augment(med_mod2,type.predict = &quot;response&quot;) %&gt;% rename(actual=starts_with(&#39;Accept ==&#39;)) %&gt;% transmute(result=as.integer(.fitted&gt;0.5), actual=as.integer(actual)) %&gt;% table() ## actual ## result 0 1 ## 0 26 4 ## 1 4 21 This model has an accuracy of \\(\\frac{47}{55}\\) or 85.5%. Find a 95% confidence interval for the probability a female student with a 3.5 GPA, a BCPM of 3.8, a verbal reasoning score of 10, a physical sciences score of 9, a writing sample score of 8, a biological score of 10, a MCAT score of 40, and who applied to 5 medical schools. predict(med_mod2,newdata = data.frame(Sex=&quot;F&quot;, GPA=3.5, BS=10), type=&quot;response&quot;,se.fit = TRUE) ## $fit ## 1 ## 0.2130332 ## ## $se.fit ## 1 ## 0.1122277 ## ## $residual.scale ## [1] 1 0.2130332+c(-1,1)*1.96*.1122277 ## [1] -0.006933092 0.432999492 For a female students with a GPA of 3.5 and a biological score of 10, we are 95% confident that the probability of being denied acceptance to medical school is between 0 and .433. Let’s try a bootstrap. set.seed(729) results &lt;- do(1000)*glm(Accept==&quot;D&quot;~Sex+GPA+BS, data=resample(MedGPA), family=binomial) head(results) ## Intercept SexM GPA BS .row .index ## 1 204.23984 20.701630 -4.547781 -20.801516 1 1 ## 2 16.78193 1.373795 -2.508340 -0.908112 1 2 ## 3 36.09536 4.170571 -7.479463 -1.219088 1 3 ## 4 1104.42269 105.236280 -90.461857 -86.132398 1 4 ## 5 25.37989 1.139262 -3.729948 -1.331750 1 5 ## 6 26.58307 3.197072 -3.426165 -1.772581 1 6 results_pred &lt;- results %&gt;% mutate(pred=1/(1+exp(-1*(Intercept+3.5*GPA+10*BS)))) cdata(~pred,data=results_pred) ## lower upper central.p ## 2.5% 7.77316e-10 0.5032905 0.95 This is close to what we found and does not make the assumption the probability of success is normally distributed. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
