[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Contained volume solutions homework problems Probability Statistics Scientists Engineers book.","code":""},{"path":"index.html","id":"book-structure-and-how-to-use-it","chapter":"Preface","heading":"0.1 Book Structure and How to Use It","text":"solution manual setup match structure accompanying book.learning outcomes course use computational mathematical statistical/probabilistic concepts :Developing probabilistic modelsDeveloping statistical models inference descriptionAdvancing practical theoretical analytic experience skills","code":""},{"path":"index.html","id":"packages","chapter":"Preface","heading":"0.2 Packages","text":"notes make use following packages R knitr (Xie 2021), rmarkdown (Allaire et al. 2021), mosaic (Pruim, Kaplan, Horton 2021), mosaicCalc (Kaplan, Pruim, Horton 2020), tidyverse (Wickham 2021), ISLR (James et al. 2021), vcd (Meyer, Zeileis, Hornik 2021), ggplot2 (Wickham et al. 2021), MASS (Ripley 2021), openintro (Çetinkaya-Rundel et al. 2021), broom (Robinson, Hayes, Couch 2021), infer (Bray et al. 2021), kableExtra (Zhu 2021), DT (Xie, Cheng, Tan 2021).book licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":""},{"path":"index.html","id":"file-creation-information","chapter":"Preface","heading":"0.3 File Creation Information","text":"File creation date: 2022-02-28R version 4.1.2 (2021-11-01)","code":""},{"path":"CS1.html","id":"CS1","chapter":"1 Case Study","heading":"1 Case Study","text":"","code":""},{"path":"CS1.html","id":"objectives","chapter":"1 Case Study","heading":"1.1 Objectives","text":"Use R basic analysis visualization.Compile report using knitr.","code":""},{"path":"CS1.html","id":"homework","chapter":"1 Case Study","heading":"1.2 Homework","text":"Load tidyverse,mosaic, knitr packages.","code":"\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(knitr)"},{"path":"CS1.html","id":"problem-1","chapter":"1 Case Study","heading":"1.2.1 Problem 1","text":"Stent study continued Complete similar analysis stent data time one year data. particularRead data working directory.Complete similar steps class notes.\n. Use inspect data.\nii. Create table outcome365 group. Comment results.\niii. Create barchart data.Using inspectThe table:Patients treatment group higher proportion strokes control group one year. treatment appear help rate strokes fact may hurt .Barchart:","code":"\nstent_study <-read_csv('data/stent_study.csv')\ninspect(stent_study)## \n## categorical variables:  \n##         name     class levels   n missing\n## 1      group character      2 451       0\n## 2  outcome30 character      2 451       0\n## 3 outcome365 character      2 451       0\n##                                    distribution\n## 1 control (50.3%), trmt (49.7%)                \n## 2 no_event (89.8%), stroke (10.2%)             \n## 3 no_event (83.8%), stroke (16.2%)\ntally(outcome365~group,data=stent_study,format=\"proportion\",margins = TRUE)##           group\n## outcome365   control      trmt\n##   no_event 0.8766520 0.7991071\n##   stroke   0.1233480 0.2008929\n##   Total    1.0000000 1.0000000\nstent_study %>%\n  gf_props(~group,fill=~outcome365,position='fill') %>%\n  gf_labs(title=\"Impact of Stents of Stroke\",\n  subtitle='Experiment with 451 Patients',\n  x=\"Experimental Group\",\n  y=\"Number of Events\")"},{"path":"CS1.html","id":"problem-2","chapter":"1 Case Study","heading":"1.2.2 Problem 2","text":"Migraine acupuncture migraine particularly painful type headache, patients sometimes wish treat acupuncture. determine whether acupuncture relieves migraine pain, researchers conducted randomized controlled study 89 females diagnosed migraine headaches randomly assigned one two groups: treatment control. 43 patients treatment group received acupuncture specifically designed treat migraines. 46 patients control group received placebo acupuncture (needle insertion nonacupoint locations). 24 hours patients received acupuncture, asked pain free.1The data file migraine_study.csv folder data.Complete following work:Read data object called migraine_study.Create table data.Report percent patients treatment group pain free 24 hours receiving acupuncture.23.2% treatment group pain free.Repeat control group.4.3% control group pain free.first glance, acupuncture appear effective treatment migraines? Explain reasoning.Yes, substantial increase percentage patients pain free acupuncture versus acupuncture, appears effective.data provide convincing evidence real pain reduction patients treatment group? think observed difference might just due chance?Either acceptable:get slightly different group estimates even real difference. Though difference big, ’m skeptical results show real difference think might due chance.difference rates looks pretty big, suspect acupuncture positive impact pain.Compile, knit, report html pdf. order knit report pdf, may need install knitr tinytex packages R.Complete computer server.","code":"\nmigraine_study <- read_csv(\"data/migraine_study.csv\")\nhead(migraine_study)## # A tibble: 6 x 2\n##   group     pain_free\n##   <chr>     <chr>    \n## 1 treatment yes      \n## 2 treatment yes      \n## 3 treatment yes      \n## 4 treatment yes      \n## 5 treatment yes      \n## 6 treatment yes\ntally(pain_free~group,data=migraine_study,format=\"proportion\",margin=TRUE)##          group\n## pain_free    control  treatment\n##     no    0.95652174 0.76744186\n##     yes   0.04347826 0.23255814\n##     Total 1.00000000 1.00000000"},{"path":"DB.html","id":"DB","chapter":"2 Data Basics","heading":"2 Data Basics","text":"","code":""},{"path":"DB.html","id":"objectives-1","chapter":"2 Data Basics","heading":"2.1 Objectives","text":"Define use properly context new terminology include limited case, observational unit, variables, data frame, associated variables, independent, discrete continuous variables.Identify define different types variables.reading study, explain research question.Create scatterplot R determine association two numerical variables plot.","code":""},{"path":"DB.html","id":"homework-1","chapter":"2 Data Basics","heading":"2.2 Homework","text":"Identify study componentsIdentify () cases, (ii) variables types, (iii) main research question studies described .","code":""},{"path":"DB.html","id":"problem-1-1","chapter":"2 Data Basics","heading":"2.2.1 Problem 1","text":"Researchers collected data examine relationship pollutants preterm births Southern California. study air pollution levels measured air quality monitoring stations. Specifically, levels carbon monoxide recorded parts per million, nitrogen dioxide ozone parts per hundred million, coarse particulate matter (PM\\(_{10}\\)) \\(\\mu g/m^3\\). Length gestation data collected 143,196 births years 1989 1993, air pollution exposure gestation calculated birth. analysis suggested increased ambient PM\\(_{10}\\) , lesser degree, CO concentrations may associated occurrence preterm births.2The cases 143,196 eligible study subjects born Southern California\n1989 1993.cases 143,196 eligible study subjects born Southern California\n1989 1993.variables measurements carbon monoxide (CO), nitrogen dioxide, ozone, particulate matter less 10\\(\\mu m\\) (PM10) collected air-quality-monitoring stations well length gestation. variables continuous numerical variables.variables measurements carbon monoxide (CO), nitrogen dioxide, ozone, particulate matter less 10\\(\\mu m\\) (PM10) collected air-quality-monitoring stations well length gestation. variables continuous numerical variables.research question association air pollution exposure preterm births?research question association air pollution exposure preterm births?","code":""},{"path":"DB.html","id":"problem-2-1","chapter":"2 Data Basics","heading":"2.2.2 Problem 2","text":"Buteyko method shallow breathing technique developed Konstantin Buteyko, Russian doctor, 1952. Anecdotal evidence suggests Buteyko method can reduce asthma symptoms improve quality life. scientific study determine effectiveness method, researchers recruited 600 asthma patients aged 18-69 relied medication asthma treatment. patients split two research groups: one practiced Buteyko method . Patients scored quality life, activity, asthma symptoms, medication reduction scale 0 10. average, participants Buteyko group experienced significant reduction asthma symptoms improvement quality life.3The cases 600 adult patients aged 18-69 years diagnosed currently treated asthma.variables whether patient practiced Buteyko method (categorical) measures quality life, activity, asthma symptoms medication reduction patients (categorical, ordinal). may also reasonable treat ratings scale 1 10 discrete numerical variables.research question asthmatic patients practice Buteyko method experience improvement condition?","code":""},{"path":"DB.html","id":"problem-3","chapter":"2 Data Basics","heading":"2.2.3 Problem 3","text":"package Stat2Data data set called Election16. Create scatterplot percent advanced degree versus per capita income state. Describe relationship two variables. Note: may load library data set.Load library:Create scatterplot.appears positive association percentage advanced degrees state per capita income.","code":"\nlibrary(Stat2Data)\ndata(Election16)\nElection16 %>%\n  gf_point(Income~Adv)"},{"path":"ODCP.html","id":"ODCP","chapter":"3 Overview of Data Collection Principles","heading":"3 Overview of Data Collection Principles","text":"","code":""},{"path":"ODCP.html","id":"objectives-2","chapter":"3 Overview of Data Collection Principles","heading":"3.1 Objectives","text":"Define use properly context new terminology.description research project, minimum able describe population interest, generalizability study, response predictor variables, differentiate whether observational experimental, determine type sample.Explain context problem conduct sample different types sampling procedures.","code":""},{"path":"ODCP.html","id":"homework-2","chapter":"3 Overview of Data Collection Principles","heading":"3.2 Homework","text":"","code":""},{"path":"ODCP.html","id":"problem-1-2","chapter":"3 Overview of Data Collection Principles","heading":"3.2.1 Problem 1","text":"Generalizability causality\nIdentify population interest sample studies described , studies previous lesson. Also comment whether results study can generalized population findings study can used establish causal relationships.Researchers collected data examine relationship pollutants preterm births Southern California. study air pollution levels measured air quality monitoring stations. Specifically, levels carbon monoxide recorded parts per million, nitrogen dioxide ozone parts per hundred million, coarse particulate matter (PM\\(_{10}\\)) \\(\\mu g/m^3\\). Length gestation data collected 143,196 births years 1989 1993, air pollution exposure gestation calculated birth. analysis suggested increased ambient PM\\(_{10}\\) , lesser degree, CO concentrations may associated occurrence preterm births.4The population interest births. sample consists 143,196 births 1989 1993 Southern California. births time span geography can considered representative births, results generalizable population Southern California. However, since study observational findings used establish causal relationships.Buteyko method shallow breathing technique developed Konstantin Buteyko, Russian doctor, 1952. Anecdotal evidence suggests Buteyko method can reduce asthma symptoms improve quality life. scientific study determine effectiveness method, researchers recruited 600 asthma patients aged 18-69 relied medication asthma treatment. patients split two research groups: one practiced Buteyko method . Patients scored quality life, activity, asthma symptoms, medication reduction scale 0 10. average, participants Buteyko group experienced significant reduction asthma symptoms improvement quality life.5The population 18-69 year olds diagnosed currently treated asthma. sample 600 adult patients aged 18-69 years diagnosed currently treated asthma. Since sample random (voluntary) results generalized population large. However, since study experiment, findings can\nused establish causal relationships.","code":""},{"path":"ODCP.html","id":"problem-2-2","chapter":"3 Overview of Data Collection Principles","heading":"3.2.2 Problem 2","text":"GPA study time\nsurvey conducted 55 undergraduates Duke University took introductory statistics course Spring 2012. Among many questions, survey asked GPA number hours spent studying per week. scatterplot displays relationship two variables.explanatory variable response variable?Describe relationship two variables. Make sure discuss unusual observations, .experiment observational study?Can conclude studying longer hours leads higher GPAs?SolutionsThe explanatory variable number study hours per week, response variable GPA.somewhat weak positive relationship two variables, though data become sparse number study hours increases. One responded reported GPA 4.0, clearly data error. Also, respondents reported unusually high study hours (60 70 hours/week). also \nnoted variability GPA much higher students study less study , also might due fact aren’t many respondents reported studying higher hours.observational study.Since observational study, conclude causal relationship two variables even though appears association.","code":""},{"path":"ODCP.html","id":"problem-3-1","chapter":"3 Overview of Data Collection Principles","heading":"3.2.3 Problem 3","text":"Income educationThe scatterplot shows relationship per capita income (thousands dollars) percent population bachelor’s degree 3,143 counties US 2010.explanatory response variables?Describe relationship two variables. Make sure discuss unusual observations, .Can conclude bachelor’s degree increases one’s income?SolutionsThe explanatory variable percent population bachelor’s degree response variable per capita income (thousands).strong positive linear relationship two variables. percentage population bachelor’s degree increases per capita income increases well. counties 60% population bachelor’s degree countries $50,000 per capita income.observational study make causal statement based results. However, can say higher percentage population bachelor’s degree associated higher per capita income.","code":""},{"path":"STUDY.html","id":"STUDY","chapter":"4 Studies","heading":"4 Studies","text":"","code":""},{"path":"STUDY.html","id":"objectives-3","chapter":"4 Studies","heading":"4.1 Objectives","text":"Define use properly context new terminology.Given study description, able identify explain study using correct terms.Given scenario, describe flaws reasoning propose study sampling designs.","code":""},{"path":"STUDY.html","id":"homework-3","chapter":"4 Studies","heading":"4.2 Homework","text":"","code":""},{"path":"STUDY.html","id":"problem-1-3","chapter":"4 Studies","heading":"4.2.1 Problem 1","text":"Propose sampling strategy\nlarge college class 160 students. 160 students attend lectures together, students divided 4 groups, 40 students, lab sections administered different teaching assistants. professor wants conduct survey satisfied students course, believes lab section student might affect student’s overall satisfaction course.type study ? Observational study.Suggest sampling strategy carrying study. Stratified sample, sample randomly within section.","code":""},{"path":"STUDY.html","id":"problem-2-3","chapter":"4 Studies","heading":"4.2.2 Problem 2","text":"Flawed reasoning\nIdentify flaw reasoning following scenarios. Explain individuals study done differently wanted make strong conclusions.Students elementary school given questionnaire required return parents completed . One questions asked , find work schedule makes difficult spend time kids school? parents replied, 85% said . Based results, school officials conclude great majority parents difficulty spending time kids school.Solution\nNon-responders may different response question. parents returned surveys probably difficulty spending time kids school. Parents work might returned surveys since probably busier schedule.survey conducted simple random sample 1,000 women recently gave birth, asking whether smoked pregnancy. follow-survey asking children respiratory problems conducted 3 years later, however, 567 women reached address. researcher reports 567 women representative mothers.Solution\nunlikely women reached address 3 years later random sample. missing responders probably renters (opposed homeowners) means might lower socio-economic status respondents.","code":""},{"path":"STUDY.html","id":"problem-3-2","chapter":"4 Studies","heading":"4.2.3 Problem 3","text":"Sampling strategies\nstatistics student curious relationship amount time students spend social networking sites performance school decides conduct survey. Four research strategies collecting data described . , name sampling method proposed bias might expect.randomly samples 40 students study’s population, gives survey, asks fill bring back next day.gives survey friends, makes sure one fills survey.posts link online survey Facebook wall asks friends fill survey.stands outside QRC asks every third person walks door fill survey.Solution\n. Simple random sample. Non-response bias, people strong opinions survey responds sample may representative population.\nb. Convenience sample. coverage bias, sample may representative population since consists friends. also possible study non-response bias choose bring back survey.\nc. Convenience sample. similar issues handing surveys friends.\nd. Systematic sample. non-respons bias. also bias three students may enough separation get good coverage.","code":""},{"path":"STUDY.html","id":"problem-4","chapter":"4 Studies","heading":"4.2.4 Problem 4","text":"Vitamin supplements\norder assess effectiveness taking large doses vitamin C reducing duration common cold, researchers recruited 400 healthy volunteers staff students university. quarter patients assigned placebo, rest evenly divided 1g Vitamin C, 3g Vitamin C, 3g Vitamin C plus additives taken onset cold following two days. tablets identical appearance packaging. nurses handed prescribed pills patients knew patient received treatment, researchers assessing patients sick . significant differences observed measure cold duration severity four medication groups, placebo group shortest duration symptoms.experiment observational study? ?explanatory response variables study?patients blinded treatment?study double-blind?Participants ultimately able choose whether use pills prescribed . might expect adhere take pills. introduce confounding variable study? Explain reasoning.Solution\n. Experiment, since researchers randomly assigned different treatments participants.\nb. Response variable: Duration cold.\nExplanatory variable: Treatment, 4 levels; placebo, 1g, 3g, 3g additives.\nc. patients blinded know treatment received.\nd. study double-blind respect researchers evaluating patients, nurses briely interacted patients distribution medication blinded. (partially double-blind.)\ne. Since patients randomly assigned treatment groups blinded expect equal number patients group adhere treatment. means final results study based fewer number participants, non-adherence introduce confounding variable study.","code":""},{"path":"STUDY.html","id":"problem-5","chapter":"4 Studies","heading":"4.2.5 Problem 5","text":"Exercise mental health\nresearcher interested effects exercise mental health proposes following study: Use stratified random sampling ensure representative proportions 18-30, 31-40 41-55 year olds population. Next, randomly assign half subjects age group exercise twice week, instruct rest exercise. Conduct mental health exam beginning end study, compare results.type study ?treatment control groups study?study make use blocking? , blocking variable?study make use blinding?Comment whether results study can used establish causal relationship exercise mental health, indicate whether conclusions can generalized population large.Suppose given task determining proposed study get funding. reservations study proposal?Solution\n. experiment since assigned subjects exercise program.\nb. treatment exercise twice week control exercise.\nc, Yes, blocking variable age.\nd. , study blinded since patients know whether exercising.\ne. Since experiment, can make causal statement. Since sample random, causal statement can generalized population large. However, cautious making causal statement possible placebo effect.\nf. difficult, impossible, successfully conduct study since randomly sampled people required participate clinical trial.","code":""},{"path":"NUMDATA.html","id":"NUMDATA","chapter":"5 Numerical Data","heading":"5 Numerical Data","text":"","code":""},{"path":"NUMDATA.html","id":"objectives-4","chapter":"5 Numerical Data","heading":"5.1 Objectives","text":"Define use properly context new terminology.Generate R summary statistics numeric variable including breaking cases.Generate R appropriate graphical summaries numerical variables.able interpret explain output graphically numerically.","code":""},{"path":"NUMDATA.html","id":"homework-4","chapter":"5 Numerical Data","heading":"5.2 Homework","text":"","code":""},{"path":"NUMDATA.html","id":"problem-1-4","chapter":"5 Numerical Data","heading":"5.2.1 Problem 1","text":"Mammals exploratoryData collected 39 species mammals distributed 13 orders. data openintro package mammalsUsing help, report units variable BrainWt.Using inspect many variables numeric?type variable danger?CategoricalCreate histogram total_sleep describe distribution.distribution unimodal skewed right. appears centered around value 11.Create boxplot life_span describe distribution.Report mean median life span mammal.Calculate summary statistics LifeSpan broken Danger.","code":"\n?mammals\ninspect(mammals)## \n## categorical variables:  \n##      name  class levels  n missing\n## 1 species factor     62 62       0\n##                                    distribution\n## 1 Africanelephant (1.6%) ...                   \n## \n## quantitative variables:  \n##               name   class    min     Q1  median       Q3    max       mean\n## ...1       body_wt numeric  0.005  0.600  3.3425  48.2025 6654.0 198.789984\n## ...2      brain_wt numeric  0.140  4.250 17.2500 166.0000 5712.0 283.134194\n## ...3  non_dreaming numeric  2.100  6.250  8.3500  11.0000   17.9   8.672917\n## ...4      dreaming numeric  0.000  0.900  1.8000   2.5500    6.6   1.972000\n## ...5   total_sleep numeric  2.600  8.050 10.4500  13.2000   19.9  10.532759\n## ...6     life_span numeric  2.000  6.625 15.1000  27.7500  100.0  19.877586\n## ...7     gestation numeric 12.000 35.750 79.0000 207.5000  645.0 142.353448\n## ...8     predation integer  1.000  2.000  3.0000   4.0000    5.0   2.870968\n## ...9      exposure integer  1.000  1.000  2.0000   4.0000    5.0   2.419355\n## ...10       danger integer  1.000  1.000  2.0000   4.0000    5.0   2.612903\n##               sd  n missing\n## ...1  899.158011 62       0\n## ...2  930.278942 62       0\n## ...3    3.666452 48      14\n## ...4    1.442651 50      12\n## ...5    4.606760 58       4\n## ...6   18.206255 58       4\n## ...7  146.805039 58       4\n## ...8    1.476414 62       0\n## ...9    1.604792 62       0\n## ...10   1.441252 62       0\ngf_histogram(~total_sleep,data=mammals,binwidth = 2)\ngf_dens(~total_sleep,data=mammals)\ngf_boxplot(~life_span,data=mammals)\nmean(~life_span,data=mammals,na.rm=TRUE)## [1] 19.87759\nmedian(~life_span,data=mammals,na.rm=TRUE)## [1] 15.1\nfavstats(life_span~danger,data=mammals)##   danger  min     Q1 median     Q3   max     mean       sd  n missing\n## 1      1  3.0  7.700  17.60 32.500 100.0 24.20556 23.53829 18       1\n## 2      2  2.3  4.500  10.40 13.000  50.0 12.92308 13.15948 13       1\n## 3      3  2.0  4.175   5.35  7.875  38.6  9.43750 11.99559  8       2\n## 4      4  2.6  9.775  22.10 27.000  69.0 23.11000 18.75482 10       0\n## 5      5 17.0 20.000  23.60 30.000  46.0 26.95556 10.18910  9       0"},{"path":"NUMDATA.html","id":"problem-2-4","chapter":"5 Numerical Data","heading":"5.2.2 Problem 2","text":"Mammals life spansContinue using mammals data set.Create side--side boxplots life_span broken exposure. Note: change exposure factor(). Report findings.Mammals exposed longer life span. must confounding variable, maybe size animal danger variable.happened median third quartile exposure group 4?median third quartile equal exposure group 4. large number observed mammals life span group.Create faceted histograms. shortcomings plot?awful.enough data histogram; histograms provide little information. Let’s denisty plots.think best graph?Create new variable exposed factor level Low exposure 1 2 High otherwise.Repeat part c new variable.","code":"\nmammals %>%\ngf_boxplot(life_span~factor(exposure))\nfavstats(life_span~factor(exposure),data=mammals)##   factor(exposure)  min    Q1 median     Q3   max     mean       sd  n missing\n## 1                1  2.0  4.35   7.25 14.550 100.0 14.55000 20.98594 24       3\n## 2                2  2.3  6.00  11.20 17.275  50.0 15.39167 14.55819 12       1\n## 3                3  7.6 19.90  26.50 32.000  41.0 25.40000 13.84582  4       0\n## 4                4  7.0 20.20  27.00 27.000  39.3 24.10000 11.78431  5       0\n## 5                5 16.3 20.00  28.00 38.600  69.0 30.53077 14.98084 13       0\ngf_histogram(~life_span,color=~factor(exposure),data=mammals)\ngf_histogram(~life_span|factor(exposure),data=mammals)\ngf_dens(~life_span,color=~factor(exposure),data=mammals)\ngf_dens(~life_span|factor(exposure),data=mammals)\nmammals <- mammals %>%\n  mutate(exposed=factor(ifelse((exposure==1)|(exposure==2),\"Low\",\"High\")))\ninspect(mammals)## \n## categorical variables:  \n##      name  class levels  n missing\n## 1 species factor     62 62       0\n## 2 exposed factor      2 62       0\n##                                    distribution\n## 1 Africanelephant (1.6%) ...                   \n## 2 Low (64.5%), High (35.5%)                    \n## \n## quantitative variables:  \n##               name   class    min     Q1  median       Q3    max       mean\n## ...1       body_wt numeric  0.005  0.600  3.3425  48.2025 6654.0 198.789984\n## ...2      brain_wt numeric  0.140  4.250 17.2500 166.0000 5712.0 283.134194\n## ...3  non_dreaming numeric  2.100  6.250  8.3500  11.0000   17.9   8.672917\n## ...4      dreaming numeric  0.000  0.900  1.8000   2.5500    6.6   1.972000\n## ...5   total_sleep numeric  2.600  8.050 10.4500  13.2000   19.9  10.532759\n## ...6     life_span numeric  2.000  6.625 15.1000  27.7500  100.0  19.877586\n## ...7     gestation numeric 12.000 35.750 79.0000 207.5000  645.0 142.353448\n## ...8     predation integer  1.000  2.000  3.0000   4.0000    5.0   2.870968\n## ...9      exposure integer  1.000  1.000  2.0000   4.0000    5.0   2.419355\n## ...10       danger integer  1.000  1.000  2.0000   4.0000    5.0   2.612903\n##               sd  n missing\n## ...1  899.158011 62       0\n## ...2  930.278942 62       0\n## ...3    3.666452 48      14\n## ...4    1.442651 50      12\n## ...5    4.606760 58       4\n## ...6   18.206255 58       4\n## ...7  146.805039 58       4\n## ...8    1.476414 62       0\n## ...9    1.604792 62       0\n## ...10   1.441252 62       0\ngf_dens(~life_span,color=~exposed,data=mammals)"},{"path":"NUMDATA.html","id":"problem-3-3","chapter":"5 Numerical Data","heading":"5.2.3 Problem 3","text":"Mammals life spans continuedCreate scatterplot life span versus length gestation.type association apparent life span length gestation?weak positive association.type association expect see axes plot reversed, .e. plotted length gestation versus life span?. Since observational data reason believe causal relationship just looking data. Switching axis preserve association.Create new scatterplot suggested c. life span length gestation independent? Explain reasoning.association appears linear. plot looked like “shotgun” blast, consider variables independent. However, remember may confounding variables impact association variables.","code":"\nmammals %>%\ngf_point(life_span~gestation)\nmammals %>%\ngf_point(gestation~life_span)"},{"path":"CATDATA.html","id":"CATDATA","chapter":"6 Categorical Data","heading":"6 Categorical Data","text":"","code":""},{"path":"CATDATA.html","id":"objectives-5","chapter":"6 Categorical Data","heading":"6.1 Objectives","text":"Define use properly context new terminology.Generate R tables categorical variable(s).Generate R appropriate graphical summaries categorical numerical variables.able interpret explain output graphically numerically.","code":""},{"path":"CATDATA.html","id":"homework-5","chapter":"6 Categorical Data","heading":"6.2 Homework","text":"Make sure plots title axes labeled.","code":""},{"path":"CATDATA.html","id":"problem-1-5","chapter":"6 Categorical Data","heading":"6.2.1 Problem 1","text":"Views immigration910 randomly sampled registered voters Tampa, FL asked thought workers illegally entered US () allowed keep jobs apply US citizenship, (ii) allowed keep jobs temporary guest workers allowed apply US citizenship, (iii) lose jobs leave country.data openintro package immigration data object.many levels political ?three levels political conservative, liberal, moderate.Create table using tally.percent Tampa, FL voters identify conservatives?table, 40.88% voters identified conservatives.percent Tampa, FL voters favor citizenship option?, table 30.55% voters favor citizenship option.percent Tampa, FL voters identify conservatives favor citizenship option?table, 6.26% voters conservative favor citizenship option.percent Tampa, FL voters identify conservatives also favor citizenship option? percent moderates liberal share view?need different table question.conservative voters, 15.32% favor citizenship option. numbers 57.71% liberals 33.06% moderates.Create stacked bar chart.Using plot, political ideology views immigration appear independent? Explain reasoning.percentages Tampa, FL conservatives, moderates, liberals favor illegal immigrants working US staying applying citizenship quite different one another. Therefore, two variables appear dependent.","code":"\nlevels(immigration$political)## [1] \"conservative\" \"liberal\"      \"moderate\"\ninspect(immigration)## \n## categorical variables:  \n##        name  class levels   n missing\n## 1  response factor      4 910       0\n## 2 political factor      3 910       0\n##                                    distribution\n## 1 Leave the country (38.5%) ...                \n## 2 conservative (40.9%), moderate (39.9%) ...\nround(tally(~response+political,data=immigration,format=\"percent\",margins = TRUE),2)##                        political\n## response                conservative liberal moderate  Total\n##   Apply for citizenship         6.26   11.10    13.19  30.55\n##   Guest worker                 13.30    3.08    12.42  28.79\n##   Leave the country            19.67    4.95    13.85  38.46\n##   Not sure                      1.65    0.11     0.44   2.20\n##   Total                        40.88   19.23    39.89 100.00\nround(tally(response~political,data=immigration,format=\"percent\",margins = TRUE),2)##                        political\n## response                conservative liberal moderate\n##   Apply for citizenship        15.32   57.71    33.06\n##   Guest worker                 32.53   16.00    31.13\n##   Leave the country            48.12   25.71    34.71\n##   Not sure                      4.03    0.57     1.10\n##   Total                       100.00  100.00   100.00\nimmigration %>%\n  gf_props(~political,fill=~response,position=\"fill\") %>%\n  gf_labs(title=\"Tampa Florida Voter Views on Illegal Immigrant Workers\",\n          subtitle=\"Broken down by political views\",x=\"Political View\",y=\"Proportion\") %>%\n  gf_theme(theme_bw())"},{"path":"CATDATA.html","id":"problem-2-5","chapter":"6 Categorical Data","heading":"6.2.2 Problem 2","text":"Views DREAM Act survey Exercise 1 also asked respondents support DREAM Act, proposed law provide path citizenship people brought illegally US children.data openintro package dream data object.Create mosaic plot.Based mosaic plot, views DREAM Act political ideology independent?vertical locations ideological groups break Yes, , Sure categories differ, indicates variables dependent.","code":"\nmosaic(stance~ideology,data=dream,sub=\"Voter views on illegal worker status\")"},{"path":"CATDATA.html","id":"problem-3-4","chapter":"6 Categorical Data","heading":"6.2.3 Problem 3","text":"Heart transplantsThe Stanford University Heart Transplant Study conducted determine whether experimental heart transplant program increased lifespan. patient entering program designated official heart transplant candidate, meaning gravely ill likely benefit new heart. patients got transplant . variable transplant indicates group patients ; patients treatment group got transplant control group . Another variable called survived used indicate whether patient alive end study.data openintro package called heart_transplant.Create mosaic plot.Based mosaic plot, survival independent whether patient got transplant? Explain reasoning.Proportion patients alive end study higher treatment group control group. data suggest survival independent whether patient got transplant.Using survtime create side--side boxplots control treatment groups.box plots suggest efficacy (effectiveness) transplants?shape distribution survival times groups right skewed one clear outlier control group possible outliers groups high end. median survival time control group much lower median survival time treatment group; patients got transplant typically lived longer. Tying together much lower variability control group, evident much smaller IQR treatment group (50 days versus 500 days), can see patients get heart transplant tended consistently die quite early relative transplant. Overall, patients without transplants made beyond year nearly half transplant patients survived least one year. also noted first third quartiles treatment group higher control group, IQR treatment group much bigger, indicating variability survival times treatment group.","code":"\nmosaic(survived~transplant,data=heart_transplant)\nheart_transplant %>%\n  gf_boxplot(survtime~transplant) %>%\n  gf_labs(title=\"Survival times for tranplant experiment\",\n          sub=\"Treatment group had the transplant\",x=\"Tranplant\",y=\"Survival time in days\") %>%\n  gf_theme(theme_classic())"},{"path":"CS2.html","id":"CS2","chapter":"7 Case Study","heading":"7 Case Study","text":"","code":""},{"path":"CS2.html","id":"objectives-6","chapter":"7 Case Study","heading":"7.1 Objectives","text":"Use R simulate probabilistic model.Use basic counting methods.","code":""},{"path":"CS2.html","id":"homework-6","chapter":"7 Case Study","heading":"7.2 Homework","text":"","code":""},{"path":"CS2.html","id":"problem-1-6","chapter":"7 Case Study","heading":"7.2.1 Problem 1","text":"Exactly 2 people birthday - Simulation\nComplete similar analysis case exactly 2 people room 23 people birthday. exercise use computational simulation.Create new R Markdown file create report. Yes, know use file want practice generating report.Create new R Markdown file create report. Yes, know use file want practice generating report.Simulate 23 people class day year equally likely. Find cases exactly 2 people birthday, alter code Notes changing 18 23.Simulate 23 people class day year equally likely. Find cases exactly 2 people birthday, alter code Notes changing 18 23.Plot frequency occurrences bar chart.Plot frequency occurrences bar chart.Estimate probability exactly two people birthday.Estimate probability exactly two people birthday.","code":"\n(do(10000)*length(unique(sample(days,size=23,replace = TRUE)))) %>%\n  mutate(match=if_else(length==22,1,0)) %>%\n  summarise(prob=mean(match))##     prob\n## 1 0.3636\n(do(1000)*length(unique(sample(days,size=23,replace = TRUE)))) %>%\n  gf_bar(~length)"},{"path":"CS2.html","id":"problem-2-6","chapter":"7 Case Study","heading":"7.2.2 Problem 2","text":"Exactly 2 people birthday - Mathematical\nRepeat problem 1 mathematically. big hint, need use choose() function. idea 23 people need choose 2 match. thus need multiply, multiplication rule , choose(23,2). trouble, work total 3 people room first.Find formula determine exact probability exactly 2 people room 23 birthday.Find formula determine exact probability exactly 2 people room 23 birthday.Generalize solution number n people room create function.Generalize solution number n people room create function.Vectorize function.Vectorize function.Plot probability exactly 2 people birthday versus number people room.Plot probability exactly 2 people birthday versus number people room.Comment shape curve explain .Comment shape curve explain .knit compile report.knit compile report.two people haveBy way, exactly three matches simulation hard. table dataTwo sets different birthdayMathematically exactly 3 easy. Simulation seems little math formula .","code":"\nchoose(23,2)*prod(365:344)/365^23## [1] 0.3634222\nexactly_two <- function(n){\n  choose(n,2)*prod(365:(365-(n-2)))/365^n\n}\nexactly_two(23)## [1] 0.3634222\nexactly_two <- Vectorize(exactly_two)\ngf_line(exactly_two(1:100)~ seq(1,100),\n        xlab=\"Number of People\",\n        ylab=\"Probability of Match\",\n        title=\"Probability of exactly least 2 people with matching birthdays\")\nset.seed(10)\ntemp <- table(sample(days,size=23,replace = TRUE))\ntemp## \n##  13  24  50  72  92 110 137 143 154 155 211 231 263 271 285 330 338 342 344 351 \n##   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   2   1   1   1   1 \n## 365 \n##   1\n(sum(temp==2) == 2)+0## [1] 1\n(do(10000)*((sum(table(sample(days,size=23,replace = TRUE)) == 3)==1)+0)) %>%\n  summarise(prob=mean(result))##     prob\n## 1 0.0117\n(do(10000)*((sum(table(sample(days,size=23,replace = TRUE)) == 2)==2)+0)) %>%\n  summarise(prob=mean(result))##     prob\n## 1 0.1139\n(do(10000)*length(unique(sample(days,size=23,replace = TRUE)))) %>%\n  mutate(match=if_else(length==21,1,0)) %>%\n  summarise(prob=mean(match))##     prob\n## 1 0.1187\nchoose(23,3)*prod(365:345)/365^23## [1] 0.007395218"},{"path":"PROBRULES.html","id":"PROBRULES","chapter":"8 Probability Rules","heading":"8 Probability Rules","text":"","code":""},{"path":"PROBRULES.html","id":"objectives-7","chapter":"8 Probability Rules","heading":"8.1 Objectives","text":"Define use properly context new terminology related probability include limited : outcome, event, sample space, probability.Apply basic probability counting rules find probabilities.Describe basic axioms probability.Use R calculate simulate probabilities events.","code":""},{"path":"PROBRULES.html","id":"homework-7","chapter":"8 Probability Rules","heading":"8.2 Homework","text":"","code":""},{"path":"PROBRULES.html","id":"problem-1-7","chapter":"8 Probability Rules","heading":"8.2.1 Problem 1","text":"Let \\(\\), \\(B\\) \\(C\\) events \\(\\mbox{P}()=0.5\\), \\(\\mbox{P}(B)=0.3\\), \\(\\mbox{P}(C)=0.4\\). Also, know \\(\\mbox{P}(\\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(\\cap C)=0.1\\), \\(\\mbox{P}(\\cap B \\cap C)=0.05\\). Find following:\\(\\mbox{P}(\\cup B)\\)\n\\[\n\\mbox{P}(\\cup B) = \\mbox{P}()+\\mbox{P}(B)-\\mbox{P}(\\cap B)= 0.5+0.3-0.2 = 0.6\n\\]\\(\\mbox{P}(\\cup B)\\)\n\\[\n\\mbox{P}(\\cup B) = \\mbox{P}()+\\mbox{P}(B)-\\mbox{P}(\\cap B)= 0.5+0.3-0.2 = 0.6\n\\]\\(\\mbox{P}(\\cup B \\cup C)\\)\n\\[\n\\mbox{P}(\\cup B \\cup C) = \\mbox{P}()+\\mbox{P}(B)+\\mbox{P}(C)-\\mbox{P}(\\cap B)-\\mbox{P}(\\cap C)-\\mbox{P}(B\\cap C)+\\mbox{P}(\\cap B \\cap C)\n\\]\n\\[\n= 0.5+0.3+0.4-0.2-0.12-0.1+0.05 = 0.83\n\\]\\(\\mbox{P}(\\cup B \\cup C)\\)\n\\[\n\\mbox{P}(\\cup B \\cup C) = \\mbox{P}()+\\mbox{P}(B)+\\mbox{P}(C)-\\mbox{P}(\\cap B)-\\mbox{P}(\\cap C)-\\mbox{P}(B\\cap C)+\\mbox{P}(\\cap B \\cap C)\n\\]\n\\[\n= 0.5+0.3+0.4-0.2-0.12-0.1+0.05 = 0.83\n\\]\\(\\mbox{P}(B'\\cap C')\\)\n\\[\n\\mbox{P}(B'\\cap C')=\\mbox{P}((B\\cup C)') = 1-\\mbox{P}(B\\cup C) = 1-[\\mbox{P}(B)+\\mbox{P}(C)-\\mbox{P}(B\\cap C)]\n\\]\n\\[\n= 1-(0.3+0.4-0.12) = 0.42\n\\]\\(\\mbox{P}(B'\\cap C')\\)\n\\[\n\\mbox{P}(B'\\cap C')=\\mbox{P}((B\\cup C)') = 1-\\mbox{P}(B\\cup C) = 1-[\\mbox{P}(B)+\\mbox{P}(C)-\\mbox{P}(B\\cap C)]\n\\]\n\\[\n= 1-(0.3+0.4-0.12) = 0.42\n\\]\\(\\mbox{P}(\\cup (B\\cap C))\\)\n\\[\n\\mbox{P}(\\cup (B\\cap C)) = \\mbox{P}()+\\mbox{P}(B\\cap C) -\\mbox{P}(\\cap B \\cap C) = 0.5+0.12-0.05 = 0.57\n\\]\\(\\mbox{P}(\\cup (B\\cap C))\\)\n\\[\n\\mbox{P}(\\cup (B\\cap C)) = \\mbox{P}()+\\mbox{P}(B\\cap C) -\\mbox{P}(\\cap B \\cap C) = 0.5+0.12-0.05 = 0.57\n\\]\\(\\mbox{P}((\\cup B \\cup C)\\cap (\\cap B \\cap C)')\\)\n\\[\n\\mbox{P}((\\cup B \\cup C)\\cap (\\cap B \\cap C)')=\\mbox{P}(\\cup B \\cup C)-\\mbox{P}(\\cap B \\cap C) = 0.83-0.05 = 0.78\n\\]\\(\\mbox{P}((\\cup B \\cup C)\\cap (\\cap B \\cap C)')\\)\n\\[\n\\mbox{P}((\\cup B \\cup C)\\cap (\\cap B \\cap C)')=\\mbox{P}(\\cup B \\cup C)-\\mbox{P}(\\cap B \\cap C) = 0.83-0.05 = 0.78\n\\]","code":""},{"path":"PROBRULES.html","id":"problem-2-7","chapter":"8 Probability Rules","heading":"8.2.2 Problem 2","text":"Consider example family reading. probability family least one boy?\\[\n\\mbox{P}(\\mbox{least one boy})=1-\\mbox{P}(\\mbox{boys})=1-\\mbox{P}(\\mbox{GGG})=1-\\frac{1}{8} = 0.875\n\\]","code":""},{"path":"PROBRULES.html","id":"problem-3-5","chapter":"8 Probability Rules","heading":"8.2.3 Problem 3","text":"Birthday Problem Revisited.Suppose \\(n=20\\) students classroom. birthday, instructor, April 3rd. probability least one student shares birthday? Assume 365 days year assume birthdays equally likely.\\[\n\\mbox{P}(\\mbox{least one person shares bday})=1-\\mbox{P}(\\mbox{one else bday}) = \n\\]\\[\n1-\\left( \\frac{364}{365}\\right)^{20} = 0.0534\n\\]R, find probability least one person shares birthday value \\(n\\) 1 300. Plot probabilities \\(n\\) \\(x\\)-axis probability \\(y\\)-axis. value \\(n\\) probability least 50%?Generalizing,\n\\[\n\\mbox{P}(\\mbox{least one person shares bday})=1-\\mbox{P}(\\mbox{one else bday}) = 1-\\left( \\frac{364}{365}\\right)^{n}\n\\]Check function.253 people.","code":"\nn<-1:300\nmybday<-function(x) 1-(364/365)^x\nmybday <- Vectorize(mybday)\nmybday(20)## [1] 0.05339153\ngf_line(mybday(n)~ n,\n        xlab=\"Number of People\",\n        ylab=\"Probability of Match\",\n        title=\"Probability of at least 1 person matching my birthday\") %>%\n  gf_theme(theme_bw)\nprob <- mybday(n)\nwhich(prob>= .5)##  [1] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n## [20] 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290\n## [39] 291 292 293 294 295 296 297 298 299 300"},{"path":"PROBRULES.html","id":"problem-4-1","chapter":"8 Probability Rules","heading":"8.2.4 Problem 4","text":"Thinking cards . Answer following questions:Define two events mutually exclusive.first card drawn red.\nfirst card drawn black.Define two events independent.first card drawn black.\nfirst card drawn face card.Define event complement.first card drawn less 5.\nfirst card drawn equal 5.","code":""},{"path":"PROBRULES.html","id":"problem-5-1","chapter":"8 Probability Rules","heading":"8.2.5 Problem 5","text":"Consider license plate example reading.probability license plate contains exactly one “B”?probability license plate contains least one “B”?\\[\n1-\\mbox{P}(\\mbox{B's})\n\\]","code":"\n#fourth spot\nnum4<-10*10*10*1*25*25\n\n#fifth spot\nnum5<-10*10*10*25*1*25\n\n#sixth spot\nnum6<-10*10*10*25*25*1\n\ndenom<-10*10*10*26*26*26\n\n(num4+num5+num6)/denom## [1] 0.1066796\nnum0<-10*10*10*25*25*25\n1-num0/denom## [1] 0.1110036"},{"path":"PROBRULES.html","id":"problem-6","chapter":"8 Probability Rules","heading":"8.2.6 Problem 6","text":"Consider party example reading.Suppose 8 people showed party dressed zombies. probability three awards won people dressed zombies?\n\\[\n\\frac{8\\cdot 7 \\cdot 6}{25\\cdot 24 \\cdot 23}\n\\]probability zombies win “creative” “funniest” “scariest”?\n\\[\n\\frac{8 \\cdot 17 \\cdot 7}{25 \\cdot 24 \\cdot 23}\n\\]","code":"\n(8*7*6)/(25*24*23)## [1] 0.02434783\n(8*17*7)/(25*24*23)## [1] 0.06898551"},{"path":"PROBRULES.html","id":"problem-7","chapter":"8 Probability Rules","heading":"8.2.7 Problem 7","text":"Consider cards example reading.many ways can obtain “two pairs” (2 one number, 2 another, final different)?pick rank two pairs.\\[\\binom{13}{2}\\]\nNotice order doesn’t matter pair Kings 4s pair 4s Kings. different full house example. Make sure understand point.Now pick two fours cards rank\\[\\binom{4}{2}\\binom{4}{2}\\]finally need last card come 44 remaining cards don’t get full house.\\(\\binom{44}{1}\\)Putting together:\\(\\binom{13}{2}\\binom{4}{2}\\binom{4}{2}\\binom{44}{1}\\)probability drawing “four kind” (four cards value)?\\[\n\\mbox{P}(\\mbox{4 kind})=\\frac{\\binom{13}{1}\\binom{4}{4}\\binom{48}{1}}{\\binom{52}{5}}\n\\]","code":"\nchoose(13,2)*choose(4,2)*choose(4,2)*choose(44,1)## [1] 123552\n(13*1*48)/choose(52,5)## [1] 0.000240096"},{"path":"PROBRULES.html","id":"problem-8","chapter":"8 Probability Rules","heading":"8.2.8 Problem 8","text":"Advanced Question: Consider rolling 5 dice. probability pour resulting full house?First pick value three kind, 6. pick value remaining 5 two kind. actually permutation. 30 distinct “flavors” full house (three 1’s & two 2’s, three 1’s & two 3’s, etc.). reading \n\\[\n\\binom{6}{1} \\times \\binom{5}{1}\n\\]\nnow 5 dice. select three value order doesn’t matter since value. Thus multiple \\(\\binom{5}{3}\\). Divide total distinct ways dice landed (assuming order matters).\n\\[\n\\mbox{P}(\\mbox{full house}) = \\frac{30 \\times \\frac{5!}{3!2!}}{6^5}\n\\]\n\\[\n\\mbox{P}(\\mbox{full house}) = \\frac{\\binom{6}{1} \\times \\binom{5}{1} \\times \\binom{5}{3}}{6^5}\n\\]Simulating tough let’s write code may help.Let’s write function.","code":"\n30*10/(6^5)## [1] 0.03858025\nset.seed(23)\ntemp<-table(sample(1:6,size=5,replace=TRUE))\ntemp## \n## 1 3 4 5 \n## 1 2 1 1\nsum(temp==2) & sum(temp==3)## [1] FALSE\ntemp<-c(1,1,1,2,2)\ntemp<-table(temp)\ntemp## temp\n## 1 2 \n## 3 2\nsum(temp==2) & sum(temp==3)## [1] TRUE\nfull_house <-function(x){\n  temp<-table(x)\n  sum(temp==2) & sum(temp==3)\n}\ntemp<-c(1,1,1,2,2)\nfull_house(temp)## [1] TRUE\nset.seed(751)\nresults<-do(10000)*full_house(sample(1:6,size=5,replace=TRUE))\nmean(~full_house,data=results)## [1] 0.039"},{"path":"CONDPROB.html","id":"CONDPROB","chapter":"9 Conditional Probability","heading":"9 Conditional Probability","text":"","code":""},{"path":"CONDPROB.html","id":"objectives-8","chapter":"9 Conditional Probability","heading":"9.1 Objectives","text":"Define conditional probability distinguish joint probability.Find conditional probability using definition.Using conditional probability, determine whether two events independent.Apply Bayes’ Rule mathematically via simulation.","code":""},{"path":"CONDPROB.html","id":"homework-8","chapter":"9 Conditional Probability","heading":"9.2 Homework","text":"","code":""},{"path":"CONDPROB.html","id":"problem-1-8","chapter":"9 Conditional Probability","heading":"9.2.1 Problem 1","text":"Consider Exercise 1 Lesson 2. Recall: \\(\\), \\(B\\) \\(C\\) events \\(\\mbox{P}()=0.5\\), \\(\\mbox{P}(B)=0.3\\), \\(\\mbox{P}(C)=0.4\\), \\(\\mbox{P}(\\cap B)=0.2\\), \\(\\mbox{P}(B \\cap C)=0.12\\), \\(\\mbox{P}(\\cap C)=0.1\\), \\(\\mbox{P}(\\cap B \\cap C)=0.05\\).\\(\\) \\(B\\) independent?. \\(\\mbox{P}()\\mbox{P}(B)=0.15\\neq \\mbox{P}(\\cap B)\\).\\(B\\) \\(C\\) independent?Yes. \\(\\mbox{P}(B)\\mbox{P}(C)=0.12 = \\mbox{P}(B\\cap C)\\). Also,\n\\[\n\\mbox{P}(B|C)=\\frac{\\mbox{P}(B\\cap C)}{\\mbox{P}(C)}= 0.12/0.4 = 0.3 =\\mbox{P}(B)\n\\]","code":""},{"path":"CONDPROB.html","id":"problem-2-8","chapter":"9 Conditional Probability","heading":"9.2.2 Problem 2","text":"Suppose biased coin (probability flip heads 0.6). flip coin twice. Assume coin memoryless (flips independent one another).probability second flip results heads?0.6What probability second flip results heads, given first also resulted heads?coin memoryless. ,\n\\[\n\\mbox{P}(\\mbox{2nd flip heads}|\\mbox{1st flip heads}) = 0.6\n\\]probability flips result heads?Since flips independent,\n\\[\n\\mbox{P}(\\mbox{heads})=\\mbox{P}(\\mbox{1st flip heads})\\mbox{P}(\\mbox{2nd flip heads}) = 0.6*0.6=0.36\n\\]probability exactly one coin flip results heads?happen two ways. first heads second heads.\n\\[\n\\mbox{P}(\\mbox{exactly one heads})=\\mbox{P}(\\mbox{1st flip heads})\\mbox{P}(\\mbox{2nd flip tails}) + \\mbox{P}(\\mbox{1st flip tails})\\mbox{P}(\\mbox{2nd flip heads})\n\\]\n\\[\n0.6*0.4+0.4*0.6 = 0.48\n\\]Now assume flip coin five times. probability result 5 heads?\n\\[\n\\mbox{P}(\\mbox{5 heads})= 0.6^5 = 0.0778\n\\]probability result exactly 2 heads (5 flips)?\\(\\binom{5}{2} = 10\\) ways happen ({HHTTT},{HTHTT},…). ,\n\\[\n\\mbox{P}(\\mbox{2 heads 5 flips})=\\binom{5}{2} 0.6^2(1-0.6)^3 = 0.2304\n\\]","code":"\n0.6^5## [1] 0.07776\nchoose(5,2)*0.6^2*0.4^3## [1] 0.2304"},{"path":"CONDPROB.html","id":"problem-3-6","chapter":"9 Conditional Probability","heading":"9.2.3 Problem 3","text":"(Adapted IPSUR, (Kerns 2010)). Suppose three assistants working company: Moe, Larry Curly. three assist filing process. one filing assistant needed time. Moe assists 60% time, Larry assists 30% time Curly assists remaining 10% time. Occasionally, make errors (misfiles); Moe misfile rate 0.01, Larry misfile rate 0.025, Curly rate 0.05. Suppose misfile discovered, unknown schedule occurred. likely committed misfile? Calculate probabilities three assistants.Let \\(E\\) event misfile committed. Also, let \\(M\\), \\(L\\), \\(C\\) denote events Moe, Larry Curly assistant time, respectively.\\[\n\\mbox{P}(E)=\\mbox{P}(E \\cap M)+\\mbox{P}(E \\cap L)+\\mbox{P}(E\\cap C) \n\\]\n\\[\n= \\mbox{P}(E|M)\\mbox{P}(M)+\\mbox{P}(E|L)\\mbox{P}(L)+\\mbox{P}(E|C)\\mbox{P}(C) = 0.01*0.6+0.025*0.3+0.05*0.1 = 0.0185\n\\]Thus,\n\\[\n\\mbox{P}(M|E)=\\frac{\\mbox{P}(E \\cap M)}{\\mbox{P}(E)}= \\frac{0.01*0.6}{0.0185}=0.3243\n\\]Similarly, \\(\\mbox{P}(L|E)=0.4054\\) \\(\\mbox{P}(C|E)=0.2702\\).Larry assistant likely committed error.","code":""},{"path":"CONDPROB.html","id":"problem-4-2","chapter":"9 Conditional Probability","heading":"9.2.4 Problem 4","text":"playing game two coins. One coin fair comes heads 80% time. One coin flipped 3 times result three heads, probability coin flipped fair coin? need make assumption probability either coin selected.Use Bayes formula solve problem.assume either coin selected 50% probability.\\[\n\\mbox{P}(Fair) = \\mbox{P}(Biased) = .5\n\\]\n\\[\n\\mbox{P}(3 Heads|Fair)=\\frac{1}{2}^3=\\frac{1}{8}\n\\]\n\\[\n\\mbox{P}(3 Heads|Biased)=.8^3=0.512\n\\]Now\\[\n\\mbox{P}(Fair | 3 Heads) = \\frac{\\mbox{P}(3 Heads | Fair)\\mbox{P}(Fair)}{\\mbox{P}(3 Heads | Fair)\\mbox{P}(Fair)+\\mbox{P}(3 Heads| Biased)\\mbox{P}(Biased)}\n\\]\\[\n\\mbox{P}(Fair | 3 Heads) =  \\frac{\\frac{1}{8}\\frac{1}{2}}{\\frac{1}{8}\\frac{1}{2}+.8^{3}\\frac{1}{2}} = 0.196\n\\]Use simulation solve problem.Let’s use assumptions. problem two ways. flip coin fixed number times combine information use random process pick flipped coin flip three times. Let’s first.Let’s flip fair coin 50,000 times count many heads get.Now flip biased coin.total 6157 + 25743 heads 6157 came fair coin.Thus probability coin fair given 3 heads flips :19.3%.Next pick one coins equal probability 100,000 times.Now fair coin flipped 50226 times.Let’s see many times get 3 heads flip coin 3 times.6270 cases 3 heads. Now biased coin.Now can determine probability fair coin given 3 heads.code easily adapt don’t think coin selected frequency. Suppose think fair coin 75% chance selected. analysis look like :Now fair coin flipped 75023 times.Let’s see many times get 3 heads flip coin 3 times.9579 cases 3 heads. Now biased coin.Now can determine probability fair coin given 3 heads.much different answer. prior getting data believed fair coin selected 75% probability. data indicates need update lower probability. flipped 3 times evidence favor biased coin, probability dropped substantially. Bayes powerful tool.Think just problem. started subjective believe either coin selected equal probability. called prior probability. collected data three flips coin. used empirical data update belief posterior probability. basis Bayesian statistical analysis. Bayesian statistics entire discipline unto .","code":"\n.125*.5/(.125*.5+.8^3*.5)## [1] 0.1962323\nset.seed(1154)\ndata.frame(do(50000)*rflip(3)) %>%\n  filter(heads==3) %>%\n  summarise(count=n()) %>%\n  pull()## [1] 6157\ndata.frame(do(50000)*rflip(3,prob=0.8)) %>%\n  filter(heads==3) %>%\n  summarise(count=n()) %>%\n  pull()## [1] 25743\n6157/(6157 + 25743)## [1] 0.1930094\nset.seed(501)\nresults <- rflip(100000,summarize = TRUE)\nresults##       n heads tails prob\n## 1 1e+05 50226 49774  0.5\ndata.frame(do(50226)*rflip(3)) %>%\n  filter(heads==3) %>%\n  summarise(count=n()) %>%\n  pull()## [1] 6270\ndata.frame(do(49774)*rflip(3,prob=0.8)) %>%\n  filter(heads==3) %>%\n  summarise(count=n()) %>%\n  pull()## [1] 25512\n6270/(6270+25512)## [1] 0.1972815\nset.seed(9021)\nresults <- rflip(100000,prob=.75,summarize = TRUE)\nresults##       n heads tails prob\n## 1 1e+05 75023 24977 0.75\ndata.frame(do(75023)*rflip(3)) %>%\n  filter(heads==3) %>%\n  summarise(count=n()) %>%\n  pull()## [1] 9579\ndata.frame(do(24977)*rflip(3,prob=0.8)) %>%\n  filter(heads==3) %>%\n  summarise(count=n()) %>%\n  pull()## [1] 12789\n9579/(9579+12789)## [1] 0.4282457"},{"path":"RANDVAR.html","id":"RANDVAR","chapter":"10 Random Variables","heading":"10 Random Variables","text":"","code":""},{"path":"RANDVAR.html","id":"objectives-9","chapter":"10 Random Variables","heading":"10.1 Objectives","text":"Define use properly context new terminology.Given discrete random variable, obtain pmf cdf, use obtain probabilities events.Simulate random variable discrete distribution.Find moments discrete random variable.Find expected value linear transformation random variable.","code":""},{"path":"RANDVAR.html","id":"homework-9","chapter":"10 Random Variables","heading":"10.2 Homework","text":"","code":""},{"path":"RANDVAR.html","id":"problem-1-9","chapter":"10 Random Variables","heading":"10.2.1 Problem 1","text":"Suppose flipping fair coin, result single coin flip either heads tails. Let \\(X\\) random variable representing number flips first heads.\\(X\\) discrete continuous? domain/support \\(X\\)?\\(X\\) discrete since number flips discrete process (can’t perform fraction flip). wording specific number flips first heads, must flip least . domain \\(X\\) \\(S_X=\\{1,2,...\\}\\).values expect \\(X\\) take? think average \\(X\\)? Don’t actually formal math, just think flipping regular coin, long take get first heads.expect \\(X\\) 1 2 fairly often, since coin fair even chance landing heads tails. expect large values \\(X\\) rare. reasons, think average \\(X\\) around 2 flips little less 2.Advanced: R, generate 10,000 observations \\(X\\). average value \\(X\\) based simulation?Note: many ways . description one approach.Now repeat using replicate() (). repeat 10000 times.predicted, mean close 2, common values \\(X\\) 1 2. common 1 occurring 50% time, think since coin comes Heads 50% time.know \\(\\mbox{P}(X=1) = \\frac{1}{2}\\) \\(\\mbox{P}(X=2) = \\frac{1}{2^2}\\) general \\(\\mbox{P}(X=x) = \\frac{1}{2^x}\\). pmf.extra, show sum infinite sequence probabilities 1 requires Calculus knowledge. Let’s start partial sum:\n\\[S_n=\\frac{1}{2}+\\frac{1}{4} +\\cdots + \\frac{1}{2^n}\\]\nNow multiply sides \\(\\frac{1}{2}\\).\\[\\frac{1}{2}S_n=\\frac{1}{4}+\\frac{1}{8} +\\cdots + \\frac{1}{2^{n+1}}\\]difference two sums \n\\[S_n-\\frac{1}{2}S_n=\\frac{1}{2}S_n=\\frac{1}{2}-\\frac{1}{2^{n+1}}\\]Now \\[\\lim_{n \\+\\infty} \\frac{1}{2^{n+1}} = 0\\]\\[\\lim_{n \\+\\infty} \\left[ \\frac{1}{2}S_n=\\frac{1}{2}-\\frac{1}{2^{n+1}} \\right]\\]implies \\(S = 1\\).","code":"\nset.seed(68)\nwhich(sample(c(\"H\",\"T\"),1000,replace=TRUE)==\"H\")[1]## [1] 2\nresults <- do(10000)*which(sample(c(\"H\",\"T\"),1000,replace=TRUE)==\"H\")[1]\nmean(~result,data=results)## [1] 1.9849\ntally(~result,data=results,format=\"percent\")## result\n##     1     2     3     4     5     6     7     8     9    10    11    12    13 \n## 49.89 25.35 12.33  6.56  3.25  1.27  0.66  0.39  0.12  0.07  0.06  0.03  0.02\nresults %>%\n  gf_props(~result,fill=\"cyan\",color = \"black\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Number of flips\",\n          subtitle=\"Number of flips until first heads\")"},{"path":"RANDVAR.html","id":"problem-2-9","chapter":"10 Random Variables","heading":"10.2.2 Problem 2","text":"Repeat Problem 1, except part d, different random variable, \\(Y\\): number coin flips fifth heads.\\(Y\\) discrete reasons \\(X\\). domain \\(Y\\) \\(S_Y=\\{5,6,...\\}\\).\\(Y\\) discrete reasons \\(X\\). domain \\(Y\\) \\(S_Y=\\{5,6,...\\}\\).order land heads five times, reasonable expect around 9 13 flips. Thus, expect \\(Y\\) take values 8, 9, 10, 11, 12 fairly often, values outside range less often. think average \\(Y\\) around 10 .order land heads five times, reasonable expect around 9 13 flips. Thus, expect \\(Y\\) take values 8, 9, 10, 11, 12 fairly often, values outside range less often. think average \\(Y\\) around 10 .common values \\(Y\\) 6 11. average \\(Y\\) simulation 9.97, close predicted.pmf bad must know binomial distribution first. get fifth heads nth flip, prior n-1 flips binomial n-1 successes. final flip success multiply binomial probability success.","code":"\nset.seed(102)\nresults <- do(10000)*which(sample(c(\"H\",\"T\"),1000,replace=TRUE)==\"H\")[5]\nmean(~result,data=results)## [1] 9.9728\ntally(~result,data=results,format=\"percent\")## result\n##     5     6     7     8     9    10    11    12    13    14    15    16    17 \n##  3.06  8.21 12.14 13.26 13.71 11.52 10.74  8.17  6.06  4.50  3.02  1.86  1.32 \n##    18    19    20    21    22    23    24    25    28    29 \n##  0.88  0.65  0.31  0.21  0.16  0.12  0.04  0.04  0.01  0.01\nresults %>%\n  gf_props(~result,fill=\"cyan\",color = \"black\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Number of flips\",\n          subtitle=\"Number of flips until 5th heads\")"},{"path":"RANDVAR.html","id":"problem-3-7","chapter":"10 Random Variables","heading":"10.2.3 Problem 3","text":"Suppose data analyst large international airport. boss, head airport, dismayed airport received negative attention press inefficiencies sluggishness. staff meeting, boss gives week build report addressing “timeliness” airport. boss big hurry gives information guidance task.Prior building report, need conduct analysis. aid , create list least three random variables help address timeliness airport. random variables,Determine whether discrete continuous.Determine whether discrete continuous.Report domain.Report domain.experimental unit?experimental unit?Explain random variable useful addressing timeliness airport.Explain random variable useful addressing timeliness airport.provide one example:Let \\(D\\) difference flight’s actual departure scheduled departure. continuous random variable, since time can measured fractions minutes. flight can early late, domain real number. experimental unit individual (non-canceled) flight. useful random variable average value \\(D\\) describe whether flights take time. also find often \\(D\\) exceeds 0 (implying late departure) often \\(D\\) exceeds 30 minutes, indicate “late” departure.many correct answers.\\(X\\): Time takes passenger go security (defined time entering security line departing security belongings). Continuous. Experimental unit individual passenger. variable help identify whether security line long. also explore \\(X\\) changes based day time day.\\(Y\\): Status scheduled departure (time, somewhat late, late, canceled). Discrete. Experimental unit scheduled departure. variable help describe often flights canceled late. also explore \\(Y\\) airline, destination, time day, etc.\\(Z\\): Number time-related complaints customer service desk given day. Discrete. Experimental unit day. variable describe attitudes/perceptions customers. probably bad sign customers feel like airport working efficiently. can explore \\(Z\\) changes time.","code":""},{"path":"RANDVAR.html","id":"problem-4-3","chapter":"10 Random Variables","heading":"10.2.4 Problem 4","text":"Consider experiment rolling two fair six-sided dice. Let random variable \\(Y\\) absolute difference two numbers appear upon rolling dice.domain/support \\(Y\\)?\\(S_Y=\\{0,1,2,3,4,5\\}\\).values expect \\(Y\\) take? think average \\(Y\\)? Don’t actually formal math, just think experiment.’d say \\(Y\\) take values 0,1 2 fairly often. ’d guess average around 1.5.Find probability mass function cumulative distribution function \\(Y\\).Using counting methods, know 36 possible values. can just count . number 0 occur numbers , happens six times. number 1 happens first die one larger second, 5 times, vice versa. Thus 1 happens 10 times. Continue process. Thus, pmf \\(Y\\) becomes:\\[\nf_Y(y)=\\left\\{ \\renewcommand{\\arraystretch}{1.4} \\begin{array}{ll} \\frac{6}{36}, & y=0 \\\\\n\\frac{10}{36}, & y=1 \\\\\n\\frac{8}{36}, & y=2 \\\\\n\\frac{6}{36}, & y=3 \\\\\n\\frac{4}{36}, & y=4 \\\\\n\\frac{2}{36}, & y=5 \\\\\n0, & \\mbox{otherwise} \\end{array} \\right . \n\\]\nalso create table count entries.\\[\n\\begin{array}{cc|cccccc} & & & &\\textbf{Die} & \\textbf{2}\n\\\\ & & 1 & 2 & 3 & 4 & 5 & 6  \n\\\\&\\hline 1 & 0 & 1 & 2 & 3 & 4 & 5 \n\\\\\\textbf{Die 1} & 2 & 1 & 0 & 1 & 2 &3 & 4  \n\\\\& 3 & 2 & 1 & 0 & 1 & 2 & 3 \n\\\\& 4 & 3 & 2 & 1 & 0 & 1 & 2\n\\\\& 5 & 4 & 3 & 2 & 1 & 0 & 1\n\\\\& 6 & 5 & 4 & 3 & 2 & 1 & 0\n\\end{array} \n\\]cdf \\(Y\\) thus,\n\\[\nF_Y(y)=\\left\\{\\renewcommand{\\arraystretch}{1.4}\n\\begin{array}{ll} 0, &  y < 0 \\\\\n\\frac{6}{36}, & 0\\leq y <1 \\\\\n\\frac{16}{36}, & 1\\leq y <2 \\\\\n\\frac{24}{36}, & 2 \\leq y <3 \\\\\n\\frac{30}{36}, & 3 \\leq y <4 \\\\\n\\frac{34}{36}, & 4 \\leq y <5 \\\\\n\\frac{36}{36}, & y\\geq 5 \\end{array} \\right .\n\\]Find expected value variance \\(Y\\).\n\\[\n\\mbox{E}(Y)=\\sum_{y=0}^5 y\\mbox{P}(Y=y) = 0\\times {6\\36} + 1 \\times {10\\36} + 2\\times {8\\36} + 3\\times {6\\36} + 4 \\times {4\\36} + 5 \\times {2\\36} =\n\\]\n\\[\n{70\\36} = 1.944\n\\]variance :Advanced: R, obtain 10,000 realizations \\(Y\\). words, simulate roll two fair dice, record absolute difference repeat 10,000 times. Construct frequency table results (percentage time get difference 0? difference 1? etc.) Find mean variance simulated sample \\(Y\\). close answers part d?got similar mean variance theoretical values.","code":"\ny<-c(0,1,2,3,4,5)\nmean_y<-sum(y*c(6,10,8,6,4,2)/36)\nmean_y## [1] 1.944444\nsum((y-mean_y)^2*(c(6,10,8,6,4,2)/36))## [1] 2.052469\nset.seed(9)\nsim_diffs<-do(10000)*abs(diff(sample(1:6,2,replace=T)))\ntally(~abs,data=sim_diffs,format=\"proportion\")## abs\n##      0      1      2      3      4      5 \n## 0.1643 0.2752 0.2273 0.1618 0.1116 0.0598\nmean(~abs,data=sim_diffs)## [1] 1.9606\nvar(sim_diffs)*9999/10000##          abs\n## abs 2.077248\ntrue_mean<-sum(c(6,10,8,6,4,2)/36*c(0,1,2,3,4,5))\ntrue_mean## [1] 1.944444\nsum(c(6,10,8,6,4,2)/36*(c(0,1,2,3,4,5)-true_mean)^2)## [1] 2.052469"},{"path":"RANDVAR.html","id":"problem-5-2","chapter":"10 Random Variables","heading":"10.2.5 Problem 5","text":"Prove Lemma Notes: Let \\(X\\) discrete random variable, let \\(\\) \\(b\\) constants. Show \\(\\mbox{E}(aX + b)=\\mbox{E}(X)+b\\).\\[\n\\mbox{E}(aX+b)=\\sum_x (ax+b)f_X(x) = \\sum_x axf_X(x)+\\sum_x bf_X(x) + \\sum_x xf_X(x)+b\\sum_x f_X(x)\n\\]Since \\(\\sum_x xf_X(x) = \\mbox{E}(X)\\) \\(\\sum_x f_X(x)=1\\), reduces \\(\\mbox{E}(X)+b\\).\\[\n\\mbox{Var}(aX+b)=\\mbox{E}\\left[(aX+b-\\mbox{E}(aX+b))^2\\right]=\\mbox{E}\\left[(aX+b-\\mbox{E}(X)-b)^2\\right]=\\mbox{E}\\left[(aX-\\mbox{E}(X)^2\\right]\n\\]\n\\[\n=\\mbox{E}\\left[^2(X-\\mbox{E}(X))^2\\right]=^2\\mbox{E}\\left[(X-\\mbox{E}(X))^2\\right]=^2\\mbox{Var}(X)\n\\]","code":""},{"path":"RANDVAR.html","id":"problem-6-1","chapter":"10 Random Variables","heading":"10.2.6 Problem 6","text":"Notes, saw \\(\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]\\). Show \\(\\mbox{Var}(X)\\) also equal \\(\\mbox{E}(X^2)-[\\mbox{E}(X)]^2\\).\n\\[\n\\mbox{Var}(X)=\\mbox{E}[(X-\\mu_X)^2]=\\mbox{E}[X^2-2\\mu_XX+\\mu_X^2] = \\mbox{E}(X^2)-\\mbox{E}(2\\mu_XX)+\\mbox{E}(\\mu_X^2)\n\\]quantity \\(\\mu_X\\) constant respect \\(X\\), \n\\[\n=\\mbox{E}(X^2)-2\\mu_X\\mbox{E}(X)+\\mu_X^2=\\mbox{E}(X^2)-2\\mu_X^2+\\mu_X^2 = \\mbox{E}(X^2)-\\mu_X^2\n\\]","code":""},{"path":"CONRANDVAR.html","id":"CONRANDVAR","chapter":"11 Continuous Random Variables","heading":"11 Continuous Random Variables","text":"","code":""},{"path":"CONRANDVAR.html","id":"objectives-10","chapter":"11 Continuous Random Variables","heading":"11.1 Objectives","text":"Define properly use new terms include probability density function (pdf) cumulative distribution function (cdf) continuous random variables.Given continuous random variable, find probabilities using pdf /cdf.Find mean variance continuous random variable.","code":""},{"path":"CONRANDVAR.html","id":"homework-10","chapter":"11 Continuous Random Variables","heading":"11.2 Homework","text":"","code":""},{"path":"CONRANDVAR.html","id":"problem-1-10","chapter":"11 Continuous Random Variables","heading":"11.2.1 Problem 1","text":"Let \\(X\\) continuous random variable domain \\(-k \\leq X \\leq k\\). Also, let \\(f(x)=\\frac{x^2}{18}\\).Assume \\(f(x)\\) valid pdf. Find value \\(k\\).\\(f\\) valid pdf, know \\(\\int_{-k}^k \\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = 1\\). ,\n\\[\n\\int_{-k}^k \\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = \\frac{x^3}{54}\\bigg|_{-k}^k = \\frac{k^3}{54}-\\frac{-k^3}{54}=\\frac{k^3}{27}=1\n\\]Thus, \\(k=3\\).Using R, see can follow code.Looks like \\(k \\approx 3\\) plot.Plot pdf \\(X\\).Find plot cdf \\(X\\).\n\\[\nF_X(x)=\\mbox{P}(X\\leq x)=\\int_{-3}^x \\frac{t^2}{18}\\mathop{}\\!\\mathrm{d}t = \\frac{t^3}{54}\\bigg|_{-3}^x = \\frac{x^3}{54}+\\frac{1}{2}\n\\]\\[\nF_X(x)=\\left\\{\\begin{array}{ll} 0, & x<-3 \\\\ \\frac{x^3}{54}+\\frac{1}{2}, & -3\\leq x \\leq 3 \\\\ 1, & x>3 \\end{array}\\right.\n\\]Find \\(\\mbox{P}(X<1)\\).\n\\[\n\\mbox{P}(X<1)=F(1)=\\frac{1}{54}+\\frac{1}{2}=0.519\n\\]Find \\(\\mbox{P}(1.5<X\\leq 2.5)\\).\n\\[\n\\mbox{P}(1.5< X \\leq 2.5)=F(2.5)-F(1.5)=\\frac{2.5^3}{54}+\\frac{1}{2}-\\frac{1.5^3}{54}-\\frac{1}{2}=0.227\n\\]Find 80th percentile \\(X\\) (value \\(x\\) 80% distribution left value).Need \\(x\\) \\(F(x)=0.8\\). Solving \\(\\frac{x^3}{54}+\\frac{1}{2}=0.8\\) \\(x\\) yields \\(x=2.530\\).Find value \\(x\\) \\(\\mbox{P}(-x \\leq X \\leq x)=0.4\\).distribution symmetric, finding \\(x\\) equivalent finding \\(x\\) \\(\\mbox{P}(X>x)=0.3\\). (helps draw picture). Thus, need \\(x\\) \\(F(x)=0.7\\). Solving \\(\\frac{x^3}{54}+\\frac{1}{2}=0.7\\) \\(x\\) yields \\(x=2.210\\).Find mean variance \\(X\\).\n\\[\n\\mbox{E}(X)=\\int_{-3}^3 x\\cdot\\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = \\frac{x^4}{72}\\bigg|_{-3}^3=\\frac{81}{72}-\\frac{81}{72} = 0\n\\]\\[\n\\mbox{E}(X^2)=\\int_{-3}^3 x^2\\cdot\\frac{x^2}{18}\\mathop{}\\!\\mathrm{d}x = \\frac{x^5}{90}\\bigg|_{-3}^3=\\frac{243}{90}-\\frac{-243}{90} = 5.4\n\\]\\[\n\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2=5.4-0^2=5.4\n\\]Simulate 10000 values distribution plot density.tricky since need cube root function. Just raising one-third power won’t work. Let’s write function.Notice smoothing operation goes past support \\(X\\) thus shows concave curve. clean limiting x-axis interval [-3,3].","code":"\nmy_pdf <- function(x)integrate(function(y)y^2/18,-x,x)$value\nmy_pdf<-Vectorize(my_pdf)\ndomain <- seq(.01,5,.1)\ngf_line(my_pdf(domain)~domain) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title=\"Cumulative probability for different values of k\",x=\"k\",y=\"Cummulative Probability\") %>%\n  gf_hline(yintercept = 1,color = \"blue\")\nuniroot(function(x)my_pdf(x)-1,c(-10,10))$root## [1] 2.999997\nx<-seq(-3,3,0.001)\nfx<-x^2/18\ngf_line(fx~x,ylab=\"f(x)\",title=\"pdf of X\") %>%\n  gf_theme(theme_classic())\nggplot(data.frame(x=c(-3, 3)), aes(x)) + \n stat_function(fun=function(x) x^2/18) +\n  theme_classic() +\n  labs(y=\"f(x)\",title=\"pdf of X\")\ncurve(x^2/18,from=-3,to=3,ylab=\"f(x)\",main=\"pdf of X\")\nx<-seq(-3.5,3.5,0.001)\nfx<-pmin(1,(1*(x>=-3)*(x^3/54+1/2)))\ngf_line(fx~x,ylab=\"F(x)\",title=\"cdf of X\") %>%\n  gf_theme(theme_classic())\nintegrate(function(x)x^2/18,-3,1)## 0.5185185 with absolute error < 5.8e-15\nintegrate(function(x)x^2/18,1.5,2.5)## 0.2268519 with absolute error < 2.5e-15\nuniroot(function(x)x^3/54+.5-.8,c(-3,3))## $root\n## [1] 2.530293\n## \n## $f.root\n## [1] -1.854422e-06\n## \n## $iter\n## [1] 6\n## \n## $init.it\n## [1] NA\n## \n## $estim.prec\n## [1] 6.103516e-05\ncuberoot <- function(x) {\n  sign(x) * abs(x)^(1/3)}\nset.seed(4)\nresults <- do(10000)*cuberoot((runif(1)-.5)*54)\nresults %>%\n  gf_dens(~cuberoot) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title=\"pdf from simulation\",x=\"x\",y=\"f(x)\") \ninspect(results)## \n## quantitative variables:  \n##          name   class       min        Q1     median       Q3      max\n## ...1 cuberoot numeric -2.999981 -2.382864 -0.1574198 2.376346 2.999347\n##              mean       sd     n missing\n## ...1 -0.002416475 2.322639 10000       0"},{"path":"CONRANDVAR.html","id":"problem-2-10","chapter":"11 Continuous Random Variables","heading":"11.2.2 Problem 2","text":"Let \\(X\\) continuous random variable. Prove cdf \\(X\\), \\(F_X(x)\\) non-decreasing function. (Hint: show \\(< b\\), \\(F_X() \\leq F_X(b)\\).)Let \\(<b\\), \\(\\) \\(b\\) domain \\(X\\). Note \\(F_X()=\\mbox{P}(X\\leq )\\) \\(F_X(b)=\\mbox{P}(X\\leq b)\\). Since \\(<b\\), can partition \\(\\mbox{P}(X\\leq b)\\) \\(\\mbox{P}(X\\leq )+\\mbox{P}(< X \\leq b)\\). One axioms probability probability must non-negative, know \\(\\mbox{P}(< X \\leq b)\\geq 0\\). Thus,\n\\[\n\\mbox{P}(X\\leq b)=\\mbox{P}(X\\leq )+\\mbox{P}(< X \\leq b) \\geq \\mbox{P}(X\\leq )\n\\], shown \\(F_X()\\leq F_X(b)\\). Thus, \\(F_X(x)\\) non-decreasing function.","code":""},{"path":"DISCRETENAMED.html","id":"DISCRETENAMED","chapter":"12 Named Discrete Distributions","heading":"12 Named Discrete Distributions","text":"","code":""},{"path":"DISCRETENAMED.html","id":"objectives-11","chapter":"12 Named Discrete Distributions","heading":"12.1 Objectives","text":"Recognize setup use common discrete distributions (Uniform, Binomial, Poisson, Hypergeometric) include parameters, assumptions, moments.Use R calculate probabilities quantiles involving random variables common discrete distributions.","code":""},{"path":"DISCRETENAMED.html","id":"homework-11","chapter":"12 Named Discrete Distributions","heading":"12.2 Homework","text":"problems , 1) define random variable help answer question, 2) state distribution parameters random variable; 3) determine expected value variance random variable, 4) use random variable answer question.demonstrate using 1a 1b.","code":""},{"path":"DISCRETENAMED.html","id":"problem-1-11","chapter":"12 Named Discrete Distributions","heading":"12.2.1 Problem 1","text":"T-6 training aircraft used UPT. Suppose training sortie, aircraft return maintenance-related failure rate 1 per 100 sorties.Find probability maintenance failures 15 sorties.\\(X\\): number maintenance failures 15 sorties.\\(X\\sim \\textsf{Bin}(n=15,p=0.01)\\)\\(\\mbox{E}(X)=15*0.01=0.15\\) \\(\\mbox{Var}(X)=15*0.01*0.99=0.1485\\).\\(\\mbox{P}(\\mbox{maintenance failures})=\\mbox{P}(X=0)={15\\choose 0}0.01^0(1-0.01)^{15}=0.99^{15}\\)probability makes sense, since expected value fairly low. , average, 0.15 failures occur every 15 trials, 0 failures common result. Graphically, pmf looks like :Find probability least two maintenance failures 15 sorties.can use \\(X\\) . Now, looking \\(\\mbox{P}(X\\geq 2)\\). equivalent finding \\(1-\\mbox{P}(X\\leq 1)\\):Find probability least 30 successful (mx failures) sorties first failure.\\(X\\): number maintenance failures 30 sorties.\\(X\\sim \\textsf{Binom}(n=30,p=0.01)\\), \\(\\mbox{E}(X)=0.3\\) \\(\\mbox{Var}(X)=0.297\\).\\(\\mbox{P}(\\mbox{0 failures})=\\mbox{P}(X=0)=0.99^{30}\\)Using negative binomial, reading can research:\\(Y\\): number successful sorties first failure.\\(Y\\sim \\textsf{NegBin}(n=1,p=0.01)\\), \\(\\mbox{E}(X)=99\\) \\(\\mbox{Var}(X)=9900\\).\\(\\mbox{P}(\\mbox{least 30 successes first failure})=\\mbox{P}(Y\\geq 30)\\)Find probability least 50 successful sorties third failure.Using binomial random variable, 52 trials need least 50 success. random variable \\(X\\) number successful sorties 52.using negative binomial, let\\(Y\\): number successful sorties third failure.\\(Y\\sim \\textsf{NegBin}(n=3,p=0.01)\\), \\(\\mbox{E}(X)=297\\) \\(\\mbox{Var}(X)=29700\\).\\(\\mbox{P}(\\mbox{least 50 successes 3rd failure})=\\mbox{P}(Y\\geq 50)\\)Notice question exactly 50 successful sorties 3 failure, different question. use either:\\(0.01\\) last trial failure.","code":"\n0.99^15## [1] 0.8600584\n## or \ndbinom(0,15,0.01)## [1] 0.8600584\ngf_dist(\"binom\",size=15,prob=0.01) %>%\n  gf_theme(theme_classic())\n## Directly\n1-(0.99^15 + 15*0.01*0.99^14)## [1] 0.009629773\n## or, using R\nsum(dbinom(2:15,15,0.01))## [1] 0.009629773\n## or\n1-sum(dbinom(0:1,15,0.01))## [1] 0.009629773\n## or\n1-pbinom(1,15,0.01)## [1] 0.009629773\n## or \npbinom(1,15,0.01,lower.tail = F)## [1] 0.009629773\n0.99^30## [1] 0.7397004\n##or \ndbinom(0,30,0.01)## [1] 0.7397004\n1-pnbinom(29,1,0.01)## [1] 0.7397004\n1-pbinom(49,52,.99)## [1] 0.9846474\n1-pnbinom(49,3,0.01)## [1] 0.9846474\ndbinom(50,52,.99)*.01## [1] 0.000802238\ndnbinom(50,3,0.01)## [1] 0.000802238"},{"path":"DISCRETENAMED.html","id":"problem-2-11","chapter":"12 Named Discrete Distributions","heading":"12.2.2 Problem 2","text":"given Saturday, suppose vehicles arrive USAFA North Gate according Poisson process rate 40 arrivals per hour.Find probability vehicles arrive 10 minutes.\\(X\\): number vehicles arrive 10 minutes\\(X\\sim \\textsf{Pois}(\\lambda=40/6=6.67)\\) \\(\\mbox{E}(X)=\\mbox{Var}(X)=6.67\\).\\(\\mbox{P}(\\mbox{arrivals 10 minutes})=\\mbox{P}(X=0)=\\frac{6.67^0 e^{-6.67}}{0!}=e^{-6.67}\\)Find probability least 50 vehicles arrive hour.\\(X\\): number vehicles arrive hour\\(X\\sim \\textsf{Pois}(\\lambda=40)\\) \\(\\mbox{E}(X)=\\mbox{Var}(X)=40\\).\\(\\mbox{P}(\\mbox{least 50 arrivals 1 hour})=\\mbox{P}(X\\geq 50)\\)Find probability least 5 minutes pass next arrival.\\(X\\): number vehicles arrive 5 minutes\\(X\\sim \\textsf{Pois}(\\lambda=40/12=3.33)\\) \\(\\mbox{E}(X)=\\mbox{Var}(X)=3.33\\).\\(\\mbox{P}(\\mbox{arrivals 5 minutes})=\\mbox{P}(X=0)=\\frac{3.33^0 e^{-3.33}}{0!}=e^{-3.33}\\)","code":"\nexp(-40/6)## [1] 0.001272634\n##or\ndpois(0,40/6)## [1] 0.001272634\n1-ppois(49,40)## [1] 0.07033507\nexp(-40/12)## [1] 0.03567399\n##or\ndpois(0,40/12)## [1] 0.03567399"},{"path":"DISCRETENAMED.html","id":"problem-3-8","chapter":"12 Named Discrete Distributions","heading":"12.2.3 Problem 3","text":"Suppose 12 male 7 female cadets classroom. select 5 completely random (without replacement).Find probability select female cadets.\\(X\\): number female cadets selected sample size 5\\(X\\sim \\textsf{Hypergeom}(m=7,n=12,k=5)\\) \\(\\mbox{E}(X)=1.842\\) \\(\\mbox{Var}(X)=0.905\\).\\[\n\\mbox{P}(\\mbox{female cadets selected})=\\mbox{P}(X=0)=\\frac{{7\\choose 0}{12\\choose 5}}{{19\\choose 5}}\n\\]Find probability select 2 female cadets.Using random variable:\n\\[\n\\mbox{P}(\\mbox{2 female})=\\mbox{P}(X>2)=1-\\mbox{P}(X\\leq 2)\n\\]","code":"\nchoose(12,5)/choose(19,5)## [1] 0.06811146\n##or\ndhyper(0,7,12,5)## [1] 0.06811146\n1-phyper(2,7,12,5)## [1] 0.2365841\n##or\nsum(dhyper(3:5,7,12,5))## [1] 0.2365841"},{"path":"CONTNNAMED.html","id":"CONTNNAMED","chapter":"13 Named Continuous Distributions","heading":"13 Named Continuous Distributions","text":"","code":""},{"path":"CONTNNAMED.html","id":"objectives-12","chapter":"13 Named Continuous Distributions","heading":"13.1 Objectives","text":"Recognize use common continuous distributions (Uniform, Exponential, Gamma, Normal, Weibull, Beta), identify parameters, find moments.Use R calculate probabilities quantiles involving random variables common continuous distributions.Understand relationship Poisson process Poisson & Exponential distributions.Know apply use memoryless property.","code":""},{"path":"CONTNNAMED.html","id":"homework-12","chapter":"13 Named Continuous Distributions","heading":"13.2 Homework","text":"problems 1-3 , 1) define random variable help answer question, 2) state distribution parameters random variable; 3) determine expected value variance random variable, 4) use random variable answer question.","code":""},{"path":"CONTNNAMED.html","id":"problem-1-12","chapter":"13 Named Continuous Distributions","heading":"13.2.1 Problem 1","text":"given Saturday, suppose vehicles arrive USAFA North Gate according Poisson process rate 40 arrivals per hour.Find probability vehicles arrive 10 minutes.\\(X\\): number vehicles arrive 10 minutes\\(X\\sim \\textsf{Pois}(\\lambda=40/6=6.67)\\) \\(\\mbox{E}(X)=\\mbox{Var}(X)=6.67\\).\\(\\mbox{P}(\\mbox{arrivals 10 minutes})=\\mbox{P}(X=0)=\\frac{6.67^0 e^{-6.67}}{0!}=e^{-6.67}\\), using exponential distribution:\\(Y\\): time minutes next arrival\\(Y\\sim \\textsf{Expon}(\\lambda=40/60=0.667)\\) \\(\\mbox{E}(Y)=1.5\\) \\(\\mbox{Var}(Y)=2.25\\).\\[\n\\mbox{P}(\\mbox{least 10 minutes next arrival})=\\mbox{P}(Y\\geq 10)=\\int_{10}^\\infty \\frac{2}{3}e^{-\\frac{2}{3}y}\\mathop{}\\!\\mathrm{d}y\n\\]using simulation:Find probability least 5 minutes pass next arrival.\\(Y\\): part \\[\n\\mbox{P}(\\mbox{least 5 minutes next arrival})=\\mbox{P}(Y\\geq 5)=\\int_{5}^\\infty \\frac{2}{3}e^{-\\frac{2}{3}y}\\mathop{}\\!\\mathrm{d}y\n\\]Find probability next vehicle arrive 2 10 minutes now.\\(Y\\) defined .Find probability least 7 minutes pass next arrival, given 2 minutes already passed. Compare answer part (b). example memoryless property exponential distribution.\n\\[\n\\mbox{P}(Y\\geq 7|Y\\geq 2) = \\frac{\\mbox{P}(Y\\geq 7, Y\\geq 2)}{\\mbox{P}(Y\\geq 2)} = \\frac{\\mbox{P}(Y\\geq 7)}{\\mbox{P}(Y\\geq 2)}\n\\]answer result memoryless property.Fill blank. probability 90% next vehicle arrive within __ minutes. value known 90% percentile random variable.Use function stripplot() visualize arrival 30 vehicles using random sample appropriate exponential distribution.","code":"\nexp(-40/6)## [1] 0.001272634\n##or\ndpois(0,40/6)## [1] 0.001272634\n1-pexp(10,2/3)## [1] 0.001272634\nset.seed(616)\nmean(rpois(100000,40/6) == 0)## [1] 0.00126\nmean(rexp(100000,2/3) >=10)## [1] 0.00127\n1-pexp(5,2/3)## [1] 0.03567399\npexp(10,2/3)-pexp(2,2/3)## [1] 0.2623245\n(1-pexp(7,2/3))/(1-pexp(2,2/3))## [1] 0.03567399\nqexp(0.9,2/3)## [1] 3.453878\nset.seed(202)\nstripplot(cumsum(rexp(30,2/3)),xlab=\"Arrival Time\")"},{"path":"CONTNNAMED.html","id":"problem-2-12","chapter":"13 Named Continuous Distributions","heading":"13.2.2 Problem 2","text":"Suppose time computer errors F-35 follows Gamma distribution mean 20 hours variance 10.Find probability 20 hours pass without computer error.\\(X\\): time hours next computer error.\\(X\\sim \\textsf{Gamma}(\\alpha = 40, \\lambda = 2)\\)need find \\(\\alpha\\) \\(\\lambda\\) given moments.\\(\\mbox{E}(X) = 20 = \\frac{\\alpha}{\\lambda}\\)\\(\\mbox{Var}(X) = 10 = \\frac{\\alpha}{\\lambda^2}\\)Notice \\(\\frac{\\mbox{E}(X)}{\\mbox{Var}(X)} = \\lambda = \\frac{20}{10}=2\\) using \\(\\mbox{E}(X) = 20 = \\frac{\\alpha}{\\lambda}\\) get \\(\\alpha = 40\\).\\(\\mbox{P}(X\\geq 20)\\):Find probability 45 hours pass without computer error, given 25 hours already passed. memoryless property apply Gamma distribution?\n\\[\nP(X\\geq 45|X\\geq 25) = \\frac{P(X\\geq 45, X\\geq 25)}{P(X\\geq 25)} = \\frac{P(X\\geq 45)}{P(X\\geq 25)}\n\\], memoryless property apply Gamma distribution.Find \\(\\) \\(b\\) 95% probability time next computer error \\(\\) \\(b\\). (Note: technically, many answers question, find \\(\\) \\(b\\) tail equal probability.)time interval \\([14,29,26.66]\\).Another answer \\([0,25,47]\\).","code":"\n1-pgamma(20,shape=40,rate=2)## [1] 0.4789711\n(1-pgamma(45,40,2))/(1-pgamma(25,40,2))## [1] 1.77803e-08\nqgamma(c(0.025,0.975),40,2)## [1] 14.28829 26.65714\nqgamma(.95,40,2)## [1] 25.46987"},{"path":"CONTNNAMED.html","id":"problem-3-9","chapter":"13 Named Continuous Distributions","heading":"13.2.3 Problem 3","text":"Suppose PFT scores cadet wing follow normal distribution mean 330 standard deviation 50.Find probability randomly selected cadet PFT score higher 450.\\(X\\): PFT score randomly selected cadet\\(X\\sim \\textsf{Norm}(\\mu=330,\\sigma=50)\\)\\(\\mbox{E}(X) = 330\\) \\(\\mbox{Var}(X)=50^2=2500\\).Find probability randomly selected cadet PFT score within 2 standard deviations mean.Need \\(\\mbox{P}(230 \\leq X \\leq 430)\\).Find \\(\\) \\(b\\) 90% PFT scores \\(\\) \\(b\\).Need \\(\\) \\(\\mbox{P}(X\\leq )=0.05\\) \\(b\\) \\(\\mbox{P}(X\\geq b)=0.05\\):Find probability randomly selected cadet PFT score higher 450 given /among top 10% cadets.Need \\(\\mbox{P}(X>450|X>x_{0.9})\\) \\(x_{0.9}\\) 90th percentile \\(X\\).90th percentile :\\[\n\\mbox{P}(X>450|X>x_{0.9})=\\frac{\\mbox{P}(X>450, X>x_{0.9})}{\\mbox{P}(X>x_{0.9})}=\\frac{\\mbox{P}(X>450, X>394.08)}{\\mbox{P}(X>x_{0.9})}=\\frac{\\mbox{P}(X>450)}{0.1}\n\\]assuming \\(x_{0.9}<450\\). Otherwise problem trivial probability 1.","code":"\n1-pnorm(450,330,50)## [1] 0.008197536\npnorm(430,330,50)-pnorm(230,330,50)## [1] 0.9544997\nqnorm(0.05,330,50)## [1] 247.7573\nqnorm(0.95,330,50)## [1] 412.2427\nqnorm(0.9,330,50)## [1] 394.0776\n(1-pnorm(450,330,50))/0.1## [1] 0.08197536"},{"path":"CONTNNAMED.html","id":"problem-4-4","chapter":"13 Named Continuous Distributions","heading":"13.2.4 Problem 4","text":"Let \\(X \\sim \\textsf{Beta}(\\alpha=1,\\beta=1)\\). Show \\(X\\sim \\textsf{Unif}(0,1)\\). Hint: write beta distribution pdf \\(\\alpha=1\\) \\(\\beta=1\\).beta pdf :\n\\[\nf_X(x)=\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}\n\\]\\(X\\sim\\textsf{Beta}(\\alpha=1,\\beta=1)\\), becomes:\n\\[\nf_X(x)=\\frac{\\Gamma(2)}{\\Gamma(1)\\Gamma(1)}x^{1-1}(1-x)^{1-1} = 1\n\\]","code":""},{"path":"CONTNNAMED.html","id":"problem-5-3","chapter":"13 Named Continuous Distributions","heading":"13.2.5 Problem 5","text":"using R calculate probabilities related gamma distribution, often use pgamma. Recall pgamma equivalent cdf gamma distribution. \\(X\\sim\\textsf{Gamma}(\\alpha,\\lambda)\\), \n\\[\n\\mbox{P}(X\\leq x)=\\textsf{pgamma(x,alpha,lambda)}\n\\]dgamma function exists R . plain language, explain dgamma returns. ’m looking definition found R documentation. ’m looking simple description function returns. output dgamma useful? , ?dgamma function returns value probability density function. probability, still useful quantity. can said larger densities (\\(f(x)\\)) imply values near \\(x\\) likely occur values associated smaller densities. also useful computing conditional probability distributions.","code":""},{"path":"CONTNNAMED.html","id":"problem-6-2","chapter":"13 Named Continuous Distributions","heading":"13.2.6 Problem 6","text":"Advanced. may heard 68-95-99.7 rule. helpful rule thumb says population normal distribution, 68% data within one standard deviation mean, 95% data within two standard deviations 99.7% data within three standard deviations. Create function R two inputs (mean standard deviation). return vector three elements: probability randomly selected observation normal distribution inputted mean standard deviation lies within one, two three standard deviations. Test function several values mu sd. get answer time.","code":"\nrulethumb<-function(mu,sd){\n  pnorm(mu+c(1,2,3)*sd,mu,sd)-pnorm(mu-c(1,2,3)*sd,mu,sd)\n}\nrulethumb(15,12)## [1] 0.6826895 0.9544997 0.9973002\nrulethumb(0,1)## [1] 0.6826895 0.9544997 0.9973002"},{"path":"CONTNNAMED.html","id":"problem-7-1","chapter":"13 Named Continuous Distributions","heading":"13.2.7 Problem 7","text":"Derive mean general uniform distribution, \\(U(,b)\\).definition\\[E(X)=\\int_{}^{b}xf(x)dx=\\]\n\\[ =\\int_{}^{b}\\frac{x}{b-}dx =\\]\\[ =\\frac{1}{b-}\\int_{}^{b}xdx = \\frac{1}{b-}\\cdot\\frac{x^2}{2}\\bigg|_{}^{b}=\\]\\[ =\\frac{1}{b-}\\cdot\\frac{b^2-^2}{2}= \\frac{1}{b-}\\cdot\\frac{(b-)(b+)}{2}=\\frac{(b+)}{2}\\]","code":""},{"path":"MULTIDISTS.html","id":"MULTIDISTS","chapter":"14 Multivariate Distributions","heading":"14 Multivariate Distributions","text":"","code":""},{"path":"MULTIDISTS.html","id":"objectives-13","chapter":"14 Multivariate Distributions","heading":"14.1 Objectives","text":"Define (distinguish ) terms joint probability mass/density function, marginal pmf/pdf, conditional pmf/pdf.Given joint pmf/pdf, obtain marginal conditional pmfs/pdfs.Use joint, marginal conditional pmfs/pdfs obtain probabilities.","code":""},{"path":"MULTIDISTS.html","id":"homework-13","chapter":"14 Multivariate Distributions","heading":"14.2 Homework","text":"","code":""},{"path":"MULTIDISTS.html","id":"problem-1-13","chapter":"14 Multivariate Distributions","heading":"14.2.1 Problem 1","text":"Let \\(X\\) \\(Y\\) continuous random variables joint pdf:\n\\[\nf_{X,Y}(x,y)=x + y\n\\]\\(0 \\leq x \\leq 1\\) \\(0 \\leq y \\leq 1\\).Verify \\(f\\) valid pdf.\n\\[\n\\int_0^1\\int_0^1 x+y\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = \\int_0^1 xy + \\frac{y^2}{2}\\bigg|_0^1 \\mathop{}\\!\\mathrm{d}x = \\int_0^1 x+\\frac{1}{2}\\mathop{}\\!\\mathrm{d}x = \\frac{x^2}{2}+\\frac{x}{2}\\bigg|_0^1=1\n\\]OrFind marginal pdfs \\(X\\) \\(Y\\).\n\\[\nf_X(x)=\\int_0^1 x+y\\mathop{}\\!\\mathrm{d}y = xy + \\frac{y^2}{2}\\bigg|_0^1 =  x+\\frac{1}{2}\n\\]\n\\(0\\leq x \\leq 1\\).Similarly, \\(f_Y(y)=y+\\frac{1}{2}\\) \\(0 \\leq y \\leq 1\\).Find conditional pdfs \\(X|Y=y\\) \\(Y|X=x\\).\n\\[\nf_{X|Y=y}(x)=\\frac{x+y}{y+\\frac{1}{2}}\n\\]\n\\(0\\leq x \\leq 1\\).Similarly, \\(f_{Y|X=x}(y)=\\frac{x+y}{x+\\frac{1}{2}}\\) \\(0\\leq y \\leq 1\\).Find following probabilities: \\(\\mbox{P}(X<0.5)\\); \\(\\mbox{P}(Y>0.8)\\); \\(\\mbox{P}(X<0.2,Y\\geq 0.75)\\); \\(\\mbox{P}(X<0.2|Y\\geq 0.75)\\); \\(\\mbox{P}(X<0.2|Y= 0.25)\\); Optional - \\(\\mbox{P}(X\\leq Y)\\).\\[\n\\mbox{P}(X<0.5)=\\int_0^{0.5} x+\\frac{1}{2}\\mathop{}\\!\\mathrm{d}x = \\frac{x^2}{2}+\\frac{x}{2}\\bigg|_0^{0.5}=0.375\n\\]using multivariate integration, integrate \\(y\\).\\[\n\\mbox{P}(Y<0.8)=\\int_{0.8}^1 y+\\frac{1}{2}\\mathop{}\\!\\mathrm{d}y = \\frac{y^2}{2}+\\frac{y}{2}\\bigg|_{0.8}^1=1-0.72=0.28\n\\]\\[\n\\mbox{P}(X<0.2,Y\\geq 0.75)=\\int_0^{0.2}\\int_{0.75}^1 x+y\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x= \\int_0^{0.2} xy+\\frac{y^2}{2}\\bigg|_{0.75}^1\\mathop{}\\!\\mathrm{d}x \n\\]\n\\[\n=\\int_0^{0.2} x+\\frac{1}{2}-\\frac{3x}{4}-\\frac{9}{32}\\mathop{}\\!\\mathrm{d}x = \\int_0^{0.2} \\frac{x}{4}+\\frac{7}{32}\\mathop{}\\!\\mathrm{d}x = \\frac{x^2}{8}+\\frac{7x}{32}\\bigg|_0^{0.2}=0.04875\n\\]\\[\n\\mbox{P}(X<0.2|Y\\geq 0.75)=\\frac{\\mbox{P}(X<0.2,Y\\geq 0.75)}{\\mbox{P}(Y\\geq 0.75)}=\\frac{0.04875}{\\int_{0.75}^1 y+\\frac{1}{2}\\mathop{}\\!\\mathrm{d}y}=\\frac{0.04875}{0.34375} \\approx 0.142\n\\]\n\\[\n\\mbox{P}(X<0.2|Y= 0.25) \n\\]\nneed\\[\nf_{X|Y=.25}(x)=\\frac{x+y}{y+\\frac{1}{2}}\\bigg|_{y=0.25}=\\frac{x+.25}{.25+\\frac{1}{2}}=\\frac{x+.25}{.75}=\\frac{4x+1}{3}\n\\]\\[\n\\mbox{P}(X<0.2|Y= 0.25) =  \\int_{0}^{0.2} \\frac{4x+1}{3} \\mathop{}\\!\\mathrm{d}x\n\\]\n\\[\n=\\frac{1}{3}\\left( 2x^2 +x \\right) \\bigg|_0^{0.2} = \\frac{1}{3}\\left( 2\\cdot0.2^2 +0.2 \\right) \\approx 0.0933\n\\]Optional\\[\n\\mbox{P}(X\\leq Y)=\\int_0^1\\int_0^y x+y \\mathop{}\\!\\mathrm{d}x \\mathop{}\\!\\mathrm{d}y = \\int_0^1 xy+\\frac{x^2}{2}\\bigg|_0^y \\mathop{}\\!\\mathrm{d}x = \\int_0^1 \\frac{3y^2}{2}\\mathop{}\\!\\mathrm{d}y = \\frac{y^3}{2}\\bigg|_0^1 = \\frac{1}{2}\n\\]","code":"\nlibrary(cubature) # load the package \"cubature\"\nf <- function(x) { (x[1] + x[2]) } # \"x\" is vector\nadaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1, 1))## $integral\n## [1] 1\n## \n## $error\n## [1] 0\n## \n## $functionEvaluations\n## [1] 17\n## \n## $returnCode\n## [1] 0\nintegrate(function(x)(x+1/2),0,1/2)## 0.375 with absolute error < 4.2e-15\nadaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1/2, 1))## $integral\n## [1] 0.375\n## \n## $error\n## [1] 5.551115e-17\n## \n## $functionEvaluations\n## [1] 17\n## \n## $returnCode\n## [1] 0\nadaptIntegrate(f, lowerLimit = c(0, 0.8), upperLimit = c(1, 1))## $integral\n## [1] 0.28\n## \n## $error\n## [1] 5.551115e-17\n## \n## $functionEvaluations\n## [1] 17\n## \n## $returnCode\n## [1] 0\nadaptIntegrate(f, lowerLimit = c(0, 0.75), upperLimit = c(0.2, 1))## $integral\n## [1] 0.04875\n## \n## $error\n## [1] 0\n## \n## $functionEvaluations\n## [1] 17\n## \n## $returnCode\n## [1] 0\nf2 <- function(x) { (4*x[1] + 1)/3 } # \"x\" is vector\nadaptIntegrate(f2, lowerLimit = c(0), upperLimit = c(0.2))## $integral\n## [1] 0.09333333\n## \n## $error\n## [1] 1.036208e-15\n## \n## $functionEvaluations\n## [1] 15\n## \n## $returnCode\n## [1] 0"},{"path":"MULTIDISTS.html","id":"problem-2-13","chapter":"14 Multivariate Distributions","heading":"14.2.2 Problem 2","text":"Notes, saw example \\(f_X(x)=f_{X|Y=y}(x)\\) \\(f_Y(y)=f_{Y|X=x}(y)\\). common important. imply \\(X\\) \\(Y\\)?Since conditional density function always equal marginal, means \\(X\\) \\(Y\\) independent one another. Also, conditioning variable appear conditional density function domain joint density rectangular, bounds two variables constants, random variables independent. variables previous problem dependent, look conditional density functions see conditional density depends conditioned variable.","code":""},{"path":"MULTIDISTS.html","id":"problem-3-10","chapter":"14 Multivariate Distributions","heading":"14.2.3 Problem 3","text":"ADVANCED: Recall earlier assignment, came random variables describe timeliness airport. Suppose course 210 days, day recorded number customer complaints regarding timeliness. Also day, recorded weather (airport located somewhere without snow without substantial wind). data displayed .\\[\n\\begin{array}{cc|ccc} & & &\\textbf{Weather Status} &\n\\\\ & & \\mbox{Clear} & \\mbox{Light Rain} & \\mbox{Rain}  \\\\\n&\\hline 0 & 28 & 11 & 4  \\\\\n\\textbf{num complaints} & 1 & 18 & 15 & 8 \\\\\n& 2 & 17 & 25 & 12  \\\\\n& 3 & 13 & 15 & 16  \\\\\n& 4 & 8 & 8 & 10 \\\\\n& 5 & 0 & 1 & 1 \\\\\n\\end{array} \n\\]First, define two random variables scenario. One (# complaints) essentially already random variable. (weather status) need assign number status.Use table build empirical joint pmf two random variables.simply label weather random variable 0, 1, 2. convert probabilities dividing 210.\\[\n\\begin{array}{cc|ccc} & & &\\textbf{Weather Status} &\n\\\\ & & \\mbox{Clear} & \\mbox{Light Rain} & \\mbox{Rain}  \\\\\n&\\hline 0 & 0.133 & 0.052 & 0.019  \\\\\n\\textbf{num complaints} & 1 & 0.086 & 0.071 & 0.038  \\\\\n& 2 & 0.081 & 0.119 & 0.057  \\\\\n& 3 & 0.062 & 0.071 & 0.076  \\\\\n& 4 & 0.038 & 0.038 & 0.048 \\\\\n& 5 & 0 & 0.005 & 0.005 \\\\\n\\end{array} \n\\]Find marginal pmfs random variable.\n\\[\nf_X(x)=\\left\\{\\begin{array}{ll} 0.400, & x=0 \\\\\n0.357, & x=1 \\\\\n0.243, & x=2 \\\\\n0, & \\mbox{otherwise} \n\\end{array}\\right.\n\\]\n\\[\nf_Y(y)=\\left\\{\\begin{array}{ll} 0.205, & y=0 \\\\\n0.195, & y=1 \\\\\n0.257, & y=2 \\\\\n0.210, & y=3 \\\\\n0.124, & y=4 \\\\\n0.010, & y=5 \\\\\n0, & \\mbox{otherwise} \n\\end{array}\\right.\n\\]Find marginal pmfs random variable.\n\\[\nf_X(x)=\\left\\{\\begin{array}{ll} 0.400, & x=0 \\\\\n0.357, & x=1 \\\\\n0.243, & x=2 \\\\\n0, & \\mbox{otherwise} \n\\end{array}\\right.\n\\]\n\\[\nf_Y(y)=\\left\\{\\begin{array}{ll} 0.205, & y=0 \\\\\n0.195, & y=1 \\\\\n0.257, & y=2 \\\\\n0.210, & y=3 \\\\\n0.124, & y=4 \\\\\n0.010, & y=5 \\\\\n0, & \\mbox{otherwise} \n\\end{array}\\right.\n\\]Find probability fewer 3 complaints.Find probability fewer 3 complaints.\\[\n\\mbox{P}(Y<3)=0.205+0.195+0.257=0.657\n\\]Find probability fewer 3 complaints given rain.\n\\[\n\\mbox{P}(Y<3|X=0)=\\frac{0.133+0.086+0.081}{0.4}=0.75\n\\]","code":""},{"path":"MULTIDISTS.html","id":"problem-4-5","chapter":"14 Multivariate Distributions","heading":"14.2.4 Problem 4","text":"Optional like Calc III want challenge.Let \\(X\\) \\(Y\\) continuous random variables joint pmf:\n\\[\nf_{X,Y}(x,y)=1\n\\]\\(0 \\leq x \\leq 1\\) \\(0 \\leq y \\leq 2x\\).Verify \\(f\\) valid pdf.\n\\[\n\\int_0^1 \\int_0^{2x} 1 \\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = \\int_0^1 y\\bigg|_0^{2x}\\mathop{}\\!\\mathrm{d}x = \\int_0^1 2x\\mathop{}\\!\\mathrm{d}x = x^2\\bigg|_0^1 = 1\n\\]Verify \\(f\\) valid pdf.\n\\[\n\\int_0^1 \\int_0^{2x} 1 \\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = \\int_0^1 y\\bigg|_0^{2x}\\mathop{}\\!\\mathrm{d}x = \\int_0^1 2x\\mathop{}\\!\\mathrm{d}x = x^2\\bigg|_0^1 = 1\n\\]Find marginal pdfs \\(X\\) \\(Y\\).\n\\[\nf_X(x)=\\int_0^{2x} 1 \\mathop{}\\!\\mathrm{d}y = y\\bigg|_0^{2x}=2x\n\\]\n\\(0\\leq x \\leq 1\\).Find marginal pdfs \\(X\\) \\(Y\\).\n\\[\nf_X(x)=\\int_0^{2x} 1 \\mathop{}\\!\\mathrm{d}y = y\\bigg|_0^{2x}=2x\n\\]\n\\(0\\leq x \\leq 1\\).\\[\nf_Y(y)=\\int_{y/2}^1 1 \\mathop{}\\!\\mathrm{d}x = x\\bigg|_{y/2}^1 = 1-\\frac{y}{2}\n\\]\n\\(0 \\leq y \\leq 2\\).Find conditional pdfs \\(X|Y=y\\) \\(Y|X=x\\).\n\\[\nf_{X|Y=y}(x)=\\frac{1}{1-\\frac{y}{2}}=\\frac{2}{2-y} \n\\]\n\\(y/2 \\leq x \\leq 1\\).\\[\nf_{Y|X=x}(y)=\\frac{1}{2x}\n\\]\n\\(0\\leq y \\leq 2x\\).Find following probabilities: \\(\\mbox{P}(X<0.5)\\); \\(\\mbox{P}(Y>1)\\); \\(\\mbox{P}(X<0.5,Y\\leq 0.8)\\); \\(\\mbox{P}(X<0.5|Y= 0.8)\\); Optional \\(\\mbox{P}(Y\\leq 1-X)\\). (probably help draw pictures.)\n\\[\n\\mbox{P}(X<0.5)=\\int_0^{0.5} 2x \\mathop{}\\!\\mathrm{d}x = x^2\\bigg|_0^{0.5}=0.25\n\\]\n\\[\n\\mbox{P}(Y>1)=\\int_1^2 1-\\frac{y}{2}\\mathop{}\\!\\mathrm{d}y = y-\\frac{y^2}{4}\\bigg|_1^2 = 1-\\frac{3}{4}=0.25\n\\]\n\\[\n\\mbox{P}(X<0.5,Y\\leq 0.8)=\\int_0^{0.4}\\int_0^{2x} 1 \\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x + \\int_{0.4}^{0.5}\\int_0^{0.8} 1 \\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = 0.16+0.08=0.24\n\\]\n\\[\n\\mbox{P}(X<0.5|Y= 0.8)=\\int_{0.4}^{0.5} \\frac{2}{2-0.8}\\mathop{}\\!\\mathrm{d}x = \\frac{5x}{3}\\bigg|_{0.4}^{0.5}=0.1667\n\\]\n\\[\n\\mbox{P}(Y\\leq 1-X)=\\int_0^{1/3}\\int_0^{2x} 1 \\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x + \\int_{1/3}^1\\int_0^{1-x}1 \\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = \\int_0^{1/3}2x\\mathop{}\\!\\mathrm{d}x + \\int_{1/3}^1 1-x\\mathop{}\\!\\mathrm{d}x\n\\]\n\\[\n=\\frac{1}{9}+\\left(x-\\frac{x^2}{2}\\right)_{1/3}^1=\\frac{1}{9}+\\frac{1}{2}-\\frac{1}{3}+\\frac{1}{18} = \\frac{1}{3}\n\\]","code":""},{"path":"MULTIEXP.html","id":"MULTIEXP","chapter":"15 Multivariate Expectation","heading":"15 Multivariate Expectation","text":"","code":""},{"path":"MULTIEXP.html","id":"objectives-14","chapter":"15 Multivariate Expectation","heading":"15.1 Objectives","text":"Given joint pmf/pdf, obtain means variances random variables functions random variables.Define terms covariance correlation, given joint pmf/pdf, obtain covariance correlation two random variables.Given joint pmf/pdf, determine whether random variables independent one another.Find conditional expectations.","code":""},{"path":"MULTIEXP.html","id":"homework-14","chapter":"15 Multivariate Expectation","heading":"15.2 Homework","text":"","code":""},{"path":"MULTIEXP.html","id":"problem-1-14","chapter":"15 Multivariate Expectation","heading":"15.2.1 Problem 1","text":"Let \\(X\\) \\(Y\\) continuous random variables joint pdf:\n\\[\nf_{X,Y}(x,y)=x + y\n\\]\\(0 \\leq x \\leq 1\\) \\(0 \\leq y \\leq 1\\).Find \\(\\mbox{E}(X)\\) \\(\\mbox{E}(Y)\\). use marginal pdfs found Application 14 solution.\\[\n\\mbox{E}(X)=\\int_0^1 x\\left(x+\\frac{1}{2}\\right)\\mathop{}\\!\\mathrm{d}x=\\frac{x^3}{3}+\\frac{x^2}{4}\\bigg|_0^1=\\frac{1}{3}+\\frac{1}{4}=\\frac{7}{12}=0.583\n\\]\nnumerically:\\[\n\\mbox{E}(Y)=\\int_0^1 y\\left(y+\\frac{1}{2}\\right)\\mathop{}\\!\\mathrm{d}y = 0.583\n\\]Find \\(\\mbox{Var}(X)\\) \\(\\mbox{Var}(Y)\\).\\[\n\\mbox{Var}(X)=\\mbox{E}(X^2)-\\mbox{E}(X)^2\n\\]\n\\[\n\\mbox{E}(X^2)=\\int_0^1 x^2\\left(x+\\frac{1}{2}\\right)\\mathop{}\\!\\mathrm{d}x = \\frac{x^4}{4}+\\frac{x^3}{6}\\bigg|_0^1=\\frac{1}{ 4}+\\frac{1}{6}=\\frac{5}{12}=0.417\n\\]\ncheck:, \\(\\mbox{Var}(X)=0.417-0.583^2=0.076\\).Similarly, \\(\\mbox{Var}(Y)=0.076\\).Find \\(\\mbox{Cov}(X,Y)\\) \\(\\rho\\). \\(X\\) \\(Y\\) independent?\n\\[\n\\mbox{Cov}(X,Y)=\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)\n\\]\n\\[\n\\mbox{E}(XY)=\\int_0^1\\int_0^1 xy(x+y)\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = \\int_0^1 \\frac{x^2y^2}{2}+\\frac{xy^3}{3}\\bigg|_0^1 \\mathop{}\\!\\mathrm{d}x = \\int_0^1 \\frac{x^2}{2}+\\frac{x}{3}\\mathop{}\\!\\mathrm{d}x\n\\]\n\\[\n=\\frac{x^3}{6}+\\frac{x^2}{6}\\bigg|_0^1=\\frac{1}{ 3}=0.333\n\\]check:,\n\\[\n\\mbox{Cov}(X,Y)=\\frac{1}{3}-\\left(\\frac{7}{12}\\right)^2=-0.007\n\\]\\[\n\\rho=\\frac{\\mbox{Cov}(X,Y)}{\\sqrt{\\mbox{Var}(X)\\mbox{Var}(Y)}}=\\frac{-0.007}{\\sqrt{0.076\\times0.076}}=-0.0909\n\\]check:Using exact values:non-zero covariance, \\(X\\) \\(Y\\) independent.Find \\(\\mbox{Var}(3X+2Y)\\).\n\\[\n\\mbox{Var}(3X+2Y)=\\mbox{Var}(3X)+\\mbox{Var}(2Y)+2\\mbox{Cov}(3X,2Y)=\n\\]\n\\[\n9\\mbox{Var}(X)+4\\mbox{Var}(Y)+12\\mbox{Cov}(X,Y) =\n\\]\n\\[\n9*0.076+4*0.076+12*-0.007 = 0.910\n\\]","code":"\nf <- function(x) { x[1]*(x[1] + x[2]) } # \"x\" is vector\nadaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1, 1))## $integral\n## [1] 0.5833333\n## \n## $error\n## [1] 1.110223e-16\n## \n## $functionEvaluations\n## [1] 17\n## \n## $returnCode\n## [1] 0\nf <- function(x) { x[1]^2*(x[1] + x[2]) } # \"x\" is vector\nround(adaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1, 1))$integral,3)## [1] 0.417\nf <- function(x) { x[1]*x[2]*(x[1] + x[2]) } # \"x\" is vector\nround(adaptIntegrate(f, lowerLimit = c(0, 0), upperLimit = c(1, 1))$integral,3)## [1] 0.333\n-0.007/sqrt(.076^2)## [1] -0.09210526\n(1/3-(7/12)^2)/sqrt((5/12-(7/12)^2)^2)## [1] -0.09090909"},{"path":"MULTIEXP.html","id":"problem-2-14","chapter":"15 Multivariate Expectation","heading":"15.2.2 Problem 2","text":"Optional - difficult small Calc III idea. Let \\(X\\) \\(Y\\) continuous random variables joint pmf:\n\\[\nf_{X,Y}(x,y)=1\n\\]\\(0 \\leq x \\leq 1\\) \\(0 \\leq y \\leq 2x\\).Find \\(\\mbox{E}(X)\\) \\(\\mbox{E}(Y)\\).\n\\[\n\\mbox{E}(X)=\\int_0^1 x\\cdot 2x\\mathop{}\\!\\mathrm{d}x = \\frac{2x^3}{3}\\bigg|_0^1=0.667\n\\]\\[\n\\mbox{E}(Y)=\\int_0^2 y\\left(1-\\frac{y}{2}\\right)\\mathop{}\\!\\mathrm{d}y = \\frac{y^2}{2}-\\frac{y^3}{6}\\bigg|_0^2=2-\\frac{8}{ 6}=0.667\n\\]Find \\(\\mbox{Var}(X)\\) \\(\\mbox{Var}(Y)\\).\n\\[\n\\mbox{E}(X^2)=\\int_0^1 x^2\\cdot 2x\\mathop{}\\!\\mathrm{d}x = \\frac{x^4}{2}\\bigg|_0^1=0.5\n\\], \\(\\mbox{Var}(X)=0.5-\\left(\\frac{2}{3}\\right)^2=\\frac{1}{ 18}=0.056\\)\\[\n\\mbox{E}(Y^2)=\\int_0^2 y^2\\left(1-\\frac{y}{2}\\right)\\mathop{}\\!\\mathrm{d}y = \\frac{y^3}{3}-\\frac{y^4}{8}\\bigg|_0^2=\\frac{8}{ 3}-2=0.667\n\\], \\(\\mbox{Var}(Y)=\\frac{2}{ 3}-\\left(\\frac{2}{3}\\right)^2=\\frac{2}{9}=0.222\\)Find \\(\\mbox{Cov}(X,Y)\\) \\(\\rho\\). \\(X\\) \\(Y\\) independent?\\[\n\\mbox{E}(XY)=\\int_0^1\\int_0^{2x} xy\\mathop{}\\!\\mathrm{d}y \\mathop{}\\!\\mathrm{d}x = \\int_0^1 \\frac{xy^2}{2}\\bigg|_0^{2x}\\mathop{}\\!\\mathrm{d}x = \\int_0^1 2x^3\\mathop{}\\!\\mathrm{d}x = \\frac{x^4}{2}\\bigg|_0^1=\\frac{1}{2}\n\\],\n\\[\n\\mbox{Cov}(X,Y)=\\frac{1}{2}-\\frac{2}{3}\\frac{2}{3}=\\frac{1}{18}=0.056\n\\]\\[\n\\rho=\\frac{\\mbox{Cov}(X,Y)}{ \\sqrt{\\mbox{Var}(X)\\mbox{Var}(Y)}}=\\frac{\\frac{1}{ 18}}{\\sqrt{\\frac{1}{18}\\frac{2}{9}}}=0.5\n\\]\\(X\\) \\(Y\\) appear positively correlated (thus independent).Find \\(\\mbox{Var}\\left(\\frac{X}{2}+2Y\\right)\\).\n\\[\n\\mbox{Var}\\left(\\frac{X}{2}+2Y\\right) = \\frac{1}{ 4}\\mbox{Var}(X)+4\\mbox{Var}(Y)+2\\mbox{Cov}(X,Y)=\\frac{1}{72}+\\frac{8}{ 9}+\\frac{1}{9}=1.014\n\\]","code":""},{"path":"MULTIEXP.html","id":"problem-3-11","chapter":"15 Multivariate Expectation","heading":"15.2.3 Problem 3","text":"Suppose \\(X\\) \\(Y\\) independent random variables. Show \\(\\mbox{E}(XY)=\\mbox{E}(X)\\mbox{E}(Y)\\).\\(X\\) \\(Y\\) independent, \\(\\mbox{Cov}(X,Y)=0\\). ,\n\\[\n\\mbox{Cov}(X,Y)=\\mbox{E}(XY)-\\mbox{E}(X)\\mbox{E}(Y)=0\n\\]Thus,\n\\[\n\\mbox{E}(XY)=\\mbox{E}(X)\\mbox{E}(Y)\n\\]","code":""},{"path":"MULTIEXP.html","id":"problem-4-6","chapter":"15 Multivariate Expectation","heading":"15.2.4 Problem 4","text":"playing game friend. roll fair sided die record result.Write joint probability mass function.Let \\(X\\) number die \\(Y\\) number friend’s die.\\[\n\\begin{array}{cc|ccc} & & &\\textbf{X} & \\\\ \n& & 1 & 2 & 3 & 4 & 5 & 6 \\\\\n&\\hline  1 & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} \\\\\n & 2 & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} \\\\\n\\textbf{Y}& 3 & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} \\\\\n& 4 & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} \\\\\n& 5 & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} \\\\\n& 6 & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36} & \\frac{1}{36}  \\\\\n\\end{array} \n\\]Find expected value product score friend’s score.find \\(E[XY]\\), determine 36 values product \\(X\\) \\(Y\\) multiply associated probabilities. Since probabilities equal, take \\(\\frac{1}{36}\\) summation. Now\\[E[XY]=\\frac{1}{36}(1+2+3+4+5+6+2+4+\\]\n\\[6+8+10+12+3+6+9+12+15+18+4+8+12+16+20+24+\\]\n\\[5+10+15+20+25+30+6+12+18+24+30+36)\\]\n\\[=12.25\\]Verify previous part using simulation.Using simulation, find expected value maximum number two roles.","code":"\nset.seed(1012)\n(do(100000)*(sample(1:6,size=2,replace=TRUE))) %>%\n  mutate(prod=V1*V2) %>%\n  summarize(Expec=mean(prod))##      Expec\n## 1 12.25016\n(do(100000)*max(sample(1:6,size=2,replace=TRUE))) %>%\n    summarize(Expec=mean(max))##    Expec\n## 1 4.4737"},{"path":"MULTIEXP.html","id":"problem-5-4","chapter":"15 Multivariate Expectation","heading":"15.2.5 Problem 5","text":"miner trapped mine containing three doors. first door leads tunnel takes safety two hours travel. second door leads tunnel returns mine three hours travel. third door leads tunnel returns mine five hours. Assuming miner times equally likely choose one doors, yes bad assumption makes nice problem, expected length time miner reaches safety?Simulating little challenging need conditional try first going mathematical solution.Let’s write function takes vector returns sum values first time number 2 appears, using time values sample space. Anytime repeating something 5 times, might make sense write function.Now let’s find mathematically.Let \\(X\\) time takes \\(Y\\) door. \\[E[X] = E[E[X|Y]] \\]\n\\[ = \\frac{1}{3}E[X|Y=1]+\\frac{1}{3}E[X|Y=2]+\\frac{1}{3}E[X|Y=3]\\]\nNow door 2 selected\n\\[E[X|Y=2]=E[X]+3\\]\nsince miner travel 3 hours back starting point.Likewise door 3 select\n\\[E[X|Y=2]=E[X]+5\\]\n\n\\[ E[x]= \\frac{1}{3}2+\\frac{1}{3}\\left( E[X]+3 \\right)+\\frac{1}{3}\\left( E[X]+5 \\right)\\]\\[E[x] -  \\frac{2}{3}E[X] = \\frac{2}{3}+\\frac{3}{3}+\\frac{5}{3}\\]\n\\[\\frac{1}{3}E[X]=\\frac{10}{3}\\]\\[E[X]=10\\]","code":"\nminer_time <- function(x){\n  index <- which(x==2)[1]\n  total<-cumsum(x)\n  return(total[index])\n}\nset.seed(113)\n(do(10000)*miner_time(sample(c(2,3,5),size=20,replace=TRUE))) %>% \n  summarise(Exp=mean(miner_time))##       Exp\n## 1 10.0092"},{"path":"MULTIEXP.html","id":"problem-6-3","chapter":"15 Multivariate Expectation","heading":"15.2.6 Problem 6","text":"ADVANCED: Let \\(X_1,X_2,...,X_n\\) independent, identically distributed random variables. (often abbreviated “iid”). \\(X_i\\) mean \\(\\mu\\) variance \\(\\sigma^2\\) (.e., \\(\\), \\(\\mbox{E}(X_i)=\\mu\\) \\(\\mbox{Var}(X_i)=\\sigma^2\\)).Let \\(S=X_1+X_2+...+X_n=\\sum_{=1}^n X_i\\). let \\(\\bar{X}={\\sum_{=1}^n \\frac{X_i}{n}}\\).Find \\(\\mbox{E}(S)\\), \\(\\mbox{Var}(S)\\), \\(\\mbox{E}(\\bar{X})\\) \\(\\mbox{Var}(\\bar{X})\\).\n\\[\n\\mbox{E}(S)=\\mbox{E}(X_1+X_2+...+X_n)=\\mbox{E}(X_1)+\\mbox{E}(X_2)+...+\\mbox{E}(X_n)=\\mu+\\mu+...+\\mu=n\\mu\n\\]Since \\(X_i\\)s independent:\n\\[\n\\mbox{Var}(S)=\\mbox{Var}(X_1+X_2+...+X_n)=\\mbox{Var}(X_1)+\\mbox{Var}(X_2)+...+\\mbox{Var}(X_n)=n\\sigma^2\n\\]\\[\n\\mbox{E}(\\bar{X})=\\frac{1}{n}\\mbox{E}(X_1+X_2+...+X_n)=\\frac{1}{n}n\\mu=\\mu\n\\]\n\\[\n\\mbox{Var}(\\bar{X})=\\frac{1}{n^2}\\mbox{Var}(X_1+X_2+...+X_n)=\\frac{1}{n^2}n\\sigma^2=\\frac{\\sigma^2}{n}\n\\]","code":""},{"path":"CS3.html","id":"CS3","chapter":"16 Case Study","heading":"16 Case Study","text":"","code":""},{"path":"CS3.html","id":"objectives-15","chapter":"16 Case Study","heading":"16.1 Objectives","text":"Define use properly context new terminology.Conduct hypothesis test using permutation test include 4 steps.","code":""},{"path":"CS3.html","id":"homework-15","chapter":"16 Case Study","heading":"16.2 Homework","text":"","code":""},{"path":"CS3.html","id":"problem-1-15","chapter":"16 Case Study","heading":"16.2.1 Problem 1","text":"Side effects AvandiaRosiglitazone active ingredient controversial type~2 diabetes medicine Avandia linked increased risk serious cardiovascular problems stroke, heart failure, death. common alternative treatment pioglitazone, active ingredient diabetes medicine called Actos. nationwide retrospective observational study 227,571 Medicare beneficiaries aged 65 years older, found 2,593 67,593 patients using rosiglitazone 5,386 159,978 using pioglitazone serious cardiovascular problems. data summarized contingency table .\\[\n\\begin{array}{ccc|cc|c} & & &\\textbf{Cardiovascular} & \\textbf{problems} &\n\\\\& &       & Yes   &        & Total \\\\\n&\\hline \\textbf{Treatment}      & \\textit{Rosiglitazone}    & 2,593 & 65,000        & 67,593    \\\\\n& & \\textit{Pioglitazone}       & 5,386     & 154,592   & 159,978\\\\\n&\\hline &Total          & 7,979 & 219,592       & 227,571\n\\end{array} \n\\]Determine following statements true false. false, explain .  reasoning may wrong even statement’s conclusion correct. cases, statement considered false.Since patients pioglitazone cardiovascular problems (5,386 vs. 2,593), can conclude rate cardiovascular problems pioglitazone treatment higher.False. Instead comparing counts, compare percentages.data suggest diabetic patients taking rosiglitazone likely cardiovascular problems since rate incidence (2,593 / 67,593 = 0.038) 3.8% patients treatment, (5,386 / 159,978 = 0.034) 3.4% patients pioglitazone.True.fact rate incidence higher rosiglitazone group proves rosiglitazone causes serious cardiovascular problems.False. infer causal relationship association observational study. However, can say drug person affects risk case, chose drug choice may associated variables, part (b) true. difference statements subtle important.Based information provided far, tell difference rates incidences due relationship two variables due chance.True.","code":""},{"path":"CS3.html","id":"problem-2-15","chapter":"16 Case Study","heading":"16.2.2 Problem 2","text":"Heart transplantsThe Stanford University Heart Transplant Study conducted determine whether experimental heart transplant program increased lifespan. patient entering program designated official heart transplant candidate, meaning gravely ill likely benefit new heart. patients got transplant . variable  indicates group patients ; patients treatment group got transplant control group . Another variable called  used indicate whether patient alive end study.study, 34 patients control group, 4 alive end study. 69 patients treatment group, 24 alive. contingency table summarizes results.\\[\n\\begin{array}{ccc|cc|c} & & &\\textbf{Group} &  &\n\\\\& &       & Control   & Treatment         & Total \\\\\n&\\hline \\textbf{Outcome}        & \\textit{Alive}    & 4     & 24            & 28    \\\\\n& & \\textit{Dead}       & 30        & 45            & 75\\\\\n&\\hline &Total              & 34        & 69            & 103\n\\end{array} \n\\]data file called Stanford_heart_study.csv. Read data answer following questions.proportion patients treatment group proportion patients control group died?88.2% patients control group died 65.2% treatment group.One approach investigating whether treatment effective use randomization technique.claims tested? Use null alternative hypothesis notation used lesson notes.\\(H_0\\): Independence model. variables group outcome independent. relationship, difference survival rates control treatment groups due chance. words, heart transplant effective.\\(H_A\\): Alternative hypothesis. variables group outcome independent. difference survival rates control treatment groups due\nchance heart transplant effective.paragraph describes set approach, without using statistical software. Fill blanks number phrase, whichever appropriate.write alive 28 cards representing patients alive end study, dead 75 cards representing patients . , shuffle cards split two groups: one group size 69 representing treatment, another group size 34 representing control. calculate difference proportion dead cards control treatment groups (control - treatment), just positive observed value, record value. repeat many times build distribution centered zero. Lastly, calculate fraction simulations simulated differences proportions 0.23 greater. fraction simulations, empirical p-value, low, conclude unlikely observed outcome chance null hypothesis rejected favor alternative.Next perform simulation use results decide effectiveness transplant program.Find observed value test statistic, decided use difference proportions.Simulate 1000 values test statistic using shuffle() variable group.Plot distribution results. Include vertical line observed value. Clean plot presenting decision maker.Find p-value. Think carefully extreme mean.wanted use hypergeometric. use cell table. use upper left. extreme 4 fewer control group alive. get similar p-value.Decide treatment effective.independence model, 11 1000 times (1.1%) get difference 0.23 higher proportions patients died control treatment groups. Since low probability, can reject claim independence favor alternate model. convincing evidence suggest transplant program effective.","code":"\nheart<-read_csv(\"data/Stanford_heart_study.csv\")\ninspect(heart)## \n## categorical variables:  \n##      name     class levels   n missing\n## 1 outcome character      2 103       0\n## 2   group character      2 103       0\n##                                    distribution\n## 1 Dead (72.8%), Alive (27.2%)                  \n## 2 Treatment (67%), Control (33%)\ntally(~outcome+group,data=heart,margins = TRUE)##        group\n## outcome Control Treatment Total\n##   Alive       4        24    28\n##   Dead       30        45    75\n##   Total      34        69   103\ntally(outcome~group,data=heart,margins = TRUE,format=\"percent\")##        group\n## outcome   Control Treatment\n##   Alive  11.76471  34.78261\n##   Dead   88.23529  65.21739\n##   Total 100.00000 100.00000\nobs<-diffprop(outcome~group,data=heart)\nobs## diffprop \n## 0.230179\nset.seed(1213)\nresults <- do(1000)*diffprop(outcome~shuffle(group),data=heart)\nresults %>%\n  gf_histogram(~diffprop,xlab=\"Difference in proportions\",\n               ylab=\"Count\",\n               fill=\"cyan\",\n               color=\"black\",\n               title=\"Stanford Heart Study\",\n               subtitle=\"Distribution of difference between control and treatment groups\") %>%\n  gf_vline(xintercept =obs ) %>%\n  gf_theme(theme_classic())\nresults %>%\n  summarise(p_value = mean(diffprop>=obs))##   p_value\n## 1   0.011\ntally(~outcome+group,data=heart,margins = TRUE)##        group\n## outcome Control Treatment Total\n##   Alive       4        24    28\n##   Dead       30        45    75\n##   Total      34        69   103\nphyper(4,34,69,28)## [1] 0.01039537"},{"path":"HYPOTEST.html","id":"HYPOTEST","chapter":"17 Hypothesis Testing","heading":"17 Hypothesis Testing","text":"","code":""},{"path":"HYPOTEST.html","id":"objectives-16","chapter":"17 Hypothesis Testing","heading":"17.1 Objectives","text":"Know properly use terminology hypothesis test.Conduct four steps hypothesis test using randomization.Discuss explain ideas decision errors, one-sided versus two-sided, choice statistical significance.","code":""},{"path":"HYPOTEST.html","id":"homework-16","chapter":"17 Hypothesis Testing","heading":"17.2 Homework","text":"","code":""},{"path":"HYPOTEST.html","id":"problem-1-16","chapter":"17 Hypothesis Testing","heading":"17.2.1 Problem 1","text":"Repeat analysis commercial length notes. time use different test statistic.State null alternative hypotheses.\\(H_0\\): Null hypothesis. distribution length commercials premium basic channels .\\(H_A\\): Alternative hypothesis. distribution length commercials premium basic channels different.Compute test statistic.use difference means can use diffmeans() mosiac.Determine p-value.Next create plot empirical sampling distribution difference means., notice centered zero symmetrical.p-value much smaller! test statistic matters terms efficiency testing procedure.Draw conclusion.Based data, really difference distribution lengths commercials 30 minute shows basic premium channels probability finding observed difference means 0.005. Since less significance level 0.05, reject null favor alternative basic channel longer commercials.","code":"\nads<-read_csv(\"data/ads.csv\")\nads## # A tibble: 10 x 2\n##    basic premium\n##    <dbl>   <dbl>\n##  1  6.95    3.38\n##  2 10.0     7.8 \n##  3 10.6     9.42\n##  4 10.2     4.66\n##  5  8.58    5.36\n##  6  7.62    7.63\n##  7  8.23    4.95\n##  8 10.4     8.01\n##  9 11.0     7.8 \n## 10  8.52    9.58\nads <- ads %>%\n  pivot_longer(cols=everything(),names_to=\"channel\",values_to = \"length\")\nads## # A tibble: 20 x 2\n##    channel length\n##    <chr>    <dbl>\n##  1 basic     6.95\n##  2 premium   3.38\n##  3 basic    10.0 \n##  4 premium   7.8 \n##  5 basic    10.6 \n##  6 premium   9.42\n##  7 basic    10.2 \n##  8 premium   4.66\n##  9 basic     8.58\n## 10 premium   5.36\n## 11 basic     7.62\n## 12 premium   7.63\n## 13 basic     8.23\n## 14 premium   4.95\n## 15 basic    10.4 \n## 16 premium   8.01\n## 17 basic    11.0 \n## 18 premium   7.8 \n## 19 basic     8.52\n## 20 premium   9.58\nfavstats(length~channel,data=ads)##   channel   min      Q1 median       Q3    max   mean       sd  n missing\n## 1   basic 6.950 8.30375  9.298 10.30000 11.016 9.2051 1.396126 10       0\n## 2 premium 3.383 5.05250  7.715  7.95975  9.580 6.8592 2.119976 10       0\nobs <- diffmean(length~channel,data=ads)\nobs## diffmean \n##  -2.3459\nset.seed(4172)\nresults <- do(10000)*diffmean(length~shuffle(channel),data=ads)\nresults %>%\n  gf_histogram(~diffmean,\n               fill=\"cyan\",\n               color=\"black\") %>%\n  gf_vline(xintercept =obs) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Test statistic\")\nprop1(~(diffmean<=obs),data=results)##  prop_TRUE \n## 0.00459954"},{"path":"HYPOTEST.html","id":"problem-2-16","chapter":"17 Hypothesis Testing","heading":"17.2.2 Problem 2","text":"yawning contagious?experiment conducted MythBusters, science entertainment TV program Discovery Channel, tested person can subconsciously influenced yawning another person near yawns. 50 people randomly assigned two groups: 34 group person near yawned (treatment) 16 group wasn’t person yawning near (control). following table shows results experiment.\\[\n\\begin{array}{ccc|cc|c} & & &\\textbf{Group} &  &\n\\\\& &       & Treatment     & Control       & Total \\\\\n&\\hline \\textbf{Result}     & \\textit{Yawn}     & 10        & 4         & 14    \\\\\n& & \\textit{Yawn}        & 24        & 12            & 36\\\\\n&\\hline &Total              & 34        & 16            & 50\n\\end{array} \n\\]data file “yawn.csv”.hypotheses?\\(H_0\\): Yawning contagious, someone group yawning impact percentage group yawns. \\(p_c - p_t = 0\\) equivalently \\(p_c = p_t\\) .\\(H_A\\): Yawning impact, contagious. someone yawns likely yawn. \\(p_t > p_c\\) \\(p_c - p_t < 0\\).Calculate observed difference yawning rates two scenarios. Yes giving test statistic.Notice negative. positive, even need next step; fail reject null p-value much larger 0.05. Think make sure understand.Estimate p-value using randomization.large p-value. Notice two-sided hypothesis test, doubling p-value exceed 1. Since p-value probability, possible report p-value approximately 1. reason sampling distribution symmetrical. can see plot hypergeometric distribution problem.Plot empirical sampling distribution.Determine conclusion hypothesis test.Since p-value, 0.54, high, larger 0.05, fail reject null hypothesis yawning contagious. data provide convincing evidence people likely yawn person near yawns.traditional belief yawning contagious – one yawn can lead another yawn, might lead another, . exercise, option selecting one-sided two-sided test. recommend (choose)? Justify answer 1-3 sentences.chose one-sided test since researcher, thought someone group yawn lead people group yawning.select level significance? Explain 1-3 sentences.Since clear impact one type error worse , stayed default 0.05.","code":"\nyawn <- read_csv(\"data/yawn.csv\")\nglimpse(yawn)## Rows: 50\n## Columns: 2\n## $ group   <chr> \"treatment\", \"treatment\", \"control\", \"treatment\", \"treatment\",~\n## $ outcome <chr> \"no_yawn\", \"no_yawn\", \"no_yawn\", \"no_yawn\", \"no_yawn\", \"yawn\",~\ninspect(yawn)## \n## categorical variables:  \n##      name     class levels  n missing\n## 1   group character      2 50       0\n## 2 outcome character      2 50       0\n##                                    distribution\n## 1 treatment (68%), control (32%)               \n## 2 no_yawn (72%), yawn (28%)\ntally(outcome~group,data=yawn,margins = TRUE,format=\"proportion\")##          group\n## outcome     control treatment\n##   no_yawn 0.7500000 0.7058824\n##   yawn    0.2500000 0.2941176\n##   Total   1.0000000 1.0000000\nobs <- diffprop(outcome~group,data=yawn)\nobs##    diffprop \n## -0.04411765\nset.seed(56)\nresults<-do(10000)*diffprop(outcome~shuffle(group),data=yawn)\nprop1(~(diffprop<=obs),data=results)## prop_TRUE \n## 0.5140486\ngf_dist(\"hyper\",m=16,n=34,k=14) %>%\n  gf_theme(theme_classic()) %>%\n  gf_refine(scale_x_continuous(breaks=0:14))\nresults %>%\n  gf_histogram(~diffprop,\n               fill=\"cyan\",\n               color=\"black\") %>%\n  gf_vline(xintercept =obs ) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Test statistic\")"},{"path":"PVALUES.html","id":"PVALUES","chapter":"18 Empirical p-values","heading":"18 Empirical p-values","text":"","code":""},{"path":"PVALUES.html","id":"objective","chapter":"18 Empirical p-values","heading":"18.1 Objective","text":"Conduct four steps hypothesis test using probability models.","code":""},{"path":"PVALUES.html","id":"homework-17","chapter":"18 Empirical p-values","heading":"18.2 Homework","text":"","code":""},{"path":"PVALUES.html","id":"problem-1-17","chapter":"18 Empirical p-values","heading":"18.2.1 Problem 1","text":"Repeat analysis yawning data last lesson time use hypergeometric distribution.yawning contagious?experiment conducted MythBusters, science entertainment TV program Discovery Channel, tested person can subconsciously influenced yawning another person near yawns. 50 people randomly assigned two groups: 34 group person near yawned (treatment) 16 group wasn’t person yawning near (control). following table shows results experiment.\\[\n\\begin{array}{ccc|cc|c} & & &\\textbf{Group} &  &\n\\\\& &       & Treatment     & Control       & Total \\\\\n&\\hline \\textbf{Result}     & \\textit{Yawn}     & 10        & 4         & 14    \\\\\n& & \\textit{Yawn}        & 24        & 12            & 36\\\\\n&\\hline &Total              & 34        & 16            & 50\n\\end{array} \n\\]data file “yawn.csv”.hypotheses?\\(H_0\\): Yawning contagious, someone group yawning impact percentage group yawns.\\(H_A\\): Yawning impact, contagious. someone yawns likely yawn.Calculate observed statistic, pick cell.random variable number control patients yawned population 16 control patients, 34 treatment patients, total 14 yawned.Find p-value using hypergeometric distribution.case want find \\(\\mbox{P}(X \\leq 4)\\) double since two-sided test.Doubling take us 1, valid. might seen randomization test problem previous lesson. , since hypergeometric symmetrical, can’t just double p-value one-sided test. simply report result \\(\\approx 1\\). look plot pmf, see figure, see \\(X=4\\) highest probability outcome. Thus p-value 1 sum values less equal \\(P(X=4)\\).Plot sampling distribution.Determine conclusion hypothesis test.Since p-value, 1, high, larger 0.05, fail reject null hypothesis yawning contagious. data provide convincing evidence people likely yawn person near yawns.Compare results randomization test.result essentially randomization test.","code":"\nyawn <- read_csv(\"data/yawn.csv\")\nglimpse(yawn)## Rows: 50\n## Columns: 2\n## $ group   <chr> \"treatment\", \"treatment\", \"control\", \"treatment\", \"treatment\",~\n## $ outcome <chr> \"no_yawn\", \"no_yawn\", \"no_yawn\", \"no_yawn\", \"no_yawn\", \"yawn\",~\ninspect(yawn)## \n## categorical variables:  \n##      name     class levels  n missing\n## 1   group character      2 50       0\n## 2 outcome character      2 50       0\n##                                    distribution\n## 1 treatment (68%), control (32%)               \n## 2 no_yawn (72%), yawn (28%)\ntally(~outcome+group,data=yawn,margins = TRUE)##          group\n## outcome   control treatment Total\n##   no_yawn      12        24    36\n##   yawn          4        10    14\n##   Total        16        34    50\nphyper(4,16,34,14)## [1] 0.5127818\ngf_dist(\"hyper\",m=16,n=34,k=14) %>%\n  gf_hline(yintercept = dhyper(4,16,34,14),color=\"red\") %>%\n  gf_labs(title=\"Hypergeometric pmf\",\n          subtitle=\"Red line is P(X=4)\",\n          y=\"Probability\") %>%\n  gf_theme(theme_classic)## Warning: geom_hline(): Ignoring `mapping` because `yintercept` was provided.\nfisher.test(tally(~group+outcome,data=yawn))## \n##  Fisher's Exact Test for Count Data\n## \n## data:  tally(~group + outcome, data = yawn)\n## p-value = 1\n## alternative hypothesis: true odds ratio is not equal to 1\n## 95 percent confidence interval:\n##  0.2790902 6.5930656\n## sample estimates:\n## odds ratio \n##   1.244531\ntemp<-dhyper(0:14,16,34,14)\nsum(temp[temp<=dhyper(4,16,34,14)])## [1] 1\ngf_dist(\"hyper\",m=16,n=34,k=14) %>%\n  gf_vline(xintercept =4,color=\"red\" ) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title=\"Hypergeometric pmf\",\n          subtitle=\"Red line is X=4\",\n          y=\"Probability\")"},{"path":"PVALUES.html","id":"problem-2-17","chapter":"18 Empirical p-values","heading":"18.2.2 Problem 2","text":"Repeat golf ball example using different test statistic.Use level significance 0.05.State null alternative hypotheses.think numbers equally likely. question one-sided versus two-sided relevant test, see write hypotheses.\\(H_0\\): numbers equally likely.\\(\\pi_1 = \\pi_2 = \\pi_3 = \\pi_4\\) \\(\\pi_1 = \\frac{1}{4}, \\pi_2 =\\frac{1}{4}, \\pi_3 =\\frac{1}{4}, \\pi_4 =\\frac{1}{4}\\)\\(H_A\\): distribution percentages population. least one population proportion \\(\\frac{1}{4}\\).Compute test statistic.use maximum deviation expected valueDetermine p-value.Draw conclusion.Since p-value larger 0.05, reject null hypothesis. , based data, find statistically significant evidence claim number golf balls equally likely.","code":"\ngolf_balls <- read_csv(\"data/golf_balls.csv\")\ninspect(golf_balls)## \n## quantitative variables:  \n##        name   class min Q1 median Q3 max     mean       sd   n missing\n## ...1 number numeric   1  1      2  3   4 2.366255 1.107432 486       0\ntally(~number,data=golf_balls,format = \"proportion\")## number\n##         1         2         3         4 \n## 0.2818930 0.2839506 0.2201646 0.2139918\nobs <- max(abs(tally(~number,data=golf_balls) -121.5))\nobs## [1] 17.5\nset.seed(2517)\nresults <- do(10000)*max(abs(table(sample(1:4,size=486,replace=TRUE))-121.5))\nresults %>%\n  gf_histogram(~max,fill=\"cyan\",color=\"black\") %>%\n  gf_vline(xintercept = obs) %>%\n  gf_labs(title=\"Sampling Distribution of Maximum Deviation\",\n          subtitle=\"Multinomial with equal probability\",\n          x=\"Range\") %>%\n  gf_theme(theme_classic)\nprop1(~(max>=obs),data=results)## prop_TRUE \n## 0.2382762"},{"path":"PVALUES.html","id":"problem-3-12","chapter":"18 Empirical p-values","heading":"18.2.3 Problem 3","text":"Body TemperatureShoemaker6 cites paper American Medical Association7 questions conventional wisdom average body temperature human 98.6. One main points original article – traditional mean 98.6 , essence, 100 years date. authors cite problems Wunderlich’s original methodology, diurnal fluctuations (0.9 degrees F per day), unreliable thermometers. authors believe average temperature less 98.6. Test hypothesis.State null alternative hypotheses.\\(H_0\\): average body temperature 98.6 \\(\\mu = 98.6\\)\\(H_A\\): average body temperature less 98.6. \\(\\mu < 98.6\\)State significance level used.reason believe one type error important another.\\(\\alpha = 0.05\\)Load data file “temperature.csv” generate summary statistics boxplot temperature data. using gender heart rate problem.Compute test statistic. going help part. randomization test since don’t second variable. nice use mean test statistic don’t yet know sampling distribution sample mean.Let’s get clever. distribution sample symmetric, assumption look boxplot summary statistics determine comfortable , null hypothesis observed values equally likely either greater less 98.6. Thus test statistic number cases positive difference observed value 98.6. binomial distribution probability success 0.5. must also account possibility observations 98.6 data.First let’s find many data points equal 98.6.ten observations equal 98.6, split make 5 positive difference 5 negative difference.Next determine number subjects positive difference.Therefore total 44 subjects whose temperature greater 98.6 86 temperature less 98.6Determine p-value.130 subjects, 86 temperature less 98.6 44 temperature greater. can use either number determine p-value. null hypothesis true, probability 86 , extreme alternative hypothesis, isWe also done 44 less.p-value 0.00014.choices drop 10 data points equal 98.6. always careful deleting data. extremely conservative move 10 points greater 98.6 still reject, comfortable conclusion. Finally, randomly assign point one side . can check p-values methods.Draw conclusion.Based data, true mean body temperature 98.6, probability 86 subjects 130 temperatures 0.00014. unlikely reject hypothesis average body temperature 98.6.clever way test claim. Make sure understand solved. coming lessons show alternative ways attack problem. made binomial random variable. assumption independence symmetry.Notice descritizing problem, taking information away. However, p-value still small.","code":"\ntemperature <- read_csv(\"data/temperature.csv\")\nglimpse(temperature)## Rows: 130\n## Columns: 3\n## $ temperature <dbl> 96.3, 96.7, 96.9, 97.0, 97.1, 97.1, 97.1, 97.2, 97.3, 97.4~\n## $ gender      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~\n## $ hr          <dbl> 70, 71, 74, 80, 73, 75, 82, 64, 69, 70, 68, 72, 78, 70, 75~\nfavstats(~temperature,data=temperature)##   min   Q1 median   Q3   max     mean        sd   n missing\n##  96.3 97.8   98.3 98.7 100.8 98.24923 0.7331832 130       0\ntemperature %>%\n  gf_boxplot(~temperature) %>%\n  gf_theme(theme_classic)\ntemperature %>%\n  count(temperature==98.6)## # A tibble: 2 x 2\n##   `temperature == 98.6`     n\n##   <lgl>                 <int>\n## 1 FALSE                   120\n## 2 TRUE                     10\ntemperature %>%\n  mutate(pos=(temperature-98.6)>0) %>%\n  summarise(num_greater=sum(pos))## # A tibble: 1 x 1\n##   num_greater\n##         <int>\n## 1          39\n1-pbinom(85,130,.5)## [1] 0.0001447744\npbinom(44,130,.5)## [1] 0.0001447744"},{"path":"CLT.html","id":"CLT","chapter":"19 Central Limit Theorem","heading":"19 Central Limit Theorem","text":"","code":""},{"path":"CLT.html","id":"objectives-17","chapter":"19 Central Limit Theorem","heading":"19.1 Objectives","text":"Explain central limit theorem can use inference.Conduct hypothesis tests single mean proportion using CLT R.Explain chi-squared \\(t\\) distributions relate normal distribution, use , describe impact shape distribution parameters changed.","code":""},{"path":"CLT.html","id":"homework-18","chapter":"19 Central Limit Theorem","heading":"19.2 Homework","text":"","code":""},{"path":"CLT.html","id":"problem-1-18","chapter":"19 Central Limit Theorem","heading":"19.2.1 Problem 1","text":"Suppose roll fair six-sided die let \\(X\\) resulting number. distribution \\(X\\) discrete uniform. (six discrete outcomes equally likely.)Suppose roll fair die 5 times record value \\(\\bar{X}\\), mean resulting rolls. central limit theorem, distribution \\(\\bar{X}\\)?mean \\(X\\) 3.5 variance \\(X = \\frac{(b-+1)^2-1}{12} = \\frac{35}{12}\\) 2.9167. ,\n\\[\n\\bar{X}\\overset{approx}{\\sim}\\textsf{Norm}(3.5,0.764)\n\\]Simulate process R. Plot resulting empirical distribution \\(\\bar{X}\\) report mean standard deviation \\(\\bar{X}\\). expected?(HINT: can simulate die roll using sample function. careful make sure use properly.)appears roughly normally distributed mean standard deviation expected.Repeat parts ) b) \\(n=20\\) \\(n=50\\). Describe notice. Make sure three plots plotted \\(x\\)-axis scale. can use facets combine data one tibble.\\(n=20\\):\n\\[\n\\bar{X}\\overset{approx}{\\sim}\\textsf{Norm}(3.5,0.382)\n\\]\\(n=50\\):\n\\[\n\\bar{X}\\overset{approx}{\\sim}\\textsf{Norm}(3.5,0.242)\n\\]Now let’s put together make easier compare.results expected. \\(n\\) increased, variance sample mean decreased.","code":"\nset.seed(2003)\nresults<-do(10000)*mean(sample(6,5,replace=T))\nresults %>%\n   gf_histogram(~mean,fill=\"cyan\",color=\"black\") %>%\n   gf_theme(theme_classic()) %>%\n   gf_labs(x=\"Test statistic\")\nfavstats(~mean,data=results)##  min Q1 median Q3 max    mean       sd     n missing\n##    1  3    3.6  4   6 3.51278 0.772254 10000       0\nresults2<-do(10000)*mean(sample(6,20,replace=T))\nresults2 %>%\n   gf_histogram(~mean,fill=\"cyan\",color=\"black\") %>%\n   gf_theme(theme_classic()) %>%\n   gf_labs(x=\"Test statistic\")\nfavstats(~mean,data=results2)##   min   Q1 median   Q3  max    mean        sd     n missing\n##  2.15 3.25    3.5 3.75 4.95 3.49896 0.3828754 10000       0\nresults3<-do(10000)*mean(sample(6,50,replace=T))\nresults3 %>%\n   gf_histogram(~mean,fill=\"cyan\",color=\"black\") %>%\n   gf_theme(theme_classic()) %>%\n   gf_labs(x=\"Test statistic\")\nfavstats(~mean,data=results3)##   min   Q1 median   Q3  max    mean        sd     n missing\n##  2.54 3.34    3.5 3.66 4.36 3.49852 0.2423665 10000       0\nfinal_results<-rbind(cbind(results,n=10),cbind(results2,n=20),cbind(results3,n=50))\nfinal_results %>%\n   gf_dhistogram(~mean|n,fill=\"cyan\",color=\"black\") %>%\n   gf_theme(theme_classic()) %>%\n   gf_labs(x=\"Test statistic\")\nfavstats(~mean|n,data=final_results) %>%\n   select(mean,sd,n)##      mean        sd  n\n## 1 3.51278 0.7722540 10\n## 2 3.49896 0.3828754 20\n## 3 3.49852 0.2423665 50"},{"path":"CLT.html","id":"problem-2-18","chapter":"19 Central Limit Theorem","heading":"19.2.2 Problem 2","text":"nutrition label bag potato chips says one ounce (28 gram) serving potato chips 130 calories contains ten grams fat, three grams saturated fat. random sample 35 bags yielded sample mean 134 calories standard deviation 17 calories. evidence nutrition label provide accurate measure calories bags potato chips? conditions necessary applying normal model checked satisfied.question framed terms two possibilities: nutrition label accurately lists correct average calories per bag chips , may framed hypothesis test.Write null alternative hypothesis.\\(H_0\\): average listed correctly. \\(\\mu = 130\\)\\(H_A\\): nutrition label incorrect. \\(\\mu \\neq 130\\)level significance going use?going use \\(\\alpha = 0.05\\).distribution test statistic \\({\\bar{X}-\\mu\\S/\\sqrt{n}}\\)? Calculate observed value.distribution test statistic \\(t\\) 34 degrees freedom.observed average \\(\\bar{x} = 134\\) standard error may calculated \\(SE = \\frac{17}{\\sqrt{35}} = 2.87\\).can compute test statistic t score:\n\\[\nt = \\frac{134 - 130}{2.87} = 1.39\n\\]\nd. Calculate p-value.upper-tail area 0.0823,orso p-value \\(2 \\times 0.0823 = 0.1646\\).Draw conclusion.Since p-value larger 0.05, reject null hypothesis. , enough evidence show nutrition label incorrect information.","code":"\npt(1.39,34,lower.tail = F)## [1] 0.08678153\n1-pt(1.39,34)## [1] 0.08678153"},{"path":"CLT.html","id":"extra-material","chapter":"19 Central Limit Theorem","heading":"19.2.2.1 Extra material","text":"used normal model based CLT p-value close value \\(t\\) sample size large.","code":"\npnorm(1.39,lower.tail = F)## [1] 0.08226444"},{"path":"CLT.html","id":"problem-3-13","chapter":"19 Central Limit Theorem","heading":"19.2.3 Problem 3","text":"Exploration chi-squared \\(t\\) distributions.R, plot pdf random variable chi-squared distribution 1 degree freedom. plot, include pdfs degrees freedom 5, 10 50. Describe behavior pdf changes increasing degrees freedom.“bump” moves rights degrees freedom increase.plot legend, find way within ggformula ggplot.Repeat part () \\(t\\) distribution. Add pdf standard normal random variable well. notice?degrees freedom increases, \\(t\\)-distribution approaches standard normal distribution.","code":"\ngf_dist(\"chisq\",df=1,col=1) %>%\n   gf_dist(\"chisq\",df=5,col=2) %>%\n   gf_dist(\"chisq\",df=10,col=3) %>%\n   gf_dist(\"chisq\",df=50,col=4) %>%\n   gf_lims(y=c(0,.25)) %>%\n   gf_labs(y=\"f(x)\") %>%\n   gf_theme(theme_classic()) \nggplot(data = data.frame(x = c(0, 75)), aes(x)) +\n  stat_function(fun = dchisq, n = 101, \n                args = list(df = 1),\n                mapping=aes(col=\"myline1\")) + \n  stat_function(fun = dchisq, n = 101, \n                args = list(df = 5),\n                mapping=aes(col=\"myline2\")) + \n  stat_function(fun = dchisq, n = 101, \n                args = list(df = 10),\n                mapping=aes(col=\"myline3\")) +    \n   stat_function(fun = dchisq, n = 101, \n                args = list(df = 50),\n                mapping=aes(col=\"myline4\")) + \n   ylab(\"\") +\n  scale_y_continuous(breaks = NULL) +\n   theme_classic()+\nscale_colour_manual(name=\"Legend\",\n    values=c(myline1=\"black\", \n             myline2=\"red\",\n             myline3=\"green\",\n             myline4=\"blue\"),\n    labels=c(\"df=1\",\"df=5\",\"df=10\",\"df=50\"))\ngf_dist(\"t\",df=1,col=\"black\") %>%\n   gf_dist(\"t\",df=5,col=\"red\") %>%\n   gf_dist(\"t\",df=10,col=\"green\") %>%\n   gf_dist(\"t\",df=50,col=\"blue\") %>%\n   gf_dist(\"norm\",lty=2,lwd=1.5) %>%\n   gf_lims(x=c(-4,4)) %>%\n   gf_labs(y=\"f(x)\") %>%\n   gf_theme(theme_classic()) \nggplot(data = data.frame(x = c(-4, 4)), aes(x)) +\n  stat_function(fun = dt, n = 101, \n                args = list(df = 1),\n                mapping=aes(col=\"myline1\")) + \n  stat_function(fun = dt, n = 101, \n                args = list(df = 5),\n                mapping=aes(col=\"myline2\")) + \n  stat_function(fun = dt, n = 101, \n                args = list(df = 10),\n                mapping=aes(col=\"myline3\")) +    \n   stat_function(fun = dt, n = 101, \n                args = list(df = 50),\n                mapping=aes(col=\"myline4\")) + \n   stat_function(fun = dnorm, n = 101, \n                args = list(mean=0,sd=1),\n                linetype=\"dashed\",\n                mapping=aes(col=\"myline5\")) + \n   ylab(\"\") +\n  scale_y_continuous(breaks = NULL) +\n   theme_classic()+\nscale_colour_manual(name=\"Legend\",\n    values=c(myline1=\"black\", \n             myline2=\"red\",\n             myline3=\"green\",\n             myline4=\"blue\",\n             myline5=\"grey\"),\n    labels=c(\"df=1\",\"df=5\",\"df=10\",\"df=50\",\"Normal\"))"},{"path":"CLT.html","id":"problem-4-7","chapter":"19 Central Limit Theorem","heading":"19.2.4 Problem 4","text":". lesson, used expression degrees freedom lot. expression mean? sample size \\(n\\), \\(n-1\\) degrees freedom \\(t\\) distribution? Give short concise answer (one paragraph). likely little research .Answers vary. One possible explanation degrees freedom represents number independent pieces information. example, ’ll notice order get unbiased estimate \\(\\sigma^2\\), divide \\(n-1\\). order estimate \\(\\sigma^2\\), need first estimate \\(\\mu\\), done obtaining sample mean. know sample mean, \\(n-1\\) pieces independent information. example, suppose sample size 10, know sample mean. given first 9 observations, know exactly 10th observation must .","code":""},{"path":"CLT.html","id":"problem-5-5","chapter":"19 Central Limit Theorem","heading":"19.2.5 Problem 5","text":". Deborah Toohey running Congress, campaign manager claims 50% support district’s electorate. Ms. Toohey’s opponent claimed Ms. Toohey less 50%. Set hypothesis test evaluate right.run one-sided two-sided hypothesis test?run two-sided. greater 50% regardless opponent claims.Write null alternative hypothesis.\\(H_0\\): Ms. Toohey’s support 50%. \\(p = 0.50\\).\\(H_A\\): Ms. Toohey’s support either 50%. \\(p \\neq 0.50\\).level significance going use?\\(\\alpha = 0.05\\)assumptions test?observations independent.observations independent.least 10 votes 10 .least 10 votes 10 .simple random sample includes fewer 10% population, observations independent. single proportion hypothesis test, success-failure condition checked using null proportion, \\(p_0=0.5\\): \\(np_0 = n(1-p_0) = 500\\times 0.5 = 250 \\geq 10\\). conditions verified, normal model based CLT may applied \\(\\hat{p}\\).Calculate test statistic.newspaper collects simple random sample 500 likely voters district estimates Toohey’s support 52%.test statistic \\(\\bar{x}=0.52\\)Calculate p-value.Based normal model, can compute one-sided p-value double get correct p-value.standard error can computed. null value used , hypothesis test single proportion specified value probability success.\\[SE = \\sqrt{\\frac{p_0\\times (1-p_0)}{n}} = \\sqrt{\\frac{0.5\\times (1-0.5)}{500}} = 0.022\\]Draw conclusion.p-value larger 0.05, reject null hypothesis, find convincing evidence support campaign manager’s claim.","code":"\n2*pnorm(.52,mean=.5,sd=0.022,lower.tail = FALSE)## [1] 0.3633021"},{"path":"CI.html","id":"CI","chapter":"20 Confidence Intervals","heading":"20 Confidence Intervals","text":"","code":""},{"path":"CI.html","id":"objectives-18","chapter":"20 Confidence Intervals","heading":"20.1 Objectives","text":"Using asymptotic methods based normal distribution, construct interpret confidence interval unknown parameter.Describe relationships confidence intervals, confidence level, sample size.proportions, able calculate three different approaches confidence intervals using R.","code":""},{"path":"CI.html","id":"homework-19","chapter":"20 Confidence Intervals","heading":"20.2 Homework","text":"","code":""},{"path":"CI.html","id":"problem-1-19","chapter":"20 Confidence Intervals","heading":"20.2.1 Problem 1","text":"Chronic illnessIn 2013, Pew Research Foundation reported “45% U.S. adults report live one chronic conditions”.8 However, value based sample, may perfect estimate population parameter interest . study reported standard error 1.2%, normal model may reasonably used setting.Create 95% confidence interval proportion U.S. adults live one chronic conditions. Also interpret confidence interval context study.\\(0.45 \\pm 1.96 \\times 0.012 = (0.426, 0.474)\\)\n95% confident 42.6% 47.4% U.S. adults live one chronic conditions.Create 99% confidence interval proportion U.S. adults live one chronic conditions. Also interpret confidence interval context study.99% confident 41.9% 48.1% U.S. adults live one chronic conditions.Identify following statements true false. Provide explanation justify answers.can say certainty confidence interval part contains true percentage U.S. adults suffer chronic illness.False, ’re 95% confident.repeated study 1,000 times constructed 95% confidence interval study, approximately 950 confidence intervals contain true fraction U.S. adults suffer chronic illnesses.True, definition confidence level.poll provides statistically significant evidence (\\(\\alpha = 0.05\\) level) percentage U.S. adults suffer chronic illnesses 50%.True, equivalent significance level two-sided hypothesis test 95% confidence interval indeed 5%, since interval lies 50% statement correct.Since standard error 1.2%, 1.2% people study communicated uncertainty answer.False, 1.2% measures uncertainty associated sample proportion (point estimate) uncertainty individual observations, uncertainty sense sure one’s answer survey question.Suppose researchers formed one-sided hypothesis, believed true proportion less 50%. find equivalent one-sided 95% confidence interval taking upper bound two-sided 95% confidence interval.False. constructing one-sided confidence interval need \\(\\alpha\\) one tail. taking upper value two-sided 95% confidence interval leads 97.5% one-sided confidence interval, conservative value.Notice 46.9 smaller 47.4.","code":"\nqnorm(.995)## [1] 2.575829\n0.45 +c(-1,1)*qnorm(.995)*0.012## [1] 0.41909 0.48091\nqnorm(.95)## [1] 1.644854\n0.45 +qnorm(.95)*0.012## [1] 0.4697382"},{"path":"CI.html","id":"problem-2-19","chapter":"20 Confidence Intervals","heading":"20.2.2 Problem 2","text":"Vegetarian college studentsSuppose 8% college students vegetarians. Determine following statements true false, explain reasoning.distribution sample proportions vegetarians random samples size 60 approximately normal since \\(n \\ge 30\\).FALSE. distribution \\(\\hat{p}\\) approximately normal, need least\n10 successes 10 failures sample.distribution sample proportions vegetarian college students random samples size 50 right skewed.TRUE. success-failure condition satisfied\n\\[np = 50 \\times 0.08 = 4\\] \n\\[n(1 - p) = 50 \\times 0.92 = 46\\]\ntherefore know distribution \\(\\hat{p}\\) approximately normal. samples expect \\(\\hat{p}\\) close 0.08, true population proportion. \\(\\hat{p}\\) can high 1 (though expect effectively never happen), can go low 0. Therefore distribution probably take right-skewed shape. Plotting sampling distribution confirm suspicion.random sample 125 college students 12% vegetarians considered unusual.FALSE.\n\\[ SE_{\\hat{p}}\n    \\approx \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\\[=\\sqrt{\\frac{.08(.92)}{125}} = 0.0243\\]\\(\\hat{p}\\) 0.12 \\(\\frac{0.12 - 0.08}{0.0243} = 1.65\\) standard errors away mean, considered unusual.p-value :random sample 250 college students 12% vegetarians considered unusual.TRUE.\\[ SE_{\\hat{p}}\n    \\approx \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\\[=\\sqrt{\\frac{.08(.92)}{250}} = 0.0172\\]Notice doubling sample size reduced standard error \\(\\sqrt{2}\\).\\(\\hat{p}\\) 0.12 \\(\\frac{0.12 - 0.08}{0.0172} = 2.32\\) standard errors away mean, might considered unusual.p-value :standard error reduced one-half increased sample size 125 250.FALSE. Since n appears square root sign formula standard error, increasing sample size 125 250 decrease standard error sample proportion factor \\(\\sqrt{2}\\).99% confidence wider 95% higher confidence level requires wider interval.TRUE. width function margin error. Keeping else , critical values 95% 99% ","code":"\n2*(1-pnorm(1.65))## [1] 0.09894294\n2*(1-pnorm(2.32))## [1] 0.02034088\ntemp<-qnorm(c(.985,.995))\nnames(temp)<-c(\"95%\",\"99%\")\ntemp##      95%      99% \n## 2.170090 2.575829"},{"path":"CI.html","id":"problem-3-14","chapter":"20 Confidence Intervals","heading":"20.2.3 Problem 3","text":"Orange tabbiesSuppose 90% orange tabby cats male. Determine following statements true false, explain reasoning.distribution sample proportions random samples size 30 left skewed.TRUE. success-failure condition satisfied\\[n\\hat{p} = 30 \\times 0.90 = 27\\] \\[n(1-\\hat{p}) = 30 \\times 0.10 = 3;\\]therefore know distribution \\(\\hat{p}\\) nearly normal. samples expect \\(\\hat{p}\\) close 0.90, true population proportion. \\(\\hat{p}\\) can low 0 (though expect happen rarely), can go high 1. Therefore distribution probably take left-skewed shape. Plotting sampling distribution confirm suspicion.Using sample size 4 times large reduce standard error sample proportion one-half.TRUE. Since \\(n\\) appears square root \\(SE\\), using sample size 4 times large reduce \\(SE\\) half.distribution sample proportions random samples size 140 approximately normal.TRUE. success-failure condition satisfied\\[n\\hat{p} = 140 \\times 0.90 = 126\\] \\[n(1-\\hat{p}) = 140 \\times 0.10 = 14;\\]therefore distribution \\(\\hat{p}\\) nearly normal.","code":""},{"path":"CI.html","id":"problem-4-8","chapter":"20 Confidence Intervals","heading":"20.2.4 Problem 4","text":"Working backwardsA 90% confidence interval population mean (65,77). population distribution approximately normal population standard deviation unknown. confidence interval based simple random sample 25 observations. Calculate sample mean, margin error, sample standard deviation.sample mean midpoint confidence interval:\\[\\bar{x} = \\frac{65+77}{2} = 71\\]\nmargin error half width confidence interval:\\[= \\frac{ \\left(77 - 65 \\right)}{2} = 6 \\]Using df = 25 - 1 = 24 confidence level 90% can find critical value t-table distribution.Lastly, using margin error critical value can solve \\(s\\):\n\\[ = t_{24}\\times \\frac{s}{\\sqrt{n}}\\]\n\\[6 = 1.71\\times \\frac{s}{\\sqrt{25}}\\]\n\\[s = 17.54\\]","code":"\nqt(.95,24)## [1] 1.710882"},{"path":"CI.html","id":"problem-5-6","chapter":"20 Confidence Intervals","heading":"20.2.5 Problem 5","text":"Find p-valueAn independent random sample selected approximately normal population unknown standard deviation. Find p-value given set hypotheses \\(T\\) test statistic. Also determine null hypothesis rejected \\(\\alpha = 0.05\\).\\(H_{}: \\mu > \\mu_{0}\\), \\(n = 11\\), \\(T = 1.91\\)p-value less 0.05, reject null hypothesis.\\(H_{}: \\mu < \\mu_{0}\\), \\(n = 17\\), \\(T = - 3.45\\)p-value less 0.05, reject null hypothesis.\\(H_{}: \\mu \\ne \\mu_{0}\\), \\(n = 7\\), \\(T = 0.83\\)p-value greater 0.05, fail reject null hypothesis.\\(H_{}: \\mu > \\mu_{0}\\), \\(n = 28\\), \\(T = 2.13\\)p-value less 0.05, reject null hypothesis.","code":"\n1-pt(1.91,10)## [1] 0.04260244\npt(-3.45,16)## [1] 0.001646786\n2*(1-pt(0.83,6))## [1] 0.4383084\n1-pt(2.13,27)## [1] 0.02121769"},{"path":"CI.html","id":"problem-6-4","chapter":"20 Confidence Intervals","heading":"20.2.6 Problem 6","text":"Sleep habits New YorkersNew York known “city never sleeps”. random sample 25 New Yorkers asked much sleep get per night. Statistical summaries data shown . data provide strong evidence New Yorkers sleep less 8 hours night average?\\[\n\\begin{array}{cccccc} & & & &  &\n\\\\& n & \\bar{x} & s     & min   & max \\\\\n&\\hline 25  & 7.73      & 0.77  & 6.17  & 9.78 \\\\ \n&   &               &       &           & \n\\end{array} \n\\]Write hypotheses symbols words.\\(H_0: \\mu = 8\\) New Yorkers sleep 8 hrs per night average.\\(H_A: \\mu < 8\\) New Yorkers sleep less 8 hrs per night average.Check conditions, calculate test statistic, \\(T\\), associated degrees freedom.Independence: sample random 25 less 10% New Yorkers, seems reasonable observations independent.Sample size: Sample size less 30, therefore use t-test implies population must normal.Skew: don’t data look qq plot make guesses. observations within three standard deviations mean. now proceed acknowledging assuming skew perhaps moderate less (moderate skew acceptable sample size).test statistic degrees freedom can calculated follows:\n\\[T = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}  = \\]\n\\[\\frac{7.73 - 8}{\\frac{0.77}{\\sqrt{25}}} = - 1.75\\]\n\\(df = 25 - 1 = 24\\).Find interpret p-value context.fact true population mean amount New Yorkers sleep per night 8 hours, probability getting random sample 25 New Yorkers average amount sleep 7.73 hrs per night less 0.046. p-value close 0.05.conclusion hypothesis test?Since p-value less 0.05, reject null hypothesis New Yorkers sleep average 8 hours per night favor alternative sleep less 8 hours per night average. However, p-value close significance level may want run study /look sample sizes determine big difference 8 hours important practical standpoint.Let’s look power test. Remember power probability rejecting null alternative true. Since alternative specifies range values parameter, must specify value. done subject matter experts. much difference average sleep needed practical standpoint say different? Let’s say half hour important detect. use function power.t.test() determine power test.high level power, typical value used researchers 80%.Let’s see power 15 minute difference.much lower power thus effective test sample size finding small difference. can turn problem around ask sample size need 80% power?need 60 subjects study.Construct 95% confidence interval corresponded hypothesis test, expect 8 hours interval?need 95% upper bound confidence interval.\\[\\bar{x} + t_{24,0.95}{\\frac{s}{\\sqrt{n}}}  = \\]critical value isSo upper confidence bound isThe value 8 included interval, close. need data another study confident results.","code":"\npt(-1.75,24)## [1] 0.04644754\npower.t.test(n=25,delta=.5,sd=.77,alternative = \"one.sided\",type=\"one.sample\")## \n##      One-sample t test power calculation \n## \n##               n = 25\n##           delta = 0.5\n##              sd = 0.77\n##       sig.level = 0.05\n##           power = 0.9342637\n##     alternative = one.sided\npower.t.test(n=25,delta=.25,sd=.77,alternative = \"one.sided\",type=\"one.sample\")## \n##      One-sample t test power calculation \n## \n##               n = 25\n##           delta = 0.25\n##              sd = 0.77\n##       sig.level = 0.05\n##           power = 0.4731184\n##     alternative = one.sided\npower.t.test(power=.8,delta=.25,sd=.77,alternative = \"one.sided\",type=\"one.sample\")## \n##      One-sample t test power calculation \n## \n##               n = 60.02642\n##           delta = 0.25\n##              sd = 0.77\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = one.sided\nqt(.95,24)## [1] 1.710882\n7.73+qt(.95,24)*0.77/sqrt(25)## [1] 7.993476"},{"path":"CI.html","id":"problem-7-2","chapter":"20 Confidence Intervals","heading":"20.2.7 Problem 7","text":"Vegetarian college students IIFrom problem 2 part c, suppose reported 8% college students vegetarians. think USAFA typical fitness health awareness, think vegetarians. collect random sample 125 cadets find 12% claimed vegetarians. enough evidence claim USAFA cadets different?Use binom.test() conduct hypothesis test find confidence interval.fail reject. Notice 0.08 interval.Use prop.test() correct=FALSE conduct hypothesis test find confidence interval.reject, going ?Use prop.test() correction=TRUE conduct hypothesis test find confidence interval.fail reject.test use?Go help binom.test() R explains intervals. way compare look coverage. mean 95% confidence interval, interval include true parameter 95% time? can checked simulation, mathematics really needed definitive answer.first test exact test stated help, guarantees coverage rate least 95%. means interval tends large. largest interval tends conservative.second test Score test, also called Wilson. found inverting p-value, beyond scope class. interval nice compromise coverage.third still score calculation p-value, continuity correction applied. correction takes limits binomial extends 0.5 direction. done give discrete binomial better approximation continuous normal CLT. interval default R via prop.test().None simple confidence interval based normal approximation. code :interval good. coverage poor especially small sample sizes. learn , read Approximate better exact interval estimation binomial proportions. . Agresti B. . Coull, American Statistician 52, 1998, 119-126.","code":"\nbinom.test(x=15,n=125,p=.08,alternative = \"greater\")## \n## \n## \n## data:  15 out of 125\n## number of successes = 15, number of trials = 125, p-value = 0.07483\n## alternative hypothesis: true probability of success is greater than 0.08\n## 95 percent confidence interval:\n##  0.07544411 1.00000000\n## sample estimates:\n## probability of success \n##                   0.12\nprop.test(x=15,n=125,p=.08,alternative = \"greater\",correct=FALSE)## \n##  1-sample proportions test without continuity correction\n## \n## data:  15 out of 125\n## X-squared = 2.7174, df = 1, p-value = 0.04963\n## alternative hypothesis: true p is greater than 0.08\n## 95 percent confidence interval:\n##  0.08007111 1.00000000\n## sample estimates:\n##    p \n## 0.12\nprop.test(x=15,n=125,p=.08,alternative = \"greater\",correct=TRUE)## \n##  1-sample proportions test with continuity correction\n## \n## data:  15 out of 125\n## X-squared = 2.2011, df = 1, p-value = 0.06896\n## alternative hypothesis: true p is greater than 0.08\n## 95 percent confidence interval:\n##  0.07682087 1.00000000\n## sample estimates:\n##    p \n## 0.12\nbinom.test(x=15,n=125,p=.08,alternative = \"greater\",ci.method = \"Wald\")## \n##  Exact binomial test (Wald CI)\n## \n## data:  15 out of 125\n## number of successes = 15, number of trials = 125, p-value = 0.07483\n## alternative hypothesis: true probability of success is greater than 0.08\n## 95 percent confidence interval:\n##  0.0721916 1.0000000\n## sample estimates:\n## probability of success \n##                   0.12\n15/125-qnorm(.95)*sqrt(.12*.88/125)## [1] 0.0721916"},{"path":"BOOT.html","id":"BOOT","chapter":"21 Bootstrap","heading":"21 Bootstrap","text":"","code":""},{"path":"BOOT.html","id":"objectives-19","chapter":"21 Bootstrap","heading":"21.1 Objectives","text":"Use bootstrap estimate standard error, standard deviation, sample statistic.Using bootstrap methods, obtain interpret confidence interval unknown parameter, based random sample.Describe advantages, disadvantages, assumptions behind using bootstrapping confidence intervals.","code":""},{"path":"BOOT.html","id":"homework-20","chapter":"21 Bootstrap","heading":"21.2 Homework","text":"","code":""},{"path":"BOOT.html","id":"problem-1-20","chapter":"21 Bootstrap","heading":"21.2.1 Problem 1","text":"PokerAn aspiring poker player recorded winnings losses 50 evenings play, data openintro package object poker. poker player like better understand volatility long term play.Load data plot histogram.Find summary statistics.Mean absolute deviation MAD intuitive measure spread variance. directly measures average distance mean. found formula:\n\\[mad = \\sum_{=1}^{n}\\frac{\\left| x_{} - \\bar{x} \\right|}{n}\\]\nWrite function find MAD data.Find bootstrap distribution MAD using 1000 replicates.Plot histogram bootstrap distribution.Report 95% confidence interval MAD.ADVANCED: think sample MAD unbiased estimator population MAD? ?don’t know without math. know sample standard deviation biased part use sample mean calculation. thing , estimate might also biased reason.","code":"\npoker<-read_csv(\"data/poker.csv\")\npoker %>%\n  gf_histogram(~winnings,fill=\"cyan\",color=\"black\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Winnings\")\nfavstats(~winnings,data=poker)##    min   Q1 median  Q3  max  mean       sd  n missing\n##  -1000 -187     11 289 3712 90.08 703.6835 50       0\nmad<-function(x){\n  xbar<-mean(x)\n  sum(abs(x-xbar))/length(x)\n}\nobs<-mad(poker$winnings)\nobs## [1] 394.1792\nset.seed(1122)\nresults<-do(1000)*mad(resample(poker$winnings))\nresults %>%\n  gf_histogram(~mad,fill=\"cyan\",color=\"black\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Mean absolute deviation\")\ncdata(~mad,data=results)##         lower    upper central.p\n## 2.5% 243.9448 636.0925      0.95"},{"path":"BOOT.html","id":"problem-2-20","chapter":"21 Bootstrap","heading":"21.2.2 Problem 2","text":"Bootstrap hypothesis testingBootstrap hypothesis testing relatively undeveloped, generally accurate permutation testing. Therefore general avoid . problem reading, may work. sample way consistent null hypothesis, calculate p-value tail probability like permutation tests. example generalize well applications like relative risk, correlation, regression, categorical data.Using HELPrct data set, store observed value difference means male female.going just select two columns need.null hypothesis requires means group equal. Pick one group adjust, either male female. First zero mean selected group subtracting sample mean group data points group. add sample mean group data point selected group. Store new object called HELP_null.tricky, data wrangling .Let’s get female observations adjust mean equal males.Combine back one data set.Run favstats() check means equal.new adjusted data set, generate bootstrap distribution difference sample means.Plot bootstrap distribution line observed difference sample means.Find p-value.p-value compare reading.similar p-value.","code":"\nHELP_sub <- HELPrct %>%\n  select(age,sex)\nobs <- diffmean(age~sex,data=HELP_sub)\nobs##   diffmean \n## -0.7841284\nmeans<-mean(age~sex,data=HELP_sub)\nmeans##   female     male \n## 36.25234 35.46821\nmeans['female']##   female \n## 36.25234\nH_female <- HELP_sub %>%\n  filter(sex==\"female\") %>%\n  mutate(age=age-means['female']+means['male'])\nmean(~age,data=H_female)## [1] 35.46821\nHELP_sub_new<-HELP_sub %>%\n  filter(sex==\"male\") %>%\n  rbind(H_female)\nfavstats(age~sex,data=HELP_sub_new)##      sex      min       Q1   median       Q3      max     mean       sd   n\n## 1 female 20.21587 30.21587 34.21587 39.71587 57.21587 35.46821 7.584858 107\n## 2   male 19.00000 30.00000 35.00000 40.00000 60.00000 35.46821 7.750110 346\n##   missing\n## 1       0\n## 2       0\nset.seed(1159)\nresults<-do(1000)*diffmean(age~sex,data=resample(HELP_sub_new))\nresults %>%\n  gf_histogram(~diffmean,fill=\"cyan\",color=\"black\") %>%\n  gf_vline(xintercept=obs) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Difference in means\")## Warning: geom_vline(): Ignoring `mapping` because `xintercept` was provided.\n2*prop1(~(diffmean<=obs),data=results)## prop_TRUE \n## 0.3476523"},{"path":"BOOT.html","id":"problem-3-15","chapter":"21 Bootstrap","heading":"21.2.3 Problem 3","text":"Paired dataAre textbooks actually cheaper online? compare price textbooks University California, Los Angeles’ (UCLA’s) bookstore prices Amazon.com. Seventy-three UCLA courses randomly sampled Spring 2010, representing less 10% UCLA courses. class multiple books, expensive text considered.data file textbooks.csv data folder.textbook two corresponding prices data set: one UCLA bookstore one Amazon. Therefore, textbook price UCLA bookstore natural correspondence textbook price Amazon. two sets observations special correspondence, said paired.analyze paired data, often useful look difference outcomes pair observations. textbooks, look difference prices, represented diff variable. important always subtract using consistent order; Amazon prices always subtracted UCLA prices.data tidy? Explain.Yes, row textbook column variable.Make scatterplot UCLA price versus Amazon price. Add 45 degree line plot.appears books UCLA bookstore expensive. One way test regression model; learn next block.Make histogram differences price.distribution skewed.hypotheses :\\(H_0\\): \\(\\mu_{diff}=0\\). difference average textbook price.\\(H_A\\): \\(\\mu_{diff} \\neq 0\\). difference average prices.use \\(t\\) distribution, variable diff independent normally distributed. Since 73 books represent less 10% population, assumption random sample independent reasonable. Check normality using qqnorsim() openintro package. generates 8 qq plots simulated normal data can use judge diff variable.normality assumption suspect large sample acceptable use \\(t\\).Run \\(t\\) test diff variable. Report p-value conclusion.use paired option since already took difference. example using paired option.p-value small don’t believe average price books UCLA bookstore Amazon .Create bootstrap distribution generate 95% confidence interval mean differences, diff column.need just pull difference.Next bootstrap distribution.bad solution problem.really differences book sources, variable binomial null probably success \\(\\pi = 0.5\\). Run hypothesis test using variable .45 books expensive total 73.Notice test failed reject null hypothesis. paired test, evidence strong binomial model . loss information making discrete variable continuous one.use permutation test example? Explain.Yes, careful want keep pairing can’t just shuffle names. shuffle names within paired values. means simply randomly switch names within row. easier just multiplying diff column random choice -1 1.None permuted values greater observed value.","code":"\ntextbooks<-read_csv(\"data/textbooks.csv\")## Rows: 73 Columns: 7## -- Column specification --------------------------------------------------------\n## Delimiter: \",\"\n## chr (4): dept_abbr, course, isbn, more\n## dbl (3): ucla_new, amaz_new, diff## \n## i Use `spec()` to retrieve the full column specification for this data.\n## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\nhead(textbooks)## # A tibble: 6 x 7\n##   dept_abbr course isbn           ucla_new amaz_new more   diff\n##   <chr>     <chr>  <chr>             <dbl>    <dbl> <chr> <dbl>\n## 1 Am Ind    C170   978-0803272620     27.7     28.0 Y     -0.28\n## 2 Anthro    9      978-0030119194     40.6     31.1 Y      9.45\n## 3 Anthro    135T   978-0300080643     31.7     32   Y     -0.32\n## 4 Anthro    191HB  978-0226206813     16       11.5 Y      4.48\n## 5 Art His   M102K  978-0892365999     19.0     14.2 Y      4.74\n## 6 Art His   118E   978-0394723693     15.0     10.2 Y      4.78\ntextbooks %>%\n  gf_point(ucla_new~amaz_new) %>%\n  gf_abline(slope=1,intercept = 0,color=\"darkblue\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(x=\"Amazon\",y=\"UCLA\")\ntextbooks %>%\n  gf_histogram(~diff,fill=\"cyan\",color=\"black\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title=\"Distribution of price differences\",\n          x=\"Price difference between UCLA and Amazon\")\nqqnormsim(diff,textbooks)\nt_test(~diff,textbooks)## \n##  One Sample t-test\n## \n## data:  diff\n## t = 7.6488, df = 72, p-value = 6.928e-11\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##   9.435636 16.087652\n## sample estimates:\n## mean of x \n##  12.76164\nt_test(textbooks$ucla_new,textbooks$amaz_new,paired=TRUE)## \n##  Paired t-test\n## \n## data:  textbooks$ucla_new and textbooks$amaz_new\n## t = 7.6488, df = 72, p-value = 6.928e-11\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##   9.435636 16.087652\n## sample estimates:\n## mean of the differences \n##                12.76164\ntextbooks %>%\n  summarise(obs_diff=mean(diff))## # A tibble: 1 x 1\n##   obs_diff\n##      <dbl>\n## 1     12.8\nobs_stat<- textbooks %>%\n  summarise(obs_diff=mean(diff)) %>%\n  pull(obs_diff)\n\nobs_stat## [1] 12.76164\nset.seed(843)\nresults<-do(1000)*mean(~diff,data=resample(textbooks))\nresults %>%\n  gf_dhistogram(~mean,fill=\"cyan\",color=\"black\") %>%\n  gf_dist(\"norm\",mean=12.76,sd=14/sqrt(72),color=\"red\") %>%\n  gf_vline(xintercept = obs_stat) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title=\"Sampling distribution of the mean of differences in price\",\n          x=\"Mean of differences in price\")\ncdata(~mean,data=results)##         lower    upper central.p\n## 2.5% 9.583829 16.05705      0.95\ninspect(textbooks)## \n## categorical variables:  \n##        name     class levels  n missing\n## 1 dept_abbr character     41 73       0\n## 2    course character     66 73       0\n## 3      isbn character     73 73       0\n## 4      more character      2 73       0\n##                                    distribution\n## 1 Mgmt (8.2%), Pol Sci (6.8%) ...              \n## 2 10 (4.1%), 101 (2.7%), 180 (2.7%) ...        \n## 3 978-0030119194 (1.4%) ...                    \n## 4 Y (61.6%), N (38.4%)                         \n## \n## quantitative variables:  \n##          name   class   min    Q1 median     Q3   max     mean       sd  n\n## ...1 ucla_new numeric 10.50 24.70  43.56 116.00 214.5 72.22192 59.65913 73\n## ...2 amaz_new numeric  8.60 20.21  34.95  88.09 176.0 59.46027 48.99557 73\n## ...3     diff numeric -9.53  3.80   8.23  17.59  66.0 12.76164 14.25530 73\n##      missing\n## ...1       0\n## ...2       0\n## ...3       0\nprop_test(45,73,p=0.5)## \n##  1-sample proportions test with continuity correction\n## \n## data:  45 out of 73\n## X-squared = 3.5068, df = 1, p-value = 0.06112\n## alternative hypothesis: true p is not equal to 0.5\n## 95 percent confidence interval:\n##  0.4948968 0.7256421\n## sample estimates:\n##         p \n## 0.6164384\nsample(c(-1,1),size=73,replace = TRUE)##  [1]  1 -1 -1  1 -1 -1 -1  1 -1 -1  1 -1 -1  1 -1  1  1  1 -1 -1 -1 -1  1 -1 -1\n## [26] -1 -1  1  1  1  1  1 -1  1  1  1  1  1  1 -1 -1  1  1 -1  1  1  1  1 -1 -1\n## [51] -1 -1 -1  1 -1  1 -1 -1  1  1 -1  1  1 -1 -1 -1  1  1 -1  1 -1  1  1\nset.seed(406)\nresults <- do(1000)*mean((~diff*sample(c(-1,1),size=73,replace = TRUE)),data=textbooks)\nresults %>%\n  gf_histogram(~mean,fill=\"cyan\",color=\"black\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title=\"Randomization sampling distribution of mean of differences in price\",\n          x=\"Mean of price difference\")\nprop1((~mean>=obs_stat),data=results)##   prop_TRUE \n## 0.000999001"},{"path":"ADDTESTS.html","id":"ADDTESTS","chapter":"22 Additional Hypothesis Tests","heading":"22 Additional Hypothesis Tests","text":"","code":""},{"path":"ADDTESTS.html","id":"objectives-20","chapter":"22 Additional Hypothesis Tests","heading":"22.1 Objectives","text":"Conduct interpret hypothesis test equality two means using permutation \\(F\\) distribution.Conduct interpret goodness fit test using Pearson’s chi-squared randomization evaluate independence two categorical variables.Conduct interpret hypothesis test equality two variances.Know check assumptions tests lesson.","code":""},{"path":"ADDTESTS.html","id":"homework-21","chapter":"22 Additional Hypothesis Tests","heading":"22.2 Homework","text":"","code":""},{"path":"ADDTESTS.html","id":"problem-1-21","chapter":"22 Additional Hypothesis Tests","heading":"22.2.1 Problem 1","text":"Golf ballsRepeat analysis golf ball problem earlier semester.Load data tally data table. data golf_balls.csv.Using function chisq.test conduct hypothesis test equally likely distribution balls. may read help menu.Repeat part b. assume balls numbers 1 2 occur 30% time balls 3 4 occur 20%.","code":"\ngolf_balls <- read_csv(\"data/golf_balls.csv\")\nhead(golf_balls)## # A tibble: 6 x 1\n##   number\n##    <dbl>\n## 1      3\n## 2      2\n## 3      1\n## 4      4\n## 5      4\n## 6      3\ntally(~number,data=golf_balls)## number\n##   1   2   3   4 \n## 137 138 107 104\nchisq.test(tally(~number,data=golf_balls),p=c(.25,.25,.25,.25))## \n##  Chi-squared test for given probabilities\n## \n## data:  tally(~number, data = golf_balls)\n## X-squared = 8.4691, df = 3, p-value = 0.03725\nchisq.test(tally(~number,data=golf_balls),p=c(.3,.3,.2,.2))## \n##  Chi-squared test for given probabilities\n## \n## data:  tally(~number, data = golf_balls)\n## X-squared = 2.4122, df = 3, p-value = 0.4914"},{"path":"ADDTESTS.html","id":"problem-2-21","chapter":"22 Additional Hypothesis Tests","heading":"22.2.2 Problem 2","text":"Bootstrap hypothesis testingRepeat analysis MLB data lesson time generate bootstrap distribution \\(F\\) statistic.First, read data.Convert position factor.Summarize data.need function resample data, use resample() mosaic package.Let’s plot sampling distribution.Now confidence interval F-statistic :95% confident \\(F\\) statistic interval \\((0.35,8.72)\\) includes 1 fail reject null hypothesis equal means. Remember null hypothesis ratio variance means pooled variance within categories 1.","code":"\nmlb_obp <- read_csv(\"data/mlb_obp.csv\")\nmlb_obp <- mlb_obp %>%\n  mutate(position=as.factor(position))\nfavstats(obp~position,data=mlb_obp)##   position   min      Q1 median      Q3   max      mean         sd   n missing\n## 1        C 0.219 0.30000 0.3180 0.35700 0.405 0.3226154 0.04513175  39       0\n## 2       DH 0.287 0.31625 0.3525 0.36950 0.412 0.3477857 0.03603669  14       0\n## 3       IF 0.174 0.30800 0.3270 0.35275 0.437 0.3315260 0.03709504 154       0\n## 4       OF 0.265 0.31475 0.3345 0.35300 0.411 0.3342500 0.02944394 120       0\nlibrary(broom)\nf_boot <- function(x){\n  aov(obp~position,data=resample(x)) %>%\n  tidy() %>%\n  summarize(stat=meansq[1]/meansq[2]) %>%\n  pull()\n}\nset.seed(541)\nresults<-do(1000)*f_boot(mlb_obp)\nresults %>%\n  gf_histogram(~f_boot,fill=\"cyan\",color=\"black\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title=\"Bootstrap sampling distribution of F test statistic\",\n          x=\"Test statistic\")\ncdata(~f_boot,data=results)##          lower    upper central.p\n## 2.5% 0.3546682 8.724895      0.95"},{"path":"ADDTESTS.html","id":"problem-3-16","chapter":"22 Additional Hypothesis Tests","heading":"22.2.3 Problem 3","text":"Test varianceWe performed test variance create .Using MLB lesson, subset .function droplevels() gets rid C DH factor levels.Create side--side boxplot.hypotheses :\\(H_0\\): \\(\\sigma^2_{}=\\sigma^2{}\\). difference variance base percentage infielders outfielders.\\(H_A\\): \\(\\sigma^2_{}\\neq \\sigma^2_{}\\). difference variances.Use differences sample standard deviations test statistic. Using permutation test, find p-value discuss decision.Let’s write function shuffle position.p-value isThis two sided test since know advance variance larger. reject hypothesis equal variance p-value close significance level. conclusion suspect. need data.Create bootstrap distribution differences sample standard deviations, report 95% confidence interval. Compare part c. Let’s write function.","code":"\nmlb_prob3 <- mlb_obp %>%\n  filter(position==\"IF\"|position==\"OF\") %>%\n  droplevels()\nsummary(mlb_prob3)##  position      obp        \n##  IF:154   Min.   :0.1740  \n##  OF:120   1st Qu.:0.3100  \n##           Median :0.3310  \n##           Mean   :0.3327  \n##           3rd Qu.:0.3530  \n##           Max.   :0.4370\nmlb_prob3 %>%\n  gf_boxplot(obp~position) %>%\n  gf_theme(theme_classic())\nmlb_prob3 %>%\n  group_by(position) %>%\n  summarize(stat=sd(obp))## # A tibble: 2 x 2\n##   position   stat\n##   <fct>     <dbl>\n## 1 IF       0.0371\n## 2 OF       0.0294\nobs <- mlb_prob3 %>%\n  summarize(stat=sd(obp[position==\"IF\"])-sd(obp[position==\"OF\"])) %>%\n  pull()\nobs## [1] 0.007651101\nperm_stat <- function(x){\n  x %>% \n  mutate(position=shuffle(position)) %>%\n  summarize(stat=sd(obp[position==\"IF\"])-sd(obp[position==\"OF\"])) %>%\n  pull()\n}\nset.seed(443)\nresults<-do(1000)*perm_stat(mlb_prob3)\nresults %>% \n  gf_histogram(~perm_stat,fill=\"cyan\",color=\"black\") %>%\n  gf_vline(xintercept=obs,color=\"red\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title=\"Sampling distribution of difference in variances\",\n          subtitle=\"Randomization permutation test\",\n          x=\"Test statistic\")\n2*prop1(~(perm_stat>=obs),data=results)##  prop_TRUE \n## 0.04395604\nvar_stat <- function(x){\n  resample(x) %>%\n  summarize(stat=sd(obp[position==\"IF\"])-sd(obp[position==\"OF\"])) %>%\n  pull()\n}\nset.seed(827)\nresults<-do(1000)*var_stat(mlb_prob3)\nresults %>% \n  gf_histogram(~var_stat,fill=\"cyan\",color=\"black\") %>%\n  gf_vline(xintercept=obs,color=\"red\")%>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title=\"Bootstrap sampling of difference in variances\",\n          x=\"Difference in variances\")"},{"path":"CS4.html","id":"CS4","chapter":"23 Case Study","heading":"23 Case Study","text":"","code":""},{"path":"CS4.html","id":"objectives-21","chapter":"23 Case Study","heading":"23.1 Objectives","text":"Using R, generate linear regression model use produce prediction model.Using plots, check assumptions linear regression model.","code":""},{"path":"CS4.html","id":"homework-22","chapter":"23 Case Study","heading":"23.2 Homework","text":"","code":""},{"path":"CS4.html","id":"problem-1-22","chapter":"23 Case Study","heading":"23.2.1 Problem 1","text":"HFIChoose another freedom variable variable think strongly correlate . Note: even though variables appear quantitative, don’t take enough different values thus appear categorical. choose caution. openintro package contains data set hfi. Type ?openintro::hfi Console window RStudio learn variables.Produce scatterplot two variables.selected pf_expression_influence measure laws regulations influence media content. kept pf_score measure personal freedom country. thought correlated.Quantify strength relationship correlation coefficient.Fit linear model. glance, seem linear relationship?relationship compare relationship \npf_expression_control pf_score? Use \\(R^2\\) values two\nmodel summaries compare. independent variable seem predict\ndependent one better? ?\nadjusted \\(R^2\\) little smaller fit good.relationship compare relationship \npf_expression_control pf_score? Use \\(R^2\\) values two\nmodel summaries compare. independent variable seem predict\ndependent one better? ?adjusted \\(R^2\\) little smaller fit good.Display model diagnostics regression model analyzing relationship.Display model diagnostics regression model analyzing relationship.Linearity:\nFigure 23.1: Fitted values versus residuals diagnostics.\nappear type fluctuation linear model may appropriate.Nearly normal residuals:normal probability plot residuals., sample small appears residual skewed left.Constant variability:Based Figure 23.1, width plot seems constant exception extreme points. constant variability assumption seems reasonable.Predict response explanatory variable value median third quartile. overestimate underestimate, much?thus predict value 7.53 pf_score.observed value 7.96, average 42 data points. tend underestimate observed value.","code":"\nhfi<-read_csv(\"data/hfi.csv\")\ngf_lm(pf_score~pf_expression_influence,data=hfi,color=\"black\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_point(alpha=0.3) %>%\n  gf_labs(title=\"Personal freedom score versus Control on media\",\n          x=\"Laws and regulations that influence media content\",\n          y=\"Personal freedom score\")\nhfi %>%\n  summarise(cor(pf_expression_influence, pf_score, use = \"complete.obs\"))## # A tibble: 1 x 1\n##   `cor(pf_expression_influence, pf_score, use = \"complete.obs\")`\n##                                                            <dbl>\n## 1                                                          0.787\nm2 <- lm(pf_score ~ pf_expression_influence, data = hfi)\nsummary(m2)## \n## Call:\n## lm(formula = pf_score ~ pf_expression_influence, data = hfi)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.9688 -0.5830  0.1681  0.5903  3.6730 \n## \n## Coefficients:\n##                         Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)              5.06135    0.05064   99.95   <2e-16 ***\n## pf_expression_influence  0.41150    0.00869   47.36   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8482 on 1376 degrees of freedom\n##   (80 observations deleted due to missingness)\n## Multiple R-squared:  0.6197, Adjusted R-squared:  0.6195 \n## F-statistic:  2243 on 1 and 1376 DF,  p-value: < 2.2e-16\nggplot(data = m2, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x=\"Fitted values\",y=\"Residuals\",title=\"Residual analysis\") +\n  theme_bw()\nggplot(data = m2, aes(x = .resid)) +\n  geom_histogram(binwidth = .4,fill=\"cyan\",color=\"black\") +\n  xlab(\"Residuals\") +\n  theme_bw()\nggplot(data = m2, aes(sample = .resid)) +\n  stat_qq() +\n  theme_bw() +\n  geom_abline(slope=1,intercept = 0)\nsummary(hfi$pf_expression_influence)##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##   0.000   3.000   5.333   5.200   7.333   9.667      80\npredict(m2,newdata=data.frame(pf_expression_influence=6))##       1 \n## 7.53036\nlibrary(broom)\naugment(m2) %>%\n   filter(pf_expression_influence==6) %>%\n    summarize(ave=mean(pf_score),n=n())## # A tibble: 1 x 2\n##     ave     n\n##   <dbl> <int>\n## 1  7.96    42"},{"path":"LRBASICS.html","id":"LRBASICS","chapter":"24 Linear Regression Basics","heading":"24 Linear Regression Basics","text":"","code":""},{"path":"LRBASICS.html","id":"objectives-22","chapter":"24 Linear Regression Basics","heading":"24.1 Objectives","text":"Obtain parameter estimates simple linear regression model given sample data.Interpret coefficients simple linear regression.Create scatterplot regression line.Explain check assumptions linear regression.Use able explain new terms.","code":""},{"path":"LRBASICS.html","id":"homework-23","chapter":"24 Linear Regression Basics","heading":"24.2 Homework","text":"","code":""},{"path":"LRBASICS.html","id":"problem-1-23","chapter":"24 Linear Regression Basics","heading":"24.2.1 Problem 1","text":"Nutrition StarbucksIn data folder file named starbucks.csv. Use answer questions .Create scatterplot number calories amount carbohydrates.put calories response.Describe relationship graph.positive, moderate, linear association number calories amount carbohydrates. addition, amount carbohydrates variable menu items higher calories, indicating non-constant variance. also appear two clusters data: patch dozen observations lower left \nlarger patch right side. might natural groupings points. example, points lower left might come light menu.scenario, explanatory response variables?Response: number calories. Explanatory: amount carbohydrates (grams).might want fit regression line data?regression line, can predict amount calories given number carbohydrates. may useful concerned carb intake impact calorie consumption. Typically can get menu model might valuable.Create scatterplot number calories amount carbohydrates regression line included.Using ’lm()` fit least squares line data.Report interpret slope coefficient.estimated slope 4.297 one additional gram carbohydrates results average increase calories 4.297.menu item 51 g carbs, estimated calorie count?use model menu item 100 g carbs?maximum carb value 80 100 outside observed data. suspect extrapolate value.assumption constant variance seem reasonable problem?going use broom package get residuals corresponding independent variable values. also get residuals model object independent variable values original dataframe.seems variance second group larger first, may reasonable assumption. Also note linearity assumption also questionable.Verify line passes mean carb mean calories, mathematically.checks.estimate standard deviation residuals? use information?estimate 78.26. normal assumption accurate, expect majority observations within \\(\\pm\\) 78.26 calories line.","code":"\nstarbucks <- read_csv(\"data/starbucks.csv\")## Rows: 77 Columns: 7## -- Column specification --------------------------------------------------------\n## Delimiter: \",\"\n## chr (2): item, type\n## dbl (5): calories, fat, carb, fiber, protein## \n## i Use `spec()` to retrieve the full column specification for this data.\n## i Specify the column types or set `show_col_types = FALSE` to quiet this message.\nglimpse(starbucks)## Rows: 77\n## Columns: 7\n## $ item     <chr> \"8-Grain Roll\", \"Apple Bran Muffin\", \"Apple Fritter\", \"Banana~\n## $ calories <dbl> 350, 350, 420, 490, 130, 370, 460, 370, 310, 420, 380, 320, 3~\n## $ fat      <dbl> 8, 9, 20, 19, 6, 14, 22, 14, 18, 25, 17, 12, 17, 21, 5, 18, 1~\n## $ carb     <dbl> 67, 64, 59, 75, 17, 47, 61, 55, 32, 39, 51, 53, 34, 57, 52, 7~\n## $ fiber    <dbl> 5, 7, 0, 4, 0, 5, 2, 0, 0, 0, 2, 3, 2, 2, 3, 3, 2, 3, 0, 2, 0~\n## $ protein  <dbl> 10, 6, 5, 7, 0, 6, 7, 6, 5, 7, 4, 6, 5, 5, 12, 7, 8, 6, 0, 10~\n## $ type     <chr> \"bakery\", \"bakery\", \"bakery\", \"bakery\", \"bakery\", \"bakery\", \"~\nstarbucks %>%\n  gf_point(calories~carb) %>%\n  gf_labs(x=\"Carbohydrate Content (g)\",y=\"Calories\") %>%\n  gf_theme(theme_classic())\nstarbucks %>%\n  gf_point(calories~carb) %>%\n  gf_labs(x=\"Carbohydrate Content (g)\",y=\"Calories\") %>%\n  gf_lm() %>%\n  gf_theme(theme_classic())\nstar_mod <- lm(calories~carb,data=starbucks)\nsummary(star_mod)## \n## Call:\n## lm(formula = calories ~ carb, data = starbucks)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -151.962  -70.556   -0.636   54.908  179.444 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 146.0204    25.9186   5.634 2.93e-07 ***\n## carb          4.2971     0.5424   7.923 1.67e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 78.26 on 75 degrees of freedom\n## Multiple R-squared:  0.4556, Adjusted R-squared:  0.4484 \n## F-statistic: 62.77 on 1 and 75 DF,  p-value: 1.673e-11\n146.0204+4.2971*51## [1] 365.1725\nsummary(starbucks)##      item              calories          fat             carb      \n##  Length:77          Min.   : 80.0   Min.   : 0.00   Min.   :16.00  \n##  Class :character   1st Qu.:300.0   1st Qu.: 9.00   1st Qu.:31.00  \n##  Mode  :character   Median :350.0   Median :13.00   Median :45.00  \n##                     Mean   :338.8   Mean   :13.77   Mean   :44.87  \n##                     3rd Qu.:420.0   3rd Qu.:18.00   3rd Qu.:59.00  \n##                     Max.   :500.0   Max.   :28.00   Max.   :80.00  \n##      fiber          protein           type          \n##  Min.   :0.000   Min.   : 0.000   Length:77         \n##  1st Qu.:0.000   1st Qu.: 5.000   Class :character  \n##  Median :2.000   Median : 7.000   Mode  :character  \n##  Mean   :2.221   Mean   : 9.481                     \n##  3rd Qu.:4.000   3rd Qu.:15.000                     \n##  Max.   :7.000   Max.   :34.000\nlibrary(broom)\naugment(star_mod) %>%\n  gf_point(.resid~carb) %>%\n  gf_hline(yintercept = 0) %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(title=\"Residual plot\",x=\"Carbohydrates\",y=\"Residual\")\n146.0204+4.2971*44.87## [1] 338.8313"},{"path":"LRINF.html","id":"LRINF","chapter":"25 Linear Regression Inference","heading":"25 Linear Regression Inference","text":"","code":""},{"path":"LRINF.html","id":"objectives-23","chapter":"25 Linear Regression Inference","heading":"25.1 Objectives","text":"Given simple linear regression model, conduct inference coefficients \\(\\beta_0\\) \\(\\beta_1\\).Given simple linear regression model, calculate predicted response given value predictor.Build interpret confidence prediction intervals values response variable.","code":""},{"path":"LRINF.html","id":"homework-24","chapter":"25 Linear Regression Inference","heading":"25.2 Homework","text":"","code":""},{"path":"LRINF.html","id":"problem-1-24","chapter":"25 Linear Regression Inference","heading":"25.2.1 Problem 1","text":"noticed 95% prediction interval much wider 95% confidence interval. words, explain .two intervals describing different parameters. 95% confidence interval describing mean value response particular value predictor. hand, 95% prediction interval describing individual value response particular value predictor. uncertainty around individual value overall mean.","code":""},{"path":"LRINF.html","id":"problem-2-22","chapter":"25 Linear Regression Inference","heading":"25.2.2 Problem 2","text":"Beer blood alcohol contentMany people believe gender, weight, drinking habits, many factors much important predicting blood alcohol content (BAC) simply considering number drinks person consumed. examine data sixteen student volunteers Ohio State University drank randomly assigned number cans beer. students evenly divided men women, differed weight drinking habits. Thirty minutes later, police officer measured blood alcohol content (BAC) grams alcohol per deciliter blood. data bac.csv file data folder.Create scatterplot cans beer blood alcohol level.put BAC response since natural predict BAC number cans beer consumed. Also notice number cans beers really discrete variable can whole numbers.Describe relationship number cans beer BAC.relationship appears strong, positive linear. one potential outlier, student 9 cans beer. discuss outliers next lesson.Write equation regression line. Interpret slope intercept context.\\[\\text{BAC} = -0.0127 + 0.0180 \\times \\text{beers} \\]\nSlope: additional can beer consumed, model predicts additional 0.0180 grams per deciliter BAC average.Intercept: Students don’t beer expected blood alcohol content -0.0127. value interpreted much BAC drops time drinking beers BAC measured. also intercept really zero estimate different variability estimator.data provide strong evidence drinking cans beer associated increase blood alcohol? State null alternative hypotheses, report p-value, state conclusion.\\(H_0\\): true slope coefficient number beers zero (\\(\\beta_1 = 0\\)).\\(H_a\\): true slope coefficient number beers greater zero (\\(\\beta_1 > 0\\)).p-value two-sided alternative hypothesis (\\(\\beta_1 \\neq 0\\)) approximately 0. (Note output doesn’t mean p-value exactly zero, rounded four decimal places zero.) Therefore p-value one-sided hypothesis also small, one half p-value reported R output. small p-value, reject \\(H_0\\) conclude data provide convincing evidence number cans beer consumed blood alcohol content positively correlated true slope parameter indeed greater 0.Build 95% confidence interval slope interpret context hypothesis test part d. need lower confidence bound since alternative \\(\\beta_1 > 0\\). 95% lower confidence bound, need 90% confidence interval just ignore upper value.95% confident true value slope greater 0.014. bound contain 0, appears number beers BAC linearly related positive direction. Notice confidence interval intercept include zero discussed .Suppose visit bar, ask people many drinks , also take BAC. think relationship number drinks BAC strong relationship found Ohio State study?probably weaker. study people similar ages, also identical drinks. bars elsewhere, drinks vary widely amount alcohol contain.Predict average BAC two beers build 90% confidence interval around prediction.Repeat except build 90% prediction interval interpret.90% confident BAC student two beers -.016 0.062. Notice interval contains unrealistic level BAC lower limit. briefing make sure note . using model based normal distribution. bootstrap may problem. truncate lower level 0, careful claiming 90% coverage.Plot data points regression line, confidence band, prediction band.","code":"\nbac <- read_csv(\"data/bac.csv\")\nbac %>%\n  gf_point(bac~beers) %>%\n  gf_labs(x=\"Number of cans of beer\",y=\"BAC\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_refine(scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9)))\nbac_mod <- lm(bac~beers,data=bac)\nsummary(bac_mod)## \n## Call:\n## lm(formula = bac ~ beers, data = bac)\n## \n## Residuals:\n##       Min        1Q    Median        3Q       Max \n## -0.027118 -0.017350  0.001773  0.008623  0.041027 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.012701   0.012638  -1.005    0.332    \n## beers        0.017964   0.002402   7.480 2.97e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.02044 on 14 degrees of freedom\n## Multiple R-squared:  0.7998, Adjusted R-squared:  0.7855 \n## F-statistic: 55.94 on 1 and 14 DF,  p-value: 2.969e-06\nconfint(bac_mod,level=0.9)##                     5 %        95 %\n## (Intercept) -0.03495916 0.009557957\n## beers        0.01373362 0.022193906\nnew_bac <- data.frame(beers=2)\npredict(bac_mod, newdata = new_bac, interval = 'confidence',level=0.9)##          fit         lwr       upr\n## 1 0.02322692 0.008308537 0.0381453\npredict(bac_mod, newdata = new_bac, interval = 'prediction',level=0.9)##          fit        lwr        upr\n## 1 0.02322692 -0.0157444 0.06219824\nbac %>%\n  gf_point(bac~beers) %>%\n  gf_labs(x=\"Number of cans of beers\",y=\"BAC\") %>%\n  gf_lm(stat=\"lm\",interval=\"confidence\") %>%\n  gf_lm(stat=\"lm\",interval=\"prediction\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_refine(scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9)))"},{"path":"LRINF.html","id":"problem-3-17","chapter":"25 Linear Regression Inference","heading":"25.2.3 Problem 3","text":"Suppose build regression fitting response variable one predictor variable. build 95% confidence interval \\(\\beta_1\\) find contains 0, meaning slope 0 feasible. mean response predictor independent?. merely means best guess two variables linearly uncorrelated. related another way (quadratically, example), still result estimated slope close 0.","code":""},{"path":"LRDIAG.html","id":"LRDIAG","chapter":"26 Regression Diagnostics","heading":"26 Regression Diagnostics","text":"","code":""},{"path":"LRDIAG.html","id":"objectives-24","chapter":"26 Regression Diagnostics","heading":"26.1 Objectives","text":"Obtain interpret \\(R\\)-squared \\(F\\)-statistic.Use R evaluate assumptions linear model.Identify explain outliers leverage points.","code":""},{"path":"LRDIAG.html","id":"homework-25","chapter":"26 Regression Diagnostics","heading":"26.2 Homework","text":"","code":""},{"path":"LRDIAG.html","id":"problem-1-25","chapter":"26 Regression Diagnostics","heading":"26.2.1 Problem 1","text":"Identify relationshipsFor six plots, identify strength relationship (e.g. weak, moderate, strong) data whether fitting linear model reasonable. ask strength relationship, mean:relationship \\(x\\) \\(y\\) anddoes relationship explain variance?\nFigure 26.1: Homework problem 1.\nStrong relationship, straight line fit data.Strong relationship, linear fit reasonable.Weak relationship, trying linear fit reasonable.Moderate relationship, straight line fit data.Strong relationship, linear fit reasonable.Weak relationship change \\(x\\) cause change \\(y\\) even though points cluster around horizontal line. Trying linear fit reasonable.","code":""},{"path":"LRDIAG.html","id":"problem-2-23","chapter":"26 Regression Diagnostics","heading":"26.2.2 Problem 2","text":"Beer blood alcohol contentWe use blood alcohol content data . reminder description data: Many people believe gender, weight, drinking habits, many factors much important predicting blood alcohol content (BAC) simply considering number drinks person consumed. examine data sixteen student volunteers Ohio State University drank randomly assigned number cans beer. students evenly divided men women, differed weight drinking habits. Thirty minutes later, police officer measured blood alcohol content (BAC) grams alcohol per deciliter blood.data bac.csv file data folder.Obtain interpret \\(R\\)-squared model.\\(R\\)-squared 0.7998, means almost 80% variance blood alcohol content explained number beers consumed. surprising. remaining variance may due measurement errors, differences students, environmental impacts.Evaluate assumptions model. anything concerned ?fit pretty good, one data point outlier, student drank 9 beers.3rd, 7th, 10th data points stand . data set small make decisions normality. suspect though.higher variance higher number beers. appears caused small number data points influence observation number 3.points ’re looking (looking ) values upper right lower right corners, outside red dashed Cook’s distance line. points influential model removing likely noticeably alter regression results. Now see observation 3 extreme leverage model. Removing potentially drastically alter model.learn measures influence, see https://cran.r-project.org/web/packages/olsrr/vignettes/influence_measures.html","code":"\nbac<-read_csv(\"data/bac.csv\")\nbac %>%\n  gf_point(bac~beers) %>%\n  gf_labs(x=\"Number of cans of beer\",y=\"BAC\") %>%\n  gf_theme(theme_classic()) %>%\n  gf_refine(scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9)))\nbac_mod <- lm(bac~beers,data=bac)\nsummary(bac_mod)## \n## Call:\n## lm(formula = bac ~ beers, data = bac)\n## \n## Residuals:\n##       Min        1Q    Median        3Q       Max \n## -0.027118 -0.017350  0.001773  0.008623  0.041027 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.012701   0.012638  -1.005    0.332    \n## beers        0.017964   0.002402   7.480 2.97e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.02044 on 14 degrees of freedom\n## Multiple R-squared:  0.7998, Adjusted R-squared:  0.7855 \n## F-statistic: 55.94 on 1 and 14 DF,  p-value: 2.969e-06\nplot(bac_mod,1)\nplot(bac_mod,2)\nbac## # A tibble: 16 x 3\n##    student beers   bac\n##      <int> <int> <dbl>\n##  1       1     5 0.1  \n##  2       2     2 0.03 \n##  3       3     9 0.19 \n##  4       4     8 0.12 \n##  5       5     3 0.04 \n##  6       6     7 0.095\n##  7       7     3 0.07 \n##  8       8     5 0.06 \n##  9       9     3 0.02 \n## 10      10     5 0.05 \n## 11      11     4 0.07 \n## 12      12     6 0.1  \n## 13      13     5 0.085\n## 14      14     7 0.09 \n## 15      15     1 0.01 \n## 16      16     4 0.05\nplot(bac_mod,3)\nplot(bac_mod,5)"},{"path":"LRDIAG.html","id":"problem-3-18","chapter":"26 Regression Diagnostics","heading":"26.2.3 Problem 3","text":"OutliersIdentify outliers scatterplots shown determine type outliers . Explain reasoning. labels treat bottom row (d), (e), (f).\nFigure 26.2: Homework problem 3.\noutlier located bottom right corner plot. exclude point analysis, slope regression line notably affected, means high-leverage influential point.outlier located bottom right corner plot. point high leverage since horizontally away center data, influential since regression line change little removed. also outlier response small residual.outlier located center top plot. Though point unlike rest data, high-leverage point since far x-axis center data. also means influential point since presence little influence slope regression line.outlier upper-left corner. Since horizontally far center data, high leverage point. Additionally, since fit regression line greatly influenced point, influential point.outlier located lower-left corner. horizontally far rest data, high-leverage point. regression line also fall relatively far point fit excluded point, meaning outlier influential.outlier upper-middle plot. Since near horizontal center data, high-leverage point. means also little influence slope regression line.","code":""},{"path":"LRSIM.html","id":"LRSIM","chapter":"27 Simulation Based Linear Regression","heading":"27 Simulation Based Linear Regression","text":"","code":""},{"path":"LRSIM.html","id":"objectives-25","chapter":"27 Simulation Based Linear Regression","heading":"27.1 Objectives","text":"Using bootstrap, generate confidence estimates standard error parameter estimates linear regression model.Generate interpret bootstrap confidence intervals predicted values.Generate bootstrap samples sampling rows data sampling residuals. Explain might prefer one .Interpret regression coefficients linear model categorical explanatory variable.","code":""},{"path":"LRSIM.html","id":"homework-26","chapter":"27 Simulation Based Linear Regression","heading":"27.2 Homework","text":"use loans data set create linear models. Remember data set represents thousands loans made Lending Club platform, platform allows individuals lend individuals.","code":""},{"path":"LRSIM.html","id":"problem-1-26","chapter":"27 Simulation Based Linear Regression","heading":"27.2.1 Problem 1","text":"LoansIn exercise examine relationship interest rate loan amount.Read data loans.csv data folder.Create subset data 200 following three variables interest_rate, loan_amount, term. Change term factor use stratified sample keep proportion loan term roughly original data.Plot interest_rate versus loan_amount. think interest_rate response.seems natural want predict interest rate loan amount.Fit linear model data regressing interest_rate loan_amount. significant relationship interest_rate loan_amount?test significant relationship interest_rate loan_amount, test \\(\\beta_1 = 0\\). p-value 0.4613, fail reject relationship interest_rate loan_amount.Using \\(t\\) distribution:\nFind 95% confidence interval slope.\nFind interpret 90% confidence interval loan amount $20000.\nFind 95% confidence interval slope.Find interpret 90% confidence interval loan amount $20000.95% confident true slope -4.592357e-05 1.008748e-04.90% confident average interest rate loan $20000 11.5% 12.9%.Repeat part e using bootstrap.using infer package:Now confidence interval average interest rate loan amount 20000:, close slightly different. Maybe assumptions normality appropriate.Check assumptions linear regression.appears lack normality residuals skewed right, large positive residuals. bootstrap probably appropriate problem.","code":"\nloans <- read_csv(\"data/loans.csv\")\ntally(~term,data=loans,format=\"percent\")## term\n##   36   60 \n## 69.7 30.3\nset.seed(2111)\nloans200 <- loans %>%\n  select(interest_rate,loan_amount,term) %>%\n  mutate(term=factor(term)) %>%\n  group_by(term) %>%\n  slice_sample(prop=0.02) %>%\n  ungroup()\ntally(~term,data=loans200,format=\"percent\")## term\n##       36       60 \n## 69.84925 30.15075\nstr(loans200)## tibble [199 x 3] (S3: tbl_df/tbl/data.frame)\n##  $ interest_rate: num [1:199] 13.59 9.92 17.47 10.9 7.34 ...\n##  $ loan_amount  : num [1:199] 13000 10000 10000 8400 4800 10000 6000 6300 10000 32000 ...\n##  $ term         : Factor w/ 2 levels \"36\",\"60\": 1 1 1 1 1 1 1 1 1 1 ...\nggplot(loans200,aes(x=loan_amount,y=interest_rate)) +\n  geom_point() +\n  labs(title=\"Lending Club\",subtitle=\"Loan amount versus Interest rate\",\n       x=\"Loan Amount\",y=\"Interest rate (percent)\") +\n  theme_bw()\nint_rate_mod <- lm(interest_rate~loan_amount,data=loans200)\nsummary(int_rate_mod)## \n## Call:\n## lm(formula = interest_rate ~ loan_amount, data = loans200)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.1611 -4.6842 -0.9666  3.0685 18.6280 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 1.165e+01  7.337e-01  15.874   <2e-16 ***\n## loan_amount 2.748e-05  3.722e-05   0.738    0.461    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5.423 on 197 degrees of freedom\n## Multiple R-squared:  0.002759,   Adjusted R-squared:  -0.002303 \n## F-statistic: 0.545 on 1 and 197 DF,  p-value: 0.4613\nconfint(int_rate_mod)##                     2.5 %       97.5 %\n## (Intercept)  1.019986e+01 1.309374e+01\n## loan_amount -4.592357e-05 1.008748e-04\npredict(int_rate_mod,newdata = data.frame(loan_amount=20000),\n        interval = \"confidence\",level=0.90)##        fit      lwr      upr\n## 1 12.19631 11.53105 12.86157\nset.seed(3011)\nresults <- do(1000)*lm(interest_rate ~ loan_amount,data=resample(loans200))\nhead(results)##   Intercept   loan_amount    sigma    r.squared          F numdf dendf .row\n## 1  10.26651  6.837311e-05 4.893962 0.0184148594 3.69578466     1   197    1\n## 2  11.63858  2.341492e-05 5.147715 0.0021348812 0.42147139     1   197    1\n## 3  12.52116  6.096805e-06 5.138114 0.0001537970 0.03030266     1   197    1\n## 4  11.93790 -6.489195e-06 5.284324 0.0001655171 0.03261227     1   197    1\n## 5  11.38254  9.000912e-05 5.472851 0.0281911082 5.71475355     1   197    1\n## 6  12.35924 -2.568690e-05 5.520788 0.0025489432 0.50342501     1   197    1\n##   .index\n## 1      1\n## 2      2\n## 3      3\n## 4      4\n## 5      5\n## 6      6\ncdata(~loan_amount,data=results)##              lower        upper central.p\n## 2.5% -4.277717e-05 9.863203e-05      0.95\nresults2 <- loans200 %>%\n  specify(interest_rate~loan_amount) %>%\n  generate(reps=1000,type=\"bootstrap\") %>%\n  calculate(stat=\"slope\")\nhead(results2)## Response: interest_rate (numeric)\n## Explanatory: loan_amount (numeric)\n## # A tibble: 6 x 2\n##   replicate        stat\n##       <int>       <dbl>\n## 1         1 -0.0000114 \n## 2         2  0.0000198 \n## 3         3  0.0000207 \n## 4         4  0.00000481\n## 5         5  0.0000549 \n## 6         6  0.0000531\nget_confidence_interval(results2)## Using `level = 0.95` to compute confidence interval.## # A tibble: 1 x 2\n##     lower_ci  upper_ci\n##        <dbl>     <dbl>\n## 1 -0.0000361 0.0000947\nresults %>%\n  mutate(pred=Intercept+loan_amount*20000) %>%\n  cdata(~pred,data=.)##        lower    upper central.p\n## 2.5% 11.4061 13.01508      0.95\nplot(int_rate_mod)"},{"path":"LRSIM.html","id":"problem-2-24","chapter":"27 Simulation Based Linear Regression","heading":"27.2.2 Problem 2","text":"Loans IIUsing loans data set 200 observations previous exercise, use variable term determine difference interest rates two different loan lengths.Build set side--side boxplots summarize interest rate term. Describe relationship see. Note: convert term variable factor prior continuing.looks like difference interest rate based length loan. also appears skewed right, positive skew. Let’s plot density see find.Just thought.Build linear model fitting interest rate term. appear significant difference mean interest rates term?significant difference average interest rate based length loan.Write estimated linear model. words, interpret coefficient estimate.intercept \\(\\beta_\\text{Intercept} = \\mu_\\text{term36}\\) average interest rate 36 month loan. \\(\\beta_\\text{term60} = \\mu_\\text{term60} - \\mu_\\text{term36}\\) difference average interest rates loan length. case, 60 month loan 4.66 percentage points higher average 36 month loan.Construct bootstrap confidence interval coefficient.95% confident difference average interest rates loans 60 month 36 month 3.03% 6.24%.Let’s check using assumption normally distributed errors.Close, slightly narrower.Check model assumptions.discrete nature predictor, first two plots interest. assumption constant variable seem reasonable assumption normally distributed errors . positive skewness.","code":"\nloans200 %>%\n  gf_boxplot(interest_rate~term) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title=\"Lending Club\",x=\"Length of Loan\",y=\"Interest Rate\")\nloans200 %>%\n  gf_dens(~interest_rate,group=~term,color=~term) %>%\n  gf_theme(theme_classic()) %>%\n  gf_labs(title=\"Lending Club\",x=\"Interest Rate\",y=\"Density\")\nint_rate_mod2 <- lm(interest_rate~term,data=loans200)\nsummary(int_rate_mod2)## \n## Call:\n## lm(formula = interest_rate ~ term, data = loans200)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -8.643 -3.993 -1.263  3.132 15.597 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  10.7031     0.4230  25.304  < 2e-16 ***\n## term60        4.6601     0.7703   6.049 7.19e-09 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.987 on 197 degrees of freedom\n## Multiple R-squared:  0.1567, Adjusted R-squared:  0.1524 \n## F-statistic:  36.6 on 1 and 197 DF,  p-value: 7.189e-09\nset.seed(331)\nresults <- do(1000)*lm(interest_rate ~ term,data=resample(loans200))\nhead(results)##   Intercept   term60    sigma  r.squared        F numdf dendf .row .index\n## 1  10.11890 4.903166 4.534598 0.20352136 50.33871     1   197    1      1\n## 2  10.91695 5.275981 5.220399 0.17564679 41.97523     1   197    1      2\n## 3  11.07592 4.064085 5.226607 0.11097921 24.59212     1   197    1      3\n## 4  11.14948 3.675445 5.250290 0.09818999 21.44956     1   197    1      4\n## 5  10.45978 4.941671 4.960234 0.17698753 42.36454     1   197    1      5\n## 6  11.02597 4.232862 4.891180 0.13743012 31.38729     1   197    1      6\ncdata(~term60,data=results)##         lower    upper central.p\n## 2.5% 3.029483 6.242261      0.95\nconfint(int_rate_mod2)##                2.5 %    97.5 %\n## (Intercept) 9.868936 11.537251\n## term60      3.140928  6.179218\nplot(int_rate_mod2)"},{"path":"LRMULTI.html","id":"LRMULTI","chapter":"28 Multiple Linear Regression","heading":"28 Multiple Linear Regression","text":"","code":""},{"path":"LRMULTI.html","id":"objectives-26","chapter":"28 Multiple Linear Regression","heading":"28.1 Objectives","text":"Create interpret model multiple predictors check assumptions.Generate interpret confidence intervals estimates.Explain adjusted \\(R^2\\) multi-collinearity.Interpret regression coefficients linear model multiple predictors.Build interpret models higher order terms.","code":""},{"path":"LRMULTI.html","id":"homework-27","chapter":"28 Multiple Linear Regression","heading":"28.2 Homework","text":"","code":""},{"path":"LRMULTI.html","id":"problem-1-27","chapter":"28 Multiple Linear Regression","heading":"28.2.1 Problem 1","text":"mtcars dataset contains average mileage (mpg) information specific makes models cars. (dataset built-R; information dataset, reference documentation ?mtcars).Build interpret coefficients model fitting mpg displacement (disp), horsepower (hp), rear axle ratio (drat), weight 1000 lbs (wt).\\[\n\\mbox{E}(\\text{mpg})=29.15+0.004*\\text{disp}-0.035*\\text{hp}+1.768*\\text{drat}-3.480*\\text{wt}\n\\]coefficient represents expected increase mpg unit increase respective variable, leaving variables constant.Given model, expected, average, mpg vehicle displacement 170, horsepower 100, drat 3.80 wt 2,900 lbs. Construct 95% confidence interval prediction interval expected mpg.Repeat part (b) bootstrap confidence interval.","code":"\ncars_mod<-lm(mpg~disp+hp+drat+wt,data=mtcars)\nsummary(cars_mod)## \n## Call:\n## lm(formula = mpg ~ disp + hp + drat + wt, data = mtcars)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.5077 -1.9052 -0.5057  0.9821  5.6883 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 29.148738   6.293588   4.631  8.2e-05 ***\n## disp         0.003815   0.010805   0.353  0.72675    \n## hp          -0.034784   0.011597  -2.999  0.00576 ** \n## drat         1.768049   1.319779   1.340  0.19153    \n## wt          -3.479668   1.078371  -3.227  0.00327 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.602 on 27 degrees of freedom\n## Multiple R-squared:  0.8376, Adjusted R-squared:  0.8136 \n## F-statistic: 34.82 on 4 and 27 DF,  p-value: 2.704e-10\npredict(cars_mod,newdata=data.frame(disp=170,hp=100,drat=3.8,wt=2.9),interval=\"confidence\")##        fit      lwr      upr\n## 1 22.94652 21.76569 24.12735\npredict(cars_mod,newdata=data.frame(disp=170,hp=100,drat=3.8,wt=2.9),interval=\"prediction\")##        fit      lwr      upr\n## 1 22.94652 17.47811 28.41494\nset.seed(732)\nresults <- do(1000)*lm(mpg~disp+hp+drat+wt,data=resample(mtcars))\nhead(results)##   Intercept          disp          hp      drat        wt    sigma r.squared\n## 1  20.28185 -0.0017783926 -0.02620557 2.8897199 -2.016023 2.300320 0.8210210\n## 2  33.66818 -0.0128734640 -0.01960700 0.3309526 -2.862143 2.102123 0.8760470\n## 3  25.54583 -0.0001983653 -0.03743247 2.1905290 -2.392723 2.814197 0.7947977\n## 4  33.22436  0.0112562822 -0.04151242 1.4796584 -4.557451 2.699223 0.8523512\n## 5  25.96975 -0.0002882344 -0.03093247 2.3689688 -2.861715 2.670069 0.8643060\n## 6  34.17742  0.0054509451 -0.04085537 0.9019272 -3.975107 2.562486 0.8377108\n##          F numdf dendf .row .index\n## 1 30.96393     4    27    1      1\n## 2 47.70611     4    27    1      2\n## 3 26.14436     4    27    1      3\n## 4 38.96659     4    27    1      4\n## 5 42.99429     4    27    1      5\n## 6 34.84241     4    27    1      6\nresults %>%\nmutate(pred=Intercept+disp*170+hp*100+drat*3.8+wt*2.9) %>%\ncdata(~pred,data=.)##         lower    upper central.p\n## 2.5% 21.76703 24.15803      0.95"},{"path":"LRMULTI.html","id":"problem-2-25","chapter":"28 Multiple Linear Regression","heading":"28.2.2 Problem 2","text":"best model predicting mpg? Try variety different models. explore higher order terms even interactions. One place start using pairs() function mtcars plot large pairwise scatterplot. high get adjusted \\(R\\)-squared? Keep mind one measure fit.Answers vary, tried got 0.8694.","code":"\nsummary(lm(mpg~disp+I(disp^2)+hp+I(hp^2)+wt,data=mtcars))## \n## Call:\n## lm(formula = mpg ~ disp + I(disp^2) + hp + I(hp^2) + wt, data = mtcars)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.1591 -1.4907 -0.3903  1.5851  3.7795 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  4.440e+01  2.639e+00  16.823 1.71e-15 ***\n## disp        -4.532e-02  2.131e-02  -2.127 0.043100 *  \n## I(disp^2)    8.844e-05  3.315e-05   2.668 0.012967 *  \n## hp          -8.652e-02  3.813e-02  -2.269 0.031813 *  \n## I(hp^2)      1.585e-04  8.932e-05   1.775 0.087666 .  \n## wt          -3.517e+00  8.874e-01  -3.963 0.000515 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.178 on 26 degrees of freedom\n## Multiple R-squared:  0.8904, Adjusted R-squared:  0.8694 \n## F-statistic: 42.26 on 5 and 26 DF,  p-value: 1.129e-11"},{"path":"LOGREG.html","id":"LOGREG","chapter":"29 Logistic Regression","heading":"29 Logistic Regression","text":"","code":""},{"path":"LOGREG.html","id":"objectives-27","chapter":"29 Logistic Regression","heading":"29.1 Objectives","text":"Using R, conduct logistic regression interpret output perform model selection.Write logistic regression model predict outputs given inputs.Find confidence intervals parameter estimates predictions.Create interpret confusion matrix.","code":""},{"path":"LOGREG.html","id":"homework-problems","chapter":"29 Logistic Regression","heading":"29.2 Homework Problems","text":"","code":""},{"path":"LOGREG.html","id":"problem-1-28","chapter":"29 Logistic Regression","heading":"29.2.1 Problem 1","text":"Possum classificationLet’s investigate possum data set . time want model binary outcome variable. reminder, common brushtail possum Australia region bit cuter distant cousin, American opossum. consider 104 brushtail possums two regions Australia, possums may considered random sample population. first region Victoria, eastern half Australia traverses southern coast. second region consists New South Wales Queensland, make eastern northeastern Australia.use logistic regression differentiate possums two regions. outcome variable, called pop, takes value Vic possum Victoria New South Wales Queensland. consider five predictors: sex, head_l, skull_w, total_l, tail_l.Explore data making histograms quantitative variables, bar charts discrete variables. outliers likely large influence logistic regression model?potential outliers skull width otherwise much concern.\ncan see head_l correlated three variables. cause multicollinearity problems.Build logistic regression model variable. Report summary model.Using p-values decide want remove variable(S) build model.Let’s remove head_l first.Since head_l correlated variables, removing increased precision, decreased standard error, predictors. p-values now less 0.05.variable decide remove, build 95% confidence interval parameter.95% confident true slope coefficient head_l -0.44 0.108.bootstrap working problem. may convergence issues resample data. reminder need careful just run methods without checking results. code:interval large. resampling process much variation . due small sample size multicollinearity.Explain remaining parameter estimates change two models.coefficient estimates sensitive variables included model, typically indicates variables collinear. example, possum’s gender may related head length, explain coefficient (p-value) sex male changed removed head length variable. Likewise, possum’s skull width likely related head length, probably even much closely related head length gender.Write form model. Also identify following variables positively associated (controlling variables) possum Victoria: head_l, skull_w, total_l, tail_l.dropped head_l model. equation:\\[\n\\log_{e}\\left( \\frac{p_i}{1-p_i} \\right)\n    = 33.5 - 1.42 \\text{ sex} -0.28 \\text{ skull width}  + 0.57 \\text{ total length} - 1.81 \\text{ tail length}\n\\]total_l positively association probability Victoria.Suppose see brushtail possum zoo US, sign says possum captured wild Australia, doesn’t say part Australia. However, sign indicate possum male, skull 63 mm wide, tail 37 cm long, total length 83 cm. reduced model’s computed probability possum Victoria? confident model’s accuracy probability calculation?Let’s predict outcome. use response type put answer form probability. See help menu predict.glm information.probability, 0.006, near zero, run diagnostics model. also little skepticism model hold possum found US zoo. However, encouraging possum caught wild.rough sense accuracy, use standard error. errors really binomial trying use normal approximation. remember back block probability, low probability, assumption normality suspect. However, use give us upper bound., probability possum Victoria 2%.","code":"\npossum <- read_csv(\"data/possum.csv\") %>%\n  select(pop,sex,head_l,skull_w,total_l,tail_l) %>%\n  mutate(pop=factor(pop),sex=factor(sex))\ninspect(possum)## \n## categorical variables:  \n##   name  class levels   n missing                                  distribution\n## 1  pop factor      2 104       0 other (55.8%), Vic (44.2%)                   \n## 2  sex factor      2 104       0 m (58.7%), f (41.3%)                         \n## \n## quantitative variables:  \n##         name   class  min     Q1 median     Q3   max     mean       sd   n\n## ...1  head_l numeric 82.5 90.675  92.80 94.725 103.1 92.60288 3.573349 104\n## ...2 skull_w numeric 50.0 54.975  56.35 58.100  68.6 56.88365 3.113426 104\n## ...3 total_l numeric 75.0 84.000  88.00 90.000  96.5 87.08846 4.310549 104\n## ...4  tail_l numeric 32.0 35.875  37.00 38.000  43.0 37.00962 1.959518 104\n##      missing\n## ...1       0\n## ...2       0\n## ...3       0\n## ...4       0\npossum %>%\n  gf_props(~pop,fill=\"cyan\",color=\"black\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"Population\")\npossum %>%\n  gf_props(~sex,fill=\"cyan\",color=\"black\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(x=\"Gender\")\npossum %>%\n  gf_boxplot(~head_l) %>%\n  gf_theme(theme_bw())\npossum %>%\n  gf_boxplot(~skull_w) %>%\n  gf_theme(theme_bw())\npossum %>%\n  gf_boxplot(~total_l) %>%\n  gf_theme(theme_bw())\npossum %>%\n  gf_boxplot(~tail_l) %>%\n  gf_theme(theme_bw())\npairs(possum[,3:6],lower.panel = panel.smooth)\npossum_mod <- glm(pop==\"Vic\"~.,data=possum,family=\"binomial\")\nsummary(possum_mod)## \n## Call:\n## glm(formula = pop == \"Vic\" ~ ., family = \"binomial\", data = possum)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.6430  -0.5514  -0.1182   0.3760   2.8501  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  39.2349    11.5368   3.401 0.000672 ***\n## sexm         -1.2376     0.6662  -1.858 0.063195 .  \n## head_l       -0.1601     0.1386  -1.155 0.248002    \n## skull_w      -0.2012     0.1327  -1.517 0.129380    \n## total_l       0.6488     0.1531   4.236 2.27e-05 ***\n## tail_l       -1.8708     0.3741  -5.001 5.71e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 142.787  on 103  degrees of freedom\n## Residual deviance:  72.155  on  98  degrees of freedom\n## AIC: 84.155\n## \n## Number of Fisher Scoring iterations: 6\nconfint(possum_mod)## Waiting for profiling to be done...##                  2.5 %      97.5 %\n## (Intercept) 18.8530781 64.66444839\n## sexm        -2.6227018  0.02472167\n## head_l      -0.4428559  0.10865739\n## skull_w     -0.4933140  0.04479826\n## total_l      0.3768179  0.98455786\n## tail_l      -2.7170468 -1.23231969\npossum_mod_red <- glm(pop==\"Vic\"~sex+skull_w+total_l+tail_l,data=possum,family=\"binomial\")\nsummary(possum_mod_red)## \n## Call:\n## glm(formula = pop == \"Vic\" ~ sex + skull_w + total_l + tail_l, \n##     family = \"binomial\", data = possum)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.8102  -0.5683  -0.1222   0.4153   2.7599  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  33.5095     9.9053   3.383 0.000717 ***\n## sexm         -1.4207     0.6457  -2.200 0.027790 *  \n## skull_w      -0.2787     0.1226  -2.273 0.023053 *  \n## total_l       0.5687     0.1322   4.302 1.69e-05 ***\n## tail_l       -1.8057     0.3599  -5.016 5.26e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 142.787  on 103  degrees of freedom\n## Residual deviance:  73.516  on  99  degrees of freedom\n## AIC: 83.516\n## \n## Number of Fisher Scoring iterations: 6\nconfint(possum_mod)## Waiting for profiling to be done...##                  2.5 %      97.5 %\n## (Intercept) 18.8530781 64.66444839\n## sexm        -2.6227018  0.02472167\n## head_l      -0.4428559  0.10865739\n## skull_w     -0.4933140  0.04479826\n## total_l      0.3768179  0.98455786\n## tail_l      -2.7170468 -1.23231969\nset.seed(952)\nresults<-do(1000)*glm(pop==\"Vic\"~.,data=resample(possum),family=\"binomial\")\nhead(results[,1:5])##     Intercept          sexm        head_l       skull_w       total_l\n## 1 -1184.61865  2.122389e+01  3.861560e+00  2.749263e+00  7.274005e+00\n## 2  6371.55494 -1.301513e+02  1.023732e+01 -2.738815e+01 -1.076913e+01\n## 3 -9612.61859 -2.392899e+03 -1.875252e+02  5.782026e+02 -2.820691e+02\n## 4   -25.18662 -1.852185e+01  2.097593e+01 -1.353619e+01  1.483815e+01\n## 5   -26.56607  2.167890e-13 -4.396307e-15  5.075768e-15  4.484497e-14\n## 6 -1025.00035  6.159665e+01  2.526181e+01 -2.032143e+01  1.438639e+01\nresults %>%\n  gf_histogram(~head_l,fill=\"cyan\",color = \"black\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(title=\"Bootstrap sampling distribtuion\",\n          x=\"sex paramater estimate\")\ncdata(~head_l,data=results)##          lower    upper central.p\n## 2.5% -187.5252 63.61867      0.95\npredict(possum_mod_red,newdata = data.frame(sex=\"m\",skull_w=63,tail_l=37,total_l=83),\n        type=\"response\",se.fit = TRUE)## $fit\n##           1 \n## 0.006205055 \n## \n## $se.fit\n##           1 \n## 0.008011468 \n## \n## $residual.scale\n## [1] 1\n0.0062+c(-1,1)*1.96*.008## [1] -0.00948  0.02188"},{"path":"LOGREG.html","id":"problem-2-26","chapter":"29 Logistic Regression","heading":"29.2.2 Problem 2","text":"Medical school admissionThe file MedGPA.csv data folder information medical school admission status GPA standardized test scores gathered 55 medical school applicants liberal arts college Midwest.variables :Accept Status: =accepted medical school D=denied admission\nAcceptance: Indicator Accept: 1=accepted 0=deniedSex: F=female M=maleBCPM: Bio/Chem/Physics/Math grade point averageGPA: College grade point averageVR: Verbal reasoning (subscore)PS: Physical sciences (subscore)WS: Writing sample (subcore)BS: Biological sciences (subscore)MCAT: Score MCAT exam (sum CR+PS+WS+BS)Apps: Number medical schools applied toBuild logistic regression model predict Acceptance GPA `Sex.Generate 95% confidence interval coefficient associated GPA.Let’s try bootstrap problem.bad. appears distribution GPA skewed left.Fit model polynomial degree 2 GPA. Drop Sex model. quadratic fit improve model?quadratic term improve model.Fit model just GPA interpret coefficient.increase 1 student’s GPA decreases odds denied acceptance 0.00428. Remember probability. reminder odds \\(\\frac{1}{2}\\) means probability success \\(\\frac{1}{3}\\). odds 1 means probability success \\(\\frac{1}{2}\\). Assume initial odds 1 odds now 0.00428 smaller, probability success \\(\\frac{428}{10428}\\) 0.04. probability decreased order magnitude.Try add different predictors come best model. use Acceptance MCAT model.Let’s take VR.Now let’s take Apps.Next, let’s remove BCPM.Now WS.Maybe PS.stop . better way. Machine learning advanced regression courses explore select predictors improve model.Generate confusion matrix best model developed.model accuracy \\(\\frac{47}{55}\\) 85.5%.Find 95% confidence interval probability female student 3.5 GPA, BCPM 3.8, verbal reasoning score 10, physical sciences score 9, writing sample score 8, biological score 10, MCAT score 40, applied 5 medical schools.female students GPA 3.5 biological score 10, 95% confident probability denied acceptance medical school 0 .433.Let’s try bootstrap.close found make assumption probability success normally distributed.","code":"\nMedGPA <- read_csv(\"data/MedGPA.csv\")\nglimpse(MedGPA)## Rows: 55\n## Columns: 11\n## $ Accept     <chr> \"D\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"D\", \"A\", \"A\", \"A\", \"A\",~\n## $ Acceptance <dbl> 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,~\n## $ Sex        <chr> \"F\", \"M\", \"F\", \"F\", \"F\", \"M\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\",~\n## $ BCPM       <dbl> 3.59, 3.75, 3.24, 3.74, 3.53, 3.59, 3.85, 3.26, 3.74, 3.86,~\n## $ GPA        <dbl> 3.62, 3.84, 3.23, 3.69, 3.38, 3.72, 3.89, 3.34, 3.71, 3.89,~\n## $ VR         <dbl> 11, 12, 9, 12, 9, 10, 11, 11, 8, 9, 11, 11, 8, 9, 11, 12, 8~\n## $ PS         <dbl> 9, 13, 10, 11, 11, 9, 12, 11, 10, 9, 9, 8, 10, 9, 8, 8, 8, ~\n## $ WS         <dbl> 9, 8, 5, 7, 4, 7, 6, 8, 6, 6, 8, 4, 7, 4, 6, 8, 8, 9, 5, 8,~\n## $ BS         <dbl> 9, 12, 9, 10, 11, 10, 11, 9, 11, 10, 11, 8, 10, 10, 7, 10, ~\n## $ MCAT       <dbl> 38, 45, 33, 40, 35, 36, 40, 39, 35, 34, 39, 31, 35, 32, 32,~\n## $ Apps       <dbl> 5, 3, 19, 5, 11, 5, 5, 7, 5, 11, 6, 9, 5, 8, 15, 6, 6, 1, 5~\nmed_mod<-glm(Accept==\"D\"~GPA+Sex,data=MedGPA,family=binomial)\nsummary(med_mod)## \n## Call:\n## glm(formula = Accept == \"D\" ~ GPA + Sex, family = binomial, data = MedGPA)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.4623  -0.7194  -0.2978   0.9753   1.8171  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  21.0680     6.4025   3.291 0.001000 ***\n## GPA          -6.1324     1.8283  -3.354 0.000796 ***\n## SexM          1.1697     0.7178   1.629 0.103210    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 75.791  on 54  degrees of freedom\n## Residual deviance: 53.945  on 52  degrees of freedom\n## AIC: 59.945\n## \n## Number of Fisher Scoring iterations: 5\nconfint(med_mod)## Waiting for profiling to be done...##                   2.5 %    97.5 %\n## (Intercept)  10.0651173 35.579760\n## GPA         -10.2977983 -3.007544\n## SexM         -0.1720454  2.697436\nset.seed(819)\nresults_med <- do(1000)*glm(Accept==\"D\"~GPA+Sex,data=resample(MedGPA),family=binomial)## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nresults_med %>%\n  gf_histogram(~GPA,fill=\"cyan\",color = \"black\") %>%\n  gf_theme(theme_bw()) %>%\n  gf_labs(title=\"Bootstrap sampling distribtuion\",\n          x=\"GPA paramater estimate\")\ncdata(~GPA,data=results_med)##         lower     upper central.p\n## 2.5% -14.2047 -3.213036      0.95\nmed_mod2<-glm(Accept==\"D\"~poly(GPA,2),data=MedGPA,family=binomial)\nsummary(med_mod2)## \n## Call:\n## glm(formula = Accept == \"D\" ~ poly(GPA, 2), family = binomial, \n##     data = MedGPA)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.9553  -0.7830  -0.3207   0.8020   1.8363  \n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)    -0.3301     0.3582  -0.921 0.356822    \n## poly(GPA, 2)1 -10.7293     3.0664  -3.499 0.000467 ***\n## poly(GPA, 2)2  -3.4967     3.0987  -1.128 0.259133    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 75.791  on 54  degrees of freedom\n## Residual deviance: 55.800  on 52  degrees of freedom\n## AIC: 61.8\n## \n## Number of Fisher Scoring iterations: 4\ntidy(glm(Accept==\"D\"~GPA,data=MedGPA,family=binomial))## # A tibble: 2 x 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    19.2       5.63      3.41 0.000644\n## 2 GPA            -5.45      1.58     -3.45 0.000553\nexp(-5.454166)## [1] 0.004278444\ntidy(glm(Accept==\"D\"~.-Acceptance-MCAT,data=MedGPA,family=binomial)) %>%\n  mutate(p.adj=p.adjust(p.value)) %>%\n  select(term,p.value,p.adj)## # A tibble: 9 x 3\n##   term        p.value  p.adj\n##   <chr>         <dbl>  <dbl>\n## 1 (Intercept) 0.00279 0.0251\n## 2 SexM        0.110   0.551 \n## 3 BCPM        0.376   1     \n## 4 GPA         0.150   0.600 \n## 5 VR          0.799   1     \n## 6 PS          0.0305  0.213 \n## 7 WS          0.0491  0.295 \n## 8 BS          0.00490 0.0392\n## 9 Apps        0.728   1\ntidy(glm(Accept==\"D\"~.-Acceptance-MCAT-VR,data=MedGPA,family=binomial)) %>%\n  mutate(p.adj=p.adjust(p.value)) %>%\n  select(term,p.value,p.adj)## # A tibble: 8 x 3\n##   term        p.value  p.adj\n##   <chr>         <dbl>  <dbl>\n## 1 (Intercept) 0.00239 0.0191\n## 2 SexM        0.0681  0.272 \n## 3 BCPM        0.362   0.723 \n## 4 GPA         0.143   0.430 \n## 5 PS          0.0280  0.168 \n## 6 WS          0.0452  0.226 \n## 7 BS          0.00476 0.0333\n## 8 Apps        0.742   0.742\ntidy(glm(Accept==\"D\"~.-Acceptance-MCAT-VR-Apps,data=MedGPA,family=binomial)) %>%\n  mutate(p.adj=p.adjust(p.value)) %>%\n  select(term,p.value,p.adj)## # A tibble: 7 x 3\n##   term        p.value  p.adj\n##   <chr>         <dbl>  <dbl>\n## 1 (Intercept) 0.00147 0.0103\n## 2 SexM        0.0272  0.133 \n## 3 BCPM        0.379   0.379 \n## 4 GPA         0.120   0.240 \n## 5 PS          0.0266  0.133 \n## 6 WS          0.0385  0.133 \n## 7 BS          0.00477 0.0286\ntidy(glm(Accept==\"D\"~.-Acceptance-MCAT-VR-Apps-BCPM,\n         data=MedGPA,family=binomial)) %>%\n  mutate(p.adj=p.adjust(p.value)) %>%\n  select(term,p.value,p.adj)## # A tibble: 6 x 3\n##   term        p.value   p.adj\n##   <chr>         <dbl>   <dbl>\n## 1 (Intercept) 0.00123 0.00739\n## 2 SexM        0.0142  0.0567 \n## 3 GPA         0.0315  0.0901 \n## 4 PS          0.0300  0.0901 \n## 5 WS          0.0401  0.0901 \n## 6 BS          0.00537 0.0269\ntidy(glm(Accept==\"D\"~.-Acceptance-MCAT-VR-Apps-BCPM-WS,\n         data=MedGPA,family=binomial)) %>%\n  mutate(p.adj=p.adjust(p.value)) %>%\n  select(term,p.value,p.adj)## # A tibble: 5 x 3\n##   term         p.value   p.adj\n##   <chr>          <dbl>   <dbl>\n## 1 (Intercept) 0.000584 0.00292\n## 2 SexM        0.0168   0.0504 \n## 3 GPA         0.0366   0.0732 \n## 4 PS          0.0634   0.0732 \n## 5 BS          0.0100   0.0401\ntidy(glm(Accept==\"D\"~.-Acceptance-MCAT-VR-Apps-BCPM-WS-PS,\n         data=MedGPA,family=binomial)) %>%\n  mutate(p.adj=p.adjust(p.value)) %>%\n  select(term,p.value,p.adj)## # A tibble: 4 x 3\n##   term         p.value   p.adj\n##   <chr>          <dbl>   <dbl>\n## 1 (Intercept) 0.000574 0.00230\n## 2 SexM        0.0306   0.0611 \n## 3 GPA         0.0325   0.0611 \n## 4 BS          0.00448  0.0134\nmed_mod2<-glm(Accept==\"D\"~Sex+GPA+BS,\n         data=MedGPA,family=binomial)\nsummary(med_mod2)## \n## Call:\n## glm(formula = Accept == \"D\" ~ Sex + GPA + BS, family = binomial, \n##     data = MedGPA)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.8388  -0.4707  -0.1347   0.5535   2.6411  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  27.4608     7.9721   3.445 0.000572 ***\n## SexM          1.9885     0.9193   2.163 0.030543 *  \n## GPA          -4.2900     2.0062  -2.138 0.032484 *  \n## BS           -1.3752     0.4838  -2.843 0.004471 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 75.791  on 54  degrees of freedom\n## Residual deviance: 40.835  on 51  degrees of freedom\n## AIC: 48.835\n## \n## Number of Fisher Scoring iterations: 6\naugment(med_mod2,type.predict = \"response\") %>%\n  rename(actual=starts_with('Accept ==')) %>%\n  transmute(result=as.integer(.fitted>0.5),\n            actual=as.integer(actual)) %>%\n  table()##       actual\n## result  0  1\n##      0 26  4\n##      1  4 21\npredict(med_mod2,newdata = data.frame(Sex=\"F\",\n                                      GPA=3.5,\n                                      BS=10),\n        type=\"response\",se.fit = TRUE)## $fit\n##         1 \n## 0.2130332 \n## \n## $se.fit\n##         1 \n## 0.1122277 \n## \n## $residual.scale\n## [1] 1\n0.2130332+c(-1,1)*1.96*.1122277## [1] -0.006933092  0.432999492\nset.seed(729)\nresults <- do(1000)*glm(Accept==\"D\"~Sex+GPA+BS,\n                      data=resample(MedGPA),\n                      family=binomial)\nhead(results)##    Intercept       SexM        GPA         BS .row .index\n## 1  204.23984  20.701630  -4.547781 -20.801516    1      1\n## 2   16.78193   1.373795  -2.508340  -0.908112    1      2\n## 3   36.09536   4.170571  -7.479463  -1.219088    1      3\n## 4 1104.42269 105.236280 -90.461857 -86.132398    1      4\n## 5   25.37989   1.139262  -3.729948  -1.331750    1      5\n## 6   26.58307   3.197072  -3.426165  -1.772581    1      6\nresults_pred <- results %>% \n  mutate(pred=1/(1+exp(-1*(Intercept+3.5*GPA+10*BS))))\ncdata(~pred,data=results_pred)##            lower     upper central.p\n## 2.5% 7.77316e-10 0.5032905      0.95"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
